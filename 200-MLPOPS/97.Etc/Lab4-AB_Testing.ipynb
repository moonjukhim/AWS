{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B testing with Amazon SageMaker\n",
    "\n",
    "A/B testing is similar to canary testing, but it has larger user groups and a longer timescale, typically days or even weeks. For this type of testing, Amazon SageMaker endpoint configuration uses two production variants: one for model A, and one for model B. To begin, configure the settings for both models to balance traffic between them equally (50/50) and make sure that both models have identical instance configurations. After you have monitored the performance of both models with the initial setting of equal weights, you can either gradually change the traffic weights to put the models out of balance (60/40, 80/20, etc.), or you can change the weights in a single step, continuing until a single model is processing all of the live traffic.\n",
    "\n",
    "For the A/B testing you are doing, the models are using two different algorithms for the same classification problem. We have updated code of the ModelA code with the **decision tree** algorithm and ModelB with the **random forest** algorithm. It is also best practice to use different hyperparameters with the same algorithm to find the optimized iteration.\n",
    "\n",
    "In this notebook you will:\n",
    "* Evaluate models by invoking specific variants\n",
    "* Gradually release a new model by specifying traffic distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Code cells display their status to the left of the code cell.\n",
    "  - Cell has not run: In [ ]\n",
    "  - Cell is scheduled to run or is currently running: In [\\*]\n",
    "  - Cell has completed running: In [#], where # is a unique run number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerrequisites\n",
    "\n",
    "First, ensure you have the updated version of boto3, which includes the latest SageMaker features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U awscli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "Let's set up some required imports and basic initial variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "from sagemaker import get_execution_role, session\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "\n",
    "region= boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_session = session.Session(boto3.Session())\n",
    "sm = boto3.Session().client(\"sagemaker\")\n",
    "sm_runtime = boto3.Session().client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the name of the S3 modelDataBucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = ''\n",
    "s3 = boto3.resource('s3')\n",
    "for buckets in s3.buckets.all():\n",
    "    if 'modeldatabucket' in buckets.name:\n",
    "        bucket = buckets.name\n",
    "print(bucket)\n",
    "prefix = 'v1.0/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the **endpoint name** you copied in your notepad earlier in the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'Endpoint-63be92ef-5728-43e0-80ed-d6e665750f8b'\n",
    "if 'enter_endpoint_name' in endpoint_name:\n",
    "    raise Exception('You need to update the endpoint_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3Downloader.download(s3_uri=f\"s3://{bucket}/{prefix}/iris.csv\", local_path= 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data in pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "shape=pd.read_csv(\"data/iris.csv\", header=None)\n",
    "shape.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the sample data for generating traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "a = [10*i for i in range(3)]\n",
    "b = [10+i for i in range(10)]\n",
    "indices = [i+j for i,j in itertools.product(a,b)]\n",
    "\n",
    "test_data = shape.drop(shape.columns[[0]],axis=1)\n",
    "test_data = test_data.iloc[indices]\n",
    "test_data_with_label = shape.iloc[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the randomized test data to the local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"data/data-test.csv\",index=False,header=False)\n",
    "test_data_with_label.to_csv(\"data/data-test-label.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Invoke the deployed models\n",
    "\n",
    "You can send data to the endpoint to get inferences in real time.\n",
    "\n",
    "This step invokes the endpoint with included sample data for about 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a subset of test data for a quick test\n",
    "print(f\"Sending test traffic to the endpoint {endpoint_name}. \\nPlease wait...\")\n",
    "predictions = ''\n",
    "\n",
    "with open('data/data-test.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = sm_runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType=\"text/csv\",\n",
    "                                   Body=payload)\n",
    "        predictions = ','.join([predictions, response['Body'].read().decode('utf-8')])\n",
    "        time.sleep(0.5)\n",
    "\n",
    "predictions = predictions.replace('\\n','')\n",
    "predictions = predictions.split(\",\")\n",
    "predictions.pop(0)\n",
    "print(\"=\"*20)\n",
    "print(predictions)\n",
    "print(\"Done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = test_data_with_label[0].to_numpy()\n",
    "preds = np.array(predictions)\n",
    "preds = preds.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = np.count_nonzero(preds == labels) / len(labels)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invocations per variant\n",
    "\n",
    "Amazon SageMaker emits metrics, such as latency and invocations (full list of metrics [here](https://alpha-docs-aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html)), for each variant in Amazon CloudWatch. Letâ€™s query CloudWatch to get the number of invocations per variant, which will show how invocations are split across variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = boto3.Session().client(\"cloudwatch\")\n",
    "\n",
    "def get_invocation_metrics_for_endpoint_variant(endpoint_name,\n",
    "                                                variant_name,\n",
    "                                                start_time,\n",
    "                                                end_time):\n",
    "    metrics = cw.get_metric_statistics(\n",
    "        Namespace=\"AWS/SageMaker\",\n",
    "        MetricName=\"Invocations\",\n",
    "        StartTime=start_time,\n",
    "        EndTime=end_time,\n",
    "        Period=60,\n",
    "        Statistics=[\"Sum\"],\n",
    "        Dimensions=[\n",
    "            {\n",
    "                \"Name\": \"EndpointName\",\n",
    "                \"Value\": endpoint_name\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"VariantName\",\n",
    "                \"Value\": variant_name\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return pd.DataFrame(metrics[\"Datapoints\"])\\\n",
    "            .sort_values(\"Timestamp\")\\\n",
    "            .set_index(\"Timestamp\")\\\n",
    "            .drop(\"Unit\", axis=1)\\\n",
    "            .rename(columns={\"Sum\": variant_name})\n",
    "\n",
    "def plot_endpoint_metrics(start_time=None):\n",
    "    start_time = start_time or datetime.now() - timedelta(minutes=60)\n",
    "    end_time = datetime.now()\n",
    "    metrics_variant1 = get_invocation_metrics_for_endpoint_variant(endpoint_name, \"Variant1\", start_time, end_time)\n",
    "    metrics_variant2 = get_invocation_metrics_for_endpoint_variant(endpoint_name, \"Variant2\", start_time, end_time)\n",
    "    metrics_variants = metrics_variant1.join(metrics_variant2, how=\"outer\")\n",
    "    metrics_variants.plot()\n",
    "    return metrics_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting a minute for initial metric creation...\")\n",
    "time.sleep(60)\n",
    "plot_endpoint_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you observe failure, such as a blank graph, re-run the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke a specific variant\n",
    "\n",
    "Now, letâ€™s invoke a specific variant. For this, simply use the new parameter to define which specific ProductionVariant you want to invoke. Let's use this to invoke Variant1 for all requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variant 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = ''\n",
    "\n",
    "print(f\"Sending test traffic to the endpoint {endpoint_name}. \\nPlease wait...\")\n",
    "with open('data/data-test.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = sm_runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType=\"text/csv\",\n",
    "                                   Body=payload,\n",
    "                                   TargetVariant='Variant1')\n",
    "        predictions1 = ','.join([predictions1, response['Body'].read().decode('utf-8')])\n",
    "        time.sleep(0.5)\n",
    "\n",
    "predictions1 = predictions1.replace('\\n','')\n",
    "predictions1 = predictions1.split(\",\")\n",
    "predictions1.pop(0)\n",
    "print(\"=\"*20)\n",
    "print(predictions1)\n",
    "print(\"Done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60) #let metrics catch up\n",
    "plot_endpoint_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you observe failure, such as a blank graph, re-run the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of variant 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "\n",
    "labels1 = test_data_with_label[0].to_numpy()\n",
    "preds1 = np.array(predictions1)\n",
    "preds1 = preds1.astype(np.int)\n",
    "\n",
    "accuracy1 = np.count_nonzero(preds1 == labels1) / len(labels1)\n",
    "print(f\"Accuracy_variant1: {accuracy1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variant 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = ''\n",
    "\n",
    "print(f\"Sending test traffic to the endpoint {endpoint_name}. \\nPlease wait...\")\n",
    "with open('data/data-test.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = sm_runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType=\"text/csv\",\n",
    "                                   Body=payload,\n",
    "                                   TargetVariant='Variant2')\n",
    "        predictions2 = ','.join([predictions2, response['Body'].read().decode('utf-8')])\n",
    "        time.sleep(0.5)\n",
    "\n",
    "predictions2 = predictions2.replace('\\n','')\n",
    "predictions2 = predictions2.split(\",\")\n",
    "predictions2.pop(0)\n",
    "print(\"=\"*20)\n",
    "print(predictions2)\n",
    "print(\"Done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60) #let metrics catch up\n",
    "plot_endpoint_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you observe failure, such as a blank graph, re-run the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of variant 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "\n",
    "labels2 = test_data_with_label[0].to_numpy()\n",
    "preds2 = np.array(predictions2)\n",
    "preds2 = preds2.astype(np.int)\n",
    "\n",
    "accuracy2 = np.count_nonzero(preds2 == labels2) / len(labels2)\n",
    "print(f\"Accuracy_variant2: {accuracy2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Dialing up your chosen variant in production\n",
    "\n",
    "Now that you have determined Variant2 to be better as compared to Variant1, you will shift more traffic to it. \n",
    "\n",
    "You can continue to use TargetVariant to continue invoking a chosen variant. A simpler approach is to update the weights assigned to each variant using UpdateEndpointWeightsAndCapacities. This changes the traffic distribution to your production variants without requiring updates to your endpoint. \n",
    "\n",
    "Recall your variant weights are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    variant[\"VariantName\"]: variant[\"CurrentWeight\"]\n",
    "    for variant in sm.describe_endpoint(EndpointName=endpoint_name)[\"ProductionVariants\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will first write a method to easily invoke your endpoint (a copy of what you had been previously doing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_endpoint_for_two_minutes():\n",
    "    with open('data/data-test.csv', 'r') as f:\n",
    "        for row in f:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            payload = row.rstrip('\\n')\n",
    "            response = sm_runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                                  ContentType='text/csv', \n",
    "                                                  Body=payload)\n",
    "            response['Body'].read()\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You invoke your endpoint for a bit, to show the even split in invocations.\n",
    "\n",
    "**Note** This step will take 3-5 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invocation_start_time = datetime.now()\n",
    "invoke_endpoint_for_two_minutes()\n",
    "time.sleep(120) # give metrics time to catch up\n",
    "plot_endpoint_metrics(invocation_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you observe failure, such as a blank graph, re-run the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shift 75% of the traffic to Variant2 by assigning new weights to each variant using UpdateEndpointWeightsAndCapacities. Amazon SageMaker will now send 75% of the inference requests to Variant2 and the remaining 25% of requests to Variant1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=endpoint_name,\n",
    "    DesiredWeightsAndCapacities=[\n",
    "        {\n",
    "            \"DesiredWeight\": 25,\n",
    "            \"VariantName\": 'Variant1'\n",
    "        },\n",
    "        {\n",
    "            \"DesiredWeight\": 75,\n",
    "            \"VariantName\": 'Variant2'\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting for update to complete\")\n",
    "while True:\n",
    "    status = sm.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        print(\"Done\")\n",
    "        break\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    time.sleep(1)\n",
    "\n",
    "{\n",
    "    variant[\"VariantName\"]: variant[\"CurrentWeight\"]\n",
    "    for variant in sm.describe_endpoint(EndpointName=endpoint_name)[\"ProductionVariants\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_endpoint_for_two_minutes()\n",
    "time.sleep(120) # give metrics time to catch up\n",
    "plot_endpoint_metrics(invocation_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you observe failure, such as a blank graph, re-run the previous step.\n",
    "\n",
    "You can continue to monitor your metrics and when you're satisfied with a variant's performance, you can route 100% of the traffic over the variant. You used UpdateEndpointWeightsAndCapacities to update the traffic assignments for the variants. The weight for Variant1 is set to 0 and the weight for Variant2 is set to 1. Therefore, SageMaker will send 100% of all inference requests to Variant2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "\n",
    "Now you know how to tune your traffic. Can you complete the following code to send 100% traffic to Variant2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_endpoint_for_two_minutes()\n",
    "time.sleep(120) # give metrics time to catch up\n",
    "plot_endpoint_metrics(invocation_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you observe failure, such as a blank graph, re-run the previous step.\n",
    "\n",
    "The Amazon CloudWatch metrics for the total invocations for each variant indicates that all inference requests are being processed by Variant2 and there are no inference requests processed by Variant1.\n",
    "\n",
    "You can now safely update your endpoint and delete Variant1 from it. You can also continue testing new models in production by adding new variants to your endpoint and following steps 2 - 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"To review the solution notebook, see the **[Lab 2 A/B testing solution](../solutions/Lab2-AB_Testing-Solution.ipynb)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the endpoint\n",
    "\n",
    "If you do not plan to use this endpoint further, you should delete it to avoid incurring additional charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_session.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
