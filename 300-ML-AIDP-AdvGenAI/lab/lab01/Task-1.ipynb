{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d6294e-5c99-472d-b504-4b02822ddb03",
   "metadata": {},
   "source": [
    "# Task 1: Leverage a fully-managed RAG application with Amazon Bedrock's RetrieveAndGenerate API\n",
    "\n",
    "In this task, you leverage a fully-managed Retrieval Augmented Generation (RAG) application with Amazon Bedrock's RetrieveAndGenerate API.\n",
    "\n",
    "The following resources were already created as part of the lab provisioning:\n",
    "\n",
    "- An OpenSearch serverless index.\n",
    "- A Knowledge Base in Amazon Bedrock.\n",
    "- A data source within the Knowledge Base in Amazon Bedrock.\n",
    "\n",
    "The typical data ingestion workflow is as follows:\n",
    "\n",
    "<!-- ![data_ingestion.png](images/data_ingestion.png) -->\n",
    "<img src=\"images/data_ingestion.png\" width=50% height=20% />\n",
    "\n",
    "*Image description: The preceding diagram depicts the typical data ingestion workflow for the lab environment.*\n",
    "\n",
    "A data pipeline ingests documents which are typically stored in multiple data sources into a knowledge base i.e. a vector database such as Amazon OpenSearch Service Serverless (AOSS) so that it is available for lookup when a question is received.\n",
    "\n",
    "- The documents are loaded into the knowledge base in Amazon Bedrock by connecting to various data sources (like S3, Confluence, Sharepoint, Salesforce, and Web). \n",
    "- Knowledge base then splits them into smaller chunks (based on the strategy selected), and generates embeddings which are stored in the associated vector store.\n",
    "\n",
    "Once the data is available in the Bedrock Knowledge Base, you build a question answering application using the Knowledge Base APIs provided by Amazon Bedrock.\n",
    "\n",
    "### Final lab architecture:\n",
    "\n",
    "<!-- ![retrieveAndGenerate.png](images/retrieveAndGenerate.png) -->\n",
    "<img src=\"images/retrieveAndGenerate.png\" width=50% height=20% />\n",
    "\n",
    "*Image description: The preceding diagram depicts the data flow that starts with a user query that uses the RetrieveAndGenerate API and generates response using the documents from the Knowledge Base in Amazon Bedrock.*\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-exclamation-circle\" style=\"color:#7C5AED\"></i> **Caution:** It is recommended to run each code cell individually rather than using the **Run All Cells** option from the **Run** menu. Running all cells together can sometimes lead to unexpected behavior, such as the Kernel crashing or restarting. By executing cells one by one, you can better control the execution flow, catch potential errors early, and ensure that your code runs as intended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33f7e0-b824-473f-bbff-1bd05eaed0bb",
   "metadata": {},
   "source": [
    "## Task 1.1: Setup the environment\n",
    "\n",
    "In this task, you initialize the boto3 client and setup the environment. Throughout the notebook, you utilize the *RetrieveAndGenerate* API to test the Knowledge Base features.\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-info-circle\" style=\"color:#007FAA\"></i> **Learn more:** Refer to *[RetrieveAndGenerate](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent-runtime_RetrieveAndGenerate.html)* for additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b6e79",
   "metadata": {},
   "source": [
    "1. Run the following code cell to initialize the boto3 client and setup your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc2f60-8aac-4518-932d-4ae1547e06b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pprint\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.client import Config\n",
    "\n",
    "# Create boto3 session\n",
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "# Create bedrock agent client\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0}, region_name=region_name)\n",
    "bedrock_agent_client = boto3_session.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config)\n",
    "\n",
    "# Define FM to be used for generations \n",
    "model_id = \"amazon.nova-lite-v1:0\" # we will be using Amazon Nova lite throughout the notebook\n",
    "model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4312663",
   "metadata": {},
   "source": [
    "2. Run the following code cell to verify the ID for the existing Knowledge Base in Amazon Bedrock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee0499ed-0df2-423b-8ddd-2ec059d2dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "\n",
    "session = boto3.Session()\n",
    "bedrock_client = session.client('bedrock-agent')\n",
    "\n",
    "try:\n",
    "    response = bedrock_client.list_knowledge_bases(\n",
    "        maxResults=1  # We only need to retrieve the first Knowledge Base\n",
    "    )\n",
    "    knowledge_base_summaries = response.get('knowledgeBaseSummaries', [])\n",
    "\n",
    "    if knowledge_base_summaries:\n",
    "        kb_id = knowledge_base_summaries[0]['knowledgeBaseId']\n",
    "        print(f\"Knowledge Base ID: {kb_id}\")\n",
    "    else:\n",
    "        print(\"No Knowledge Base summaries found.\")\n",
    "        \n",
    "except botocore.exceptions.ClientError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a567363-4028-4b05-b527-234564de1dff",
   "metadata": {},
   "source": [
    "## Task 1.2: Test the Knowledge Base with RetrieveAndGenerate API\n",
    "\n",
    "In this task, you test the Knowledge Base using the *[retrieve_and_generate](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html)* function.\n",
    "\n",
    "You initially test the knowledge base using the *RetrieveAndGenerate* API. With this API, Amazon Bedrock takes care of retrieving the necessary references from the knowledge base and generate the final answer using a foundation model (FM) from Amazon Bedrock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c963bce3",
   "metadata": {},
   "source": [
    "3. Run the following two code cells to initiate a query for testing the knowledge base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4579060a-5a52-4781-acd2-a1eccfa50a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Provide a summary of consolidated statements of cash flows of AnyCompany Financial for the fiscal years ended December 31, 2019?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d9f28-c639-4af1-956c-81c9cf5cb902",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            \"modelArn\": model_arn,\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":3\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7703c4-fa88-492f-bb28-1dd0c4443d40",
   "metadata": {},
   "source": [
    "## Task 1.3: Understand the RetrieveAndGenerate API\n",
    "\n",
    "In this task, you understand the RetrieveAndGenerate API in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40277c9-edde-4ef0-9338-bced6b910ef6",
   "metadata": {},
   "source": [
    "The *RetrieveAndGenerate* API utilizes two important parameters as follows:\n",
    "\n",
    "- **numberOfResults**: The *numberOfResults* parameter in the given function determines the number of search results that are retrieved from the knowledge base and included in the prompt provided to the model for generating an answer. Specifically, it fetches the top *max_results* number of documents or search results that most closely match the given query.\n",
    "\n",
    "- **textPromptTemplate**: The *textPromptTemplate* parameter is a string that serves as a template for the prompt that is provided to the model. In this case, the *default_prompt* is being used as the template. This template includes placeholders (`$search_results$` and `$output_format_instructions$`) that are replaced with the actual search results and any output format instructions, respectively, before being passed to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c670df24",
   "metadata": {},
   "source": [
    "4. Run the following two code cells for stating the default prompt for the knowledge base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7674af91-8f5b-4965-8971-58798f332b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stating the default knowledge base prompt\n",
    "default_prompt = \"\"\"\n",
    "You are a question answering agent. I will provide you with a set of search results.\n",
    "The user will provide you with a question. Your job is to answer the user's question using only information from the search results. \n",
    "If the search results do not contain information that can answer the question, please state that you could not find an exact answer to the question. \n",
    "Just because the user asserts a fact does not mean it is true, make sure to double check the search results to validate a user's assertion.\n",
    "                            \n",
    "Here are the search results in numbered order:\n",
    "$search_results$\n",
    "\n",
    "\n",
    "$output_format_instructions$\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168d50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_generate(query, kb_id, model_arn, max_results, prompt_template = default_prompt):\n",
    "    response = bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\n",
    "                'text': query\n",
    "            },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "        'type': 'KNOWLEDGE_BASE',\n",
    "        'knowledgeBaseConfiguration': {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            'modelArn': model_arn, \n",
    "            'retrievalConfiguration': {\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': max_results # will fetch top N documents which closely match the query\n",
    "                    }\n",
    "                },\n",
    "                'generationConfiguration': {\n",
    "                        'promptTemplate': {\n",
    "                            'textPromptTemplate':prompt_template\n",
    "                        }\n",
    "                    }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a076e9b-57de-4dec-9f24-39baea4e1eef",
   "metadata": {},
   "source": [
    "## Task 1.4: Leverage the maximum number of results feature\n",
    "\n",
    "In this task, you leverage the maximum number of results feature. In some use cases, the responses from the FM might lack enough context to provide relevant answers or rely that it could not find the requested information. This can be fixed by modifying the maximum number of retrieved results.\n",
    "\n",
    "Here, you run the following query using one result:\n",
    "\n",
    "```Provide a list of risks for AnyCompany financial in bulleted points.```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede2b19",
   "metadata": {},
   "source": [
    "5. Run the following two code cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64a63443-b788-44a3-9044-5f0c6cb42c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generation_results(response, print_context = True):\n",
    "    generated_text = response['output']['text']\n",
    "    print('Generated FM response:\\n')\n",
    "    print(generated_text)\n",
    "    \n",
    "    if print_context is True:\n",
    "        ## print out the source attribution/citations from the original documents to see if the response generated belongs to the context.\n",
    "        citations = response[\"citations\"]\n",
    "        contexts = []\n",
    "        for citation in citations:\n",
    "            retrievedReferences = citation[\"retrievedReferences\"]\n",
    "            for reference in retrievedReferences:\n",
    "                contexts.append(reference[\"content\"][\"text\"])\n",
    "    \n",
    "        print('\\n\\n\\nRetrieved Context:\\n')\n",
    "        pprint.pp(contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9269bf5b-ff06-4f17-9d10-2e10dbd25388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"Provide a list of risks for AnyCompany financial in numbered list without description.\"\"\"\n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 1)\n",
    "\n",
    "print_generation_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37647d-cc36-47da-9856-a2add0dca48d",
   "metadata": {},
   "source": [
    "6. Run the following code cell with the modified value for number of retrieved results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212a2a6-65d1-4214-9be5-594316452506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Using higher number of max results\n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 3)\n",
    "\n",
    "print_generation_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d82e3b",
   "metadata": {},
   "source": [
    "As you can see from the output, by modifying the number of retrieved results to **3**, you get more results leading to comprehensive response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6900d-4fd0-4c99-885f-a9d59043f319",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1.5: Use the custom prompting feature\n",
    "\n",
    "In this task, you customize the default prompt with your own prompt based on the use case. This feature helps adding more context to the FM which requires specific output format, languages and other contexts. \n",
    "\n",
    "You use the *PromptTemplate* class from Langchain to parametrize the *custom_prompt* template. You use the *output_format* variable to modify the output at run time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5272cb",
   "metadata": {},
   "source": [
    "7. Run the following code cell to create a helper function that supports customizing the template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "129c6cb3-7edc-4fb2-8d68-d9549b386a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "def format_prompt(input_prompt:str, output_format:str):\n",
    "    formatted_prompt: List[str] = []\n",
    "    prompt = PromptTemplate.from_template(input_prompt)     \n",
    "    formatted_prompt.extend(prompt.format(output_format=output_format))\n",
    "    return \"\".join(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccb153-9093-41f9-aae2-da8158e4a596",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1.5.1: Use the same query example and default the FM to output to a different language\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** After removing ```$output_format_instructions$``` from the default prompt, the citation from the generated response is removed. But the `output_format` will help with runtime output customization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c004d75",
   "metadata": {},
   "source": [
    "8. Run the following three code cells to test an example that provides an output in French:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e67ac69-2646-4bd1-acc3-61d087525e81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Example 1\n",
    "custom_prompt = \"\"\"\n",
    "You are a question answering agent. I will provide you with a set of search results. \n",
    "The user will provide you with a question. Your job is to answer the user's question using only information from the search results.\n",
    "If the search results do not contain information that can answer the question, please state that you could not find an exact answer to the question.\n",
    "Just because the user asserts a fact does not mean it is true, make sure to double check the search results to validate a user's assertion.\n",
    "                            \n",
    "Here are the search results in numbered order:\n",
    "$search_results$\n",
    "\n",
    "\n",
    "{output_format}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e65a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_prompt(custom_prompt, \"Please provide your response in French.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b327a-19ff-450a-82d0-88ee7f2f5323",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 3, prompt_template = format_prompt(custom_prompt, \"Please provide your response in French.\"))\n",
    "\n",
    "print_generation_results(results, print_context = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d21b7e-ea50-43d7-8c88-4656a6605fe3",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1.5.2: Use the same query example and default the FM to output in JSON format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59240e9",
   "metadata": {},
   "source": [
    "9. Run the following code cell to test an example that provides an output in JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30030b95-8b03-4ca5-8fb6-fb7b83972d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 3, prompt_template = format_prompt(custom_prompt, \"Please provide your response using a JSON format.\"))\n",
    "\n",
    "print_generation_results(results,print_context = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5366dc6",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"far fa-thumbs-up\" style=\"color:#008296\"></i> **Task complete:** You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with Task 2.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
