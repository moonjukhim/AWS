### RAGAS - Evaluation

1.  "모델이 질문에 대해 얼마나 정확하고, 문맥에 충실하며, 근거를 잘 사용해 답했는가?”를 정량적으로 평가한 결과"
2.  핵심 RAG/LLM 품질 지표

    - 검색 평가 지표

      | 지표                  | 의미                                                   |
      | --------------------- | ------------------------------------------------------ |
      | context_precision     | 가져온 컨텍스트가 질문에 얼마나 **불필요한 게 없는지** |
      | context_recall        | 정답에 필요한 정보가 **빠짐없이** 포함됐는지           |
      | context_entity_recall | 핵심 엔티티(회사, 연도, 지표 등)가 포함됐는지          |

    - 생성 평가 지표

      | 지표               | 의미                                   |
      | ------------------ | -------------------------------------- |
      | faithfulness       | 컨텍스트에 근거해 답했는가 (환각 여부) |
      | answer_relevancy   | 질문에 직접 답했는가                   |
      | answer_similarity  | 정답과 의미적으로 얼마나 유사한가      |
      | answer_correctness | 사실적으로 맞는가                      |

    - Critique (품질/안전)

      | 지표                        | 의미                               |
      | --------------------------- | ---------------------------------- |
      | harmfulness / maliciousness | 유해·악의적 여부 (0이면 문제 없음) |
      | coherence                   | 문맥적으로 자연스러운가            |
      | correctness                 | 논리적으로 타당한가                |
      | conciseness                 | 불필요하게 장황하지 않은가         |

3.  표의 핵심 인사이트

    - 1. 숫자 관련 환각이 반복됨

      - faithfulness 낮은 row 대부분이 금액·증가율
      - 해결책:
        - 숫자 전용 chunk
        - 계산은 LLM이 아니라 코드로 분리

    - 2. 검색은 잘 됐는데 생성이 문제인 경우가 많음

      - context_precision / recall은 대부분 1.0
      - → 문제는 Retriever가 아니라 Generator

    - 3. 추론 질문일수록 relevancy 하락

      - Row 4처럼 “추론/평가” 질문
      - LLM이 컨텍스트 밖 상식·홍보 문구 사용

    - 4. 요약
      - 이 결과는 검색(RAG)은 대체로 잘 동작하지만, LLM이 숫자·추론 질문에서 컨텍스트를 벗어나 환각을 일으키는 문제 발생

4.  개선 방법

    - 숫자/계산 질문 → Tool 호출로 분리
    - faithfulness 낮은 row 자동 알람
    - 추론 질문은 “컨텍스트 기반으로만 답하라” 프롬프트 강화
    - answer_relevancy=0 사례는 즉시 거절

5.  권장 방식

    | 질문 유형 | 권장 평가                            |
    | --------- | ------------------------------------ |
    | Fact QA   | faithfulness, context_precision 중시 |
    | 계산      | answer_correctness 중시              |
    | 해석/전망 | coherence, answer_relevancy 중심     |
    | 혼합형    | 질문 타입별 가중치 분리              |

---

| index | faithfulness | answer_relevancy | context_precision | context_recall | context_entity_recall | answer_similarity | answer_correctness | harmfulness | maliciousness | coherence | correctness | conciseness |
| ----: | -----------: | ---------------: | ----------------: | -------------: | --------------------: | ----------------: | -----------------: | ----------: | ------------: | --------: | ----------: | ----------: |
|     0 |  0.333333333 |      0.968512499 |                 1 |              1 |           0.999999998 |       0.589110577 |        0.747277644 |           0 |             0 |         1 |           1 |           1 |
|     1 |          0.8 |      0.829507367 |                 1 |              1 |           0.222222222 |       0.976249606 |        0.619062402 |           0 |             0 |         1 |           1 |           1 |
|     2 |            1 |      0.995492862 |                 1 |              1 |           0.499999999 |       0.931829394 |        0.795457349 |           0 |             1 |         1 |           1 |           1 |
|     3 |            1 |      0.851156444 |                 1 |              1 |           0.714285713 |       0.819253634 |        0.354813409 |           0 |             1 |         1 |           1 |           1 |
|     4 |  0.545454545 |      0.947786515 |                 0 |            0.4 |           0.111111111 |       0.890207197 |        0.584620765 |           0 |             1 |         1 |           1 |           1 |

답변은 대부분 “정확하고 잘 쓰였지만”,
컨텍스트를 ‘정확히 집어서’ 쓰는 능력은 질문 유형에 따라 크게 갈린다.

1. 질문 유형별

- Fact-based 질문 (row 0~2)
  - correctness / relevancy / similarity 높음
  - faithfulness는 “얼마나 말을 보탰느냐”에 따라 변동
- 계산형 질문 (row 3)
  - 모든 지표가 매우 안정적
    -👉 RAG + LLM이 가장 잘하는 유형
- 해석/추론 질문 (row 4)
  - 답변 품질은 좋음
  - RAG 메트릭은 낮음
  - 👉 평가 기준과 질문 유형의 미스매치

---
