{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87337121",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Ensure docker is installed and running (https://docs.docker.com/get-docker/)\n",
    "2. pip install -qU langchain_postgres\n",
    "3. Run the following command to start the postgres container:\n",
    "   \n",
    "docker run \\\n",
    "    --name pgvector-container \\\n",
    "    -e POSTGRES_USER=langchain \\\n",
    "    -e POSTGRES_PASSWORD=langchain \\\n",
    "    -e POSTGRES_DB=langchain \\\n",
    "    -p 6024:5432 \\\n",
    "    -d pgvector/pgvector:pg16\n",
    "4. Use the connection string below for the postgres container\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "# See docker command above to launch a postgres instance with pgvector enabled.\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "\n",
    "# Load the document, split it into chunks\n",
    "raw_documents = TextLoader('./test.txt', encoding='utf-8').load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# Create embeddings for the documents\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "db = PGVector.from_documents(\n",
    "    documents, embeddings_model, connection=connection)\n",
    "\n",
    "# create retriever to retrieve 2 relevant documents\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "query = 'Who are the key figures in the ancient greek history of philosophy?'\n",
    "\n",
    "# fetch relevant documents\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "print(docs[0].page_content)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\"\n",
    ")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm_chain = prompt | llm\n",
    "\n",
    "# answer the question based on relevant documents\n",
    "result = llm_chain.invoke({\"context\": docs, \"question\": query})\n",
    "\n",
    "print(result)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Run again but this time encapsulate the logic for efficiency\n",
    "\n",
    "# @chain decorator transforms this function into a LangChain runnable,\n",
    "# making it compatible with LangChain's chain operations and pipeline\n",
    "\n",
    "print(\"Running again but this time encapsulate the logic for efficiency\\n\")\n",
    "\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "# run it\n",
    "result = qa.invoke(query)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e656f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Ensure docker is installed and running (https://docs.docker.com/get-docker/)\n",
    "2. pip install -qU langchain_postgres\n",
    "3. Run the following command to start the postgres container:\n",
    "   \n",
    "docker run \\\n",
    "    --name pgvector-container \\\n",
    "    -e POSTGRES_USER=langchain \\\n",
    "    -e POSTGRES_PASSWORD=langchain \\\n",
    "    -e POSTGRES_DB=langchain \\\n",
    "    -p 6024:5432 \\\n",
    "    -d pgvector/pgvector:pg16\n",
    "4. Use the connection string below for the postgres container\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "# See docker command above to launch a postgres instance with pgvector enabled.\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "\n",
    "# Load the document, split it into chunks\n",
    "raw_documents = TextLoader('./test.txt', encoding='utf-8').load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# Create embeddings for the documents\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "db = PGVector.from_documents(\n",
    "    documents, embeddings_model, connection=connection)\n",
    "\n",
    "# create retriever to retrieve 2 relevant documents\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Query starts with irrelevant information before asking the relevant question\n",
    "query = 'Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker. Who are some key figures in the ancient greek history of philosophy?'\n",
    "\n",
    "# fetch relevant documents\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "print(docs[0].page_content)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\"\n",
    ")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "# Run again but this time encapsulate the logic for efficiency\n",
    "\n",
    "# @chain decorator transforms this function into a LangChain runnable,\n",
    "# making it compatible with LangChain's chain operations and pipeline\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "# run it\n",
    "result = qa.invoke(query)\n",
    "print(result.content)\n",
    "\n",
    "print(\"\\nRewrite the query to improve accuracy\\n\")\n",
    "\n",
    "rewrite_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Provide a better search query for web search engine to answer the given question, end the queries with ’**’. Question: {x} Answer:\"\"\")\n",
    "\n",
    "\n",
    "def parse_rewriter_output(message):\n",
    "    return message.content.strip('\"').strip(\"**\")\n",
    "\n",
    "\n",
    "rewriter = rewrite_prompt | llm | parse_rewriter_output\n",
    "\n",
    "\n",
    "@chain\n",
    "def qa_rrr(input):\n",
    "    # rewrite the query\n",
    "    new_query = rewriter.invoke(input)\n",
    "    print(\"Rewritten query: \", new_query)\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(new_query)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "print(\"\\nCall model again with rewritten query\\n\")\n",
    "\n",
    "# call model again with rewritten query\n",
    "result = qa_rrr.invoke(query)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34229925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "# See docker command above to launch a postgres instance with pgvector enabled.\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "\n",
    "# Load the document, split it into chunks\n",
    "raw_documents = TextLoader('./test.txt', encoding='utf-8').load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# Create embeddings for the documents\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "db = PGVector.from_documents(\n",
    "    documents, embeddings_model, connection=connection)\n",
    "\n",
    "# create retriever to retrieve 2 relevant documents\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# instruction to generate multiple queries\n",
    "perspectives_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. \n",
    "    By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based  similarity search. \n",
    "    Provide these alternative questions separated by newlines. \n",
    "    Original question: {question}\"\"\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split('\\n')\n",
    "\n",
    "\n",
    "query_gen = perspectives_prompt | llm | parse_queries_output\n",
    "\n",
    "\n",
    "def get_unique_union(document_lists):\n",
    "    # Flatten list of lists, and dedupe them\n",
    "    deduped_docs = {\n",
    "        doc.page_content: doc for sublist in document_lists for doc in sublist}\n",
    "    # return a flat list of unique docs\n",
    "    return list(deduped_docs.values())\n",
    "\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | get_unique_union\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\"\n",
    ")\n",
    "\n",
    "query = \"Who are the key figures in the ancient greek history of philosophy?\"\n",
    "\n",
    "\n",
    "@chain\n",
    "def multi_query_qa(input):\n",
    "    # fetch relevant documents\n",
    "    docs = retrieval_chain.invoke(input)  # format prompt\n",
    "    formatted = prompt.invoke(\n",
    "        {\"context\": docs, \"question\": input})  # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "# run\n",
    "print(\"Running multi query qa\\n\")\n",
    "result = multi_query_qa.invoke(query)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "# See docker command above to launch a postgres instance with pgvector enabled.\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "\n",
    "# Load the document, split it into chunks\n",
    "raw_documents = TextLoader('./test.txt', encoding='utf-8').load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# Create embeddings for the documents\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "db = PGVector.from_documents(\n",
    "    documents, embeddings_model, connection=connection)\n",
    "\n",
    "# create retriever to retrieve 2 relevant documents\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n Generate multiple search queries related to: {question} \\n Output (4 queries):\"\"\")\n",
    "\n",
    "\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split('\\n')\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "query_gen = prompt_rag_fusion | llm | parse_queries_output\n",
    "\n",
    "query = \"Who are the key figures in the ancient greek history of philosophy?\"\n",
    "\n",
    "generated_queries = query_gen.invoke(query)\n",
    "\n",
    "print(\"generated queries: \", generated_queries)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "we fetch relevant documents for each query and pass them into a function to rerank (that is, reorder according to relevancy) the final list of relevant documents.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"reciprocal rank fusion on multiple lists of ranked documents and an optional parameter k used in the RRF formula\"\"\"\n",
    "    # Initialize a dictionary to hold fused scores for each document\n",
    "    # Documents will be keyed by their contents to ensure uniqueness\n",
    "    fused_scores = {}\n",
    "    documents = {}\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = doc.page_content\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "                documents[doc_str] = doc\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    # sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_doc_strs = sorted(\n",
    "        fused_scores, key=lambda d: fused_scores[d], reverse=True)\n",
    "    return [documents[doc_str] for doc_str in reranked_doc_strs]\n",
    "\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | reciprocal_rank_fusion\n",
    "\n",
    "result = retrieval_chain.invoke(query)\n",
    "\n",
    "print(\"retrieved context using rank fusion: \", result[0].page_content)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Use model to answer question based on retrieved docs\\n\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\"\n",
    ")\n",
    "\n",
    "query = \"Who are the some important yet not well known philosophers in the ancient greek history of philosophy?\"\n",
    "\n",
    "\n",
    "@chain\n",
    "def rag_fusion(input):\n",
    "    # fetch relevant documents\n",
    "    docs = retrieval_chain.invoke(input)  # format prompt\n",
    "    formatted = prompt.invoke(\n",
    "        {\"context\": docs, \"question\": input})  # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "# run\n",
    "print(\"Running rag fusion\\n\")\n",
    "result = rag_fusion.invoke(query)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4612d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# See docker command above to launch a postgres instance with pgvector enabled.\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "\n",
    "# Load the document, split it into chunks\n",
    "raw_documents = TextLoader('./test.txt', encoding='utf-8').load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# Create embeddings for the documents\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "db = PGVector.from_documents(\n",
    "    documents, embeddings_model, connection=connection)\n",
    "\n",
    "# create retriever to retrieve 2 relevant documents\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Please write a passage to answer the question.\\n Question: {question} \\n Passage:\"\"\")\n",
    "\n",
    "generate_doc = (prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser())\n",
    "\n",
    "\"\"\"\n",
    "Next, we take the hypothetical document generated above and use it as input to the retriever, \n",
    "which will generate its embedding and search for similar documents in the vector store:\n",
    "\"\"\"\n",
    "retrieval_chain = generate_doc | retriever\n",
    "\n",
    "query = \"Who are some lesser known philosophers in the ancient greek history of philosophy?\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    # fetch relevant documents from the hyde retrieval chain defined earlier\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "print(\"Running hyde\\n\")\n",
    "result = qa.invoke(query)\n",
    "print(\"\\n\\n\")\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "# Data model class\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question, choose which datasource would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Prompt template\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\"\"\"\n",
    "with_structured_output: Model wrapper that returns outputs formatted to match the given schema.\n",
    "\n",
    "\"\"\"\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source. Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system), (\"human\", \"{question}\")]\n",
    ")\n",
    "\n",
    "# Define router\n",
    "router = prompt | structured_llm\n",
    "\n",
    "# Run\n",
    "question = \"\"\"Why doesn't the following code work: \n",
    "from langchain_core.prompts \n",
    "import ChatPromptTemplate \n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"]) \n",
    "prompt.invoke(\"french\") \"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})\n",
    "print(\"\\nRouting to: \", result)\n",
    "\n",
    "\"\"\"\n",
    "Once we extracted the relevant data source, we can pass the value into another function to execute additional logic as required:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        return \"chain for python_docs\"\n",
    "    else:\n",
    "        return \"chain for js_docs\"\n",
    "\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)\n",
    "\n",
    "result = full_chain.invoke({\"question\": question})\n",
    "print(\"\\nChoose route: \", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5cdc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "physics_template = \"\"\"You are a very smart physics professor. You are great at     answering questions about physics in a concise and easy-to-understand manner.     When you don't know the answer to a question, you admit that you don't know. Here is a question: {query}\"\"\"\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering     math questions. You are so good because you are able to break down hard     problems into their component parts, answer the component parts, and then     put them together to answer the broader question. Here is a question: {query}\"\"\"\n",
    "\n",
    "# Embed prompts\n",
    "embeddings = OpenAIEmbeddings()\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "# Route question to prompt\n",
    "\n",
    "\n",
    "@chain\n",
    "def prompt_router(query):\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "semantic_router = (prompt_router | ChatOpenAI() | StrOutputParser())\n",
    "\n",
    "result = semantic_router.invoke(\"What's a black hole\")\n",
    "print(\"\\nSemantic router result: \", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lark\n",
    "\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# See docker command above to launch a postgres instance with pgvector enabled.\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create embeddings for the documents\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = PGVector.from_documents(\n",
    "    docs, embeddings_model, connection=connection)\n",
    "\n",
    "# Define the fields for the query\n",
    "fields = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\",\n",
    "        description=\"A 1-10 rating for the movie\",\n",
    "        type=\"float\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "description = \"Brief summary of a movie\"\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(llm, vectorstore, description, fields)\n",
    "\n",
    "# This example only specifies a filter\n",
    "print(retriever.invoke(\"I want to watch a movie rated higher than 8.5\"))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# This example specifies multiple filters\n",
    "print(retriever.invoke(\n",
    "    \"What's a highly rated (above 8.5) science fiction film?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854303c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The below example will use a SQLite connection with the Chinook database, which is a sample database that represents a digital media store. Follow these installation steps to create Chinook.db in the same directory as this notebook. You can also download and build the database via the command line:\n",
    "\n",
    "```bash\n",
    "curl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 Chinook.db\n",
    "\n",
    "```\n",
    "\n",
    "Afterwards, place `Chinook.db` in the same directory where this code is running.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.tools import QuerySQLDatabaseTool\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain.chains import create_sql_query_chain\n",
    "# replace this with the connection details of your db\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "print(db.get_usable_table_names())\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# convert question to sql query\n",
    "write_query = create_sql_query_chain(llm, db)\n",
    "\n",
    "# Execute SQL query\n",
    "execute_query = QuerySQLDatabaseTool(db=db)\n",
    "\n",
    "# combined chain = write_query | execute_query\n",
    "combined_chain = write_query | execute_query\n",
    "\n",
    "# run the chain\n",
    "result = combined_chain.invoke({\"question\": \"How many employees are there?\"})\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
