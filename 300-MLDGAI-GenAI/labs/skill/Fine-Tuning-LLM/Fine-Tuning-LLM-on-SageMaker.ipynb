{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6cc3d8-8da5-42c5-b055-e8fe3a47b2d9",
   "metadata": {},
   "source": [
    "## <a name=\"0\">Amazon SageMakerì—ì„œ LLM ë¯¸ì„¸ ì¡°ì •</a>\n",
    "\n",
    "ì´ ì†”ë£¨ì…˜ì—ì„œëŠ” ìƒì„±í˜• ì¸ê³µì§€ëŠ¥(AI) ë¶„ì•¼ì—ì„œ ê°•ë ¥í•œ ê¸°ìˆ ì¸ ì‚¬ì „ í•™ìŠµëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë´…ë‹ˆë‹¤. LLMì€ ë°©ëŒ€í•œ ì–‘ì˜ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµë˜ì–´ ì–¸ì–´ì˜ ë¯¸ë¬˜í•œ ë‰˜ì•™ìŠ¤ë¥¼ íŒŒì•…í•˜ê³  ì¼ê´€ì„± ìˆëŠ” ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ë§¤ìš° íš¨ê³¼ì ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì€ ë°ì´í„°ì—ì„œ ìœ ìš©í•œ íŠ¹ì§•ê³¼ íŒ¨í„´ì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì— ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹(ML) ì‘ì—…ì— ë§¤ìš° ìœ ìš©í•œ ìì›ì…ë‹ˆë‹¤.\n",
    "\n",
    "**ì „ì´ í•™ìŠµ**ì´ë¼ê³ ë„ í•˜ëŠ” ë¯¸ì„¸ ì¡°ì •ì„ í†µí•´ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì´ ì–»ì€ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ê´€ë ¨ì€ ìˆì§€ë§Œ ë‹¤ë¥¸ ì‘ì—…ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµí•˜ëŠ” ëŒ€ì‹  ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì—ì„œ ì‹œì‘í•˜ì—¬ íŠ¹ì • ë¬¸ì œ ì˜ì—­ì— ë§ê²Œ ìˆ˜ì •í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ìƒë‹¹í•œ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë¥¼ ì ˆì•½í•˜ê³  ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í™œìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë‹¨ê³„ë³„ ê³¼ì •ì„ ì•ˆë‚´í•©ë‹ˆë‹¤. ì£¼ìš” ë‹¨ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "1. <a href=\"#step1\">GPU ë©”ëª¨ë¦¬ í™•ì¸</a>\n",
    "2. <a href=\"#step2\">ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°</a>\n",
    "3. <a href=\"#step3\">í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„</a>\n",
    "4. <a href=\"#step4\">ì‚¬ì „ í•™ìŠµëœ LLM ë¶ˆëŸ¬ì˜¤ê¸°</a>\n",
    "5. <a href=\"#step5\">íŠ¸ë ˆì´ë„ˆ ì •ì˜ ë° LLM ë¯¸ì„¸ ì¡°ì •</a>\n",
    "6. <a href=\"#step6\">ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ ë°°í¬</a>\n",
    "7. <a href=\"#step7\">ë°°í¬ëœ ì¶”ë¡  í…ŒìŠ¤íŠ¸</a>\n",
    "\n",
    "ì°¸ê³ : ì´ ë…¸íŠ¸ë¶ì˜ ìœ„ìª½ë¶€í„° ì•„ë˜ìª½ìœ¼ë¡œ ìˆœì„œëŒ€ë¡œ ì§„í–‰í•˜ê³ , ê° ì„¹ì…˜ì„ ê±´ë„ˆë›°ì§€ ë§ˆì‹­ì‹œì˜¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì½”ë“œ ëˆ„ë½ìœ¼ë¡œ ì¸í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882540e-68c5-4536-b5b8-90c22439b0df",
   "metadata": {},
   "source": [
    "## <a name=\"step1\">Step 1: Check GPU memory</a>\n",
    "\n",
    "To check the GPU memory, run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "635f5377-d55b-45d4-b9fd-92394eaa2b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 12 05:49:17 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   23C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f226fdc-8694-4581-b8fd-fdf21a7d1589",
   "metadata": {},
   "source": [
    "If your CUDA memory is occupied by more than half (as in the following image), you need to shut down other running notebooks.\n",
    "\n",
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/memory.png\" alt=\"drawing\" width=\"800\"/> <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafdbd52-d0b1-428e-8955-8da5370ccdbd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a name=\"step2\">Step 2: Import libraries</a>\n",
    "\n",
    "Run the following two code blocks sequentially, one at a time, to import the necessary libraries, including the Hugging Face Transformers library and the PyTorch library, which is a dependency for Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9473ffd8-877d-4d08-bb4f-f446bd95f126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05425028-4ec9-4abd-8ec1-a94395cc8678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore all warnings\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "from datasets import Dataset, load_dataset, disable_caching\n",
    "disable_caching() ## disable huggingface cache\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ab9fd-3095-4870-9001-71767da81d81",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a name=\"step3\">Step 3: Prepare the training dataset</a>\n",
    "\n",
    "Load and view the dataset. For this practice lab, you use [Amazon SageMaker FAQs](https://aws.amazon.com/sagemaker/faqs/) for the main dataset, which has two columns: `instruction` and `response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a352ed-3da7-4e8d-8981-5f9b23478215",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1cd45153ed437f814cf806776dbb10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'response'],\n",
       "    num_rows: 154\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_faqs_dataset = load_dataset(\"csv\",\n",
    "                                      data_files='data/amazon_sagemaker_faqs.csv')['train']\n",
    "sagemaker_faqs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "518fc5c4-e448-4e55-9379-bddbbdcc7fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What is Amazon SageMaker?',\n",
       " 'response': 'Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_faqs_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fcb4ed-346e-42a1-ae66-2a0fd5b57a2b",
   "metadata": {},
   "source": [
    "### <a name=\"step3\">3.1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ì¤€ë¹„</a>\n",
    "\n",
    "LLMì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ ì•„ë˜ì™€ ê°™ì´ ëª…ë ¹ì–´ ë°ì´í„°ì…‹ì— PROMPTë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4755f14f-61da-483f-91ce-748fa968bbef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
       "            ### Instruction:\n",
       "            {instruction}\n",
       "            ### Response:\n",
       "            {response}\n",
       "            ### End"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.helpers import INTRO_BLURB, INSTRUCTION_KEY, RESPONSE_KEY, END_KEY, RESPONSE_KEY_NL, DEFAULT_SEED, PROMPT\n",
    "'''\n",
    "PROMPT = \"\"\"{intro}\n",
    "            {instruction_key}\n",
    "            {instruction}\n",
    "            {response_key}\n",
    "            {response}\n",
    "            {end_key}\"\"\"\n",
    "'''\n",
    "Markdown(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e960228c-446a-429a-b6a1-473f6965293b",
   "metadata": {},
   "source": [
    "\n",
    "ì´ì œ `_add_text` íŒŒì´ì¬ í•¨ìˆ˜ë¥¼ í†µí•´ í”„ë¡¬í”„íŠ¸ë¥¼ ë°ì´í„°ì…‹ì— ì¶”ê°€í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ë ˆì½”ë“œë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìœ¼ë©°, ë‘ í•„ë“œ(ì§€ì‹œ/ì‘ë‹µ)ê°€ ëª¨ë‘ nullì´ ì•„ë‹Œì§€ í™•ì¸í•œ í›„, í•´ë‹¹ ê°’ì„ ë¯¸ë¦¬ ì •ì˜ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì „ë‹¬í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "013cbcde-2237-4dff-ab77-9b1b9cb10d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _add_text(rec):\n",
    "    instruction = rec[\"instruction\"]\n",
    "    response = rec[\"response\"]\n",
    "\n",
    "    if not instruction:\n",
    "        raise ValueError(f\"Expected an instruction in: {rec}\")\n",
    "\n",
    "    if not response:\n",
    "        raise ValueError(f\"Expected a response in: {rec}\")\n",
    "\n",
    "    rec[\"text\"] = PROMPT.format(\n",
    "        instruction=instruction, response=response)\n",
    "\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df0aea04-6e06-4879-8eb9-c1fb16483fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66556e32bf1b47f2a59ea851bec8ffb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What is Amazon SageMaker?',\n",
       " 'response': 'Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.',\n",
       " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n            ### Instruction:\\n            What is Amazon SageMaker?\\n            ### Response:\\n            Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\\n            ### End'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_faqs_dataset = sagemaker_faqs_dataset.map(_add_text)\n",
    "sagemaker_faqs_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13a4cd-e013-4e68-bbf2-2aebc1d5a133",
   "metadata": {},
   "source": [
    "Use `Markdown` to neatly display the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "945c4f1d-2e3f-4b45-bec7-1f03612b1552",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
       "            ### Instruction:\n",
       "            What is Amazon SageMaker?\n",
       "            ### Response:\n",
       "            Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n",
       "            ### End"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(sagemaker_faqs_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d83c07-93ae-48ea-a611-a6df6b44965c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "## <a name=\"#step4\">4ë‹¨ê³„: ì‚¬ì „ í•™ìŠµëœ LLM ë¡œë“œ</a>\n",
    "\n",
    "ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´ Hugging Face Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `databricks/dolly-v2-3b` ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì €ì™€ ê¸°ë³¸ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ëŠ” ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ê¸°ë³¸ ëª¨ë¸ì€ ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì•ì„œ ì„¤ëª…í•œ ì§€ì¹¨ì„ ë”°ë¥´ë©´ ì´ëŸ¬í•œ êµ¬ì„± ìš”ì†Œë¥¼ ì˜¬ë°”ë¥´ê²Œ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ì½”ë“œì—ì„œ í•´ë‹¹ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "í† í¬ë‚˜ì´ì €ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ë ¤ë©´ `AutoTokenizer.from_pretrained()` Python í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `padding_side=\"left\"`ëŠ” ì‹œí€€ìŠ¤ì— íŒ¨ë”© í† í°ì´ ì¶”ê°€ë˜ëŠ” ìœ„ì¹˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ì´ ê²½ìš° íŒ¨ë”© í† í°ì€ ê° ì‹œí€€ìŠ¤ì˜ ì™¼ìª½ì— ì¶”ê°€ë©ë‹ˆë‹¤.\n",
    "\n",
    "- `eos_token`ì€ ì‹œí€€ìŠ¤ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ìˆ˜ í† í°ì…ë‹ˆë‹¤. í† í°ì„ `pad_token`ì— í• ë‹¹í•˜ë©´ í† í°í™” ì¤‘ì— ì¶”ê°€ë˜ëŠ” ëª¨ë“  íŒ¨ë”© í† í°ë„ ì‹œí€€ìŠ¤ ë í† í°ìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì„ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ë•Œ ìœ ìš©í•  ìˆ˜ ìˆëŠ”ë°, ëª¨ë¸ì€ íŒ¨ë”© í† í°ì„ ë§Œë‚˜ë©´ í…ìŠ¤íŠ¸ ìƒì„±ì„ ì¤‘ì§€í•´ì•¼ í•  ì‹œì ì„ ì•Œ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "- `tokenizer.add_special_tokens...` í•¨ìˆ˜ëŠ” í† í¬ë‚˜ì´ì €ì˜ ì–´íœ˜ì— ì„¸ ê°€ì§€ íŠ¹ìˆ˜ í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í† í°ì€ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ íŠ¹ì • ëª©ì ì„ ìˆ˜í–‰í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ëŸ¬í•œ í† í°ì€ ëŒ€í™” ì‹œìŠ¤í…œì—ì„œ ì…ë ¥, ëª…ë ¹ ë˜ëŠ” ì‘ë‹µì˜ ëì„ í‘œì‹œí•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ í•¨ìˆ˜ê°€ ì‹¤í–‰ë˜ë©´ `tokenizer` ê°ì²´ê°€ ì´ˆê¸°í™”ë˜ì–´ í…ìŠ¤íŠ¸ í† í°í™”ì— ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dbd90bc-74f8-4d91-b70d-81f469483958",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5d68824ef64ee18011e0606d0f4886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620331ef8d484a8cb903c99156f624f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b28d8ad50d4578948806c0c57f4a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"verseAI/databricks-dolly-v2-3b\",\n",
    "                                          padding_side=\"left\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\":\n",
    "                              [END_KEY, INSTRUCTION_KEY, RESPONSE_KEY_NL]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcd4d817-2008-42c2-84d5-13867c8053cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919e0c80869c4c53bb62a8e91a008a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4d34a77f2f4942bcbc3fd59af00ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"verseAI/databricks-dolly-v2-3b\",\n",
    "    # use_cache=False,\n",
    "    device_map=\"auto\", #\"balanced\",\n",
    "    load_in_8bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d9354-c68b-4637-8985-8f781fadfae0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    " \n",
    "### <a name=\"#step4.1\">4.1ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ì¤€ë¹„</a>\n",
    "ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ ë¯¸ì„¸ ì¡°ì •(PEFT)ì„ ì‚¬ìš©í•˜ì—¬ INT8 ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ì „ì— ëª‡ ê°€ì§€ ì „ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ `prepare_model_for_int8_training`ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "\n",
    "- ì•ˆì •ì„±ì„ ìœ„í•´ ëª¨ë“  ë¹„ INT8 ëª¨ë“ˆì„ ì „ì²´ ì •ë°€ë„(FP32)ë¡œ ìºìŠ¤íŒ…í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ì…ë ¥ ì€ë‹‰ ìƒíƒœì˜ ê¸°ìš¸ê¸° ê³„ì‚°ì„ í™œì„±í™”í•˜ê¸° ìœ„í•´ ì…ë ¥ ì„ë² ë”© ë ˆì´ì–´ì— forward_hookì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ„í•´ ê¸°ìš¸ê¸° ì²´í¬í¬ì¸íŒ…ì„ í™œì„±í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df096cfc-4c37-4155-bb77-e2c7164e3bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50281, 2560)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc7ea0-89b3-4164-a8d5-3ced6179645f",
   "metadata": {},
   "source": [
    "Use the `preprocess_batch` function to preprocess the text field of the batch, applying tokenization, truncation, and other relevant operations based on the specified maximum length. The field takes a batch of data, a tokenizer, and a maximum length as input.\n",
    "\n",
    "For more details, refer to `mlu_utils/helpers.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9de28ac-e6cb-46f3-8410-8d82923b1270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from utils.helpers import mlu_preprocess_batch\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "_preprocessing_function = partial(mlu_preprocess_batch, max_length=MAX_LENGTH, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a47b1-03c4-45b9-82b6-14bbf3c46bd0",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ, ë°ì´í„°ì…‹ì˜ ê° ë°°ì¹˜ì— ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í•„ë“œë¥¼ ì ì ˆíˆ ìˆ˜ì •í•©ë‹ˆë‹¤. ë§¤í•‘ ì‘ì—…ì€ ë°°ì¹˜ ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ë˜ë©°, ì§€ì‹œì‚¬í•­, ì‘ë‹µ ë° í…ìŠ¤íŠ¸ ì—´ì´ ë°ì´í„°ì…‹ì—ì„œ ì œê±°ë©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, `sagemaker_faqs_dataset`ì—ì„œ `input_ids` í•„ë“œì˜ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§í•˜ì—¬ ì§€ì •ëœ `MAX_LENGTH` ë‚´ì— ë§ë„ë¡ í•˜ì—¬ `processed_dataset`ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "SageMaker FAQ ë°ì´í„°ì…‹ì„ ëª¨ë¸ í•™ìŠµìš©ìœ¼ë¡œ ì „ì²˜ë¦¬(tokenize)í•œ ë’¤, ê¸¸ì´ê°€ ë„ˆë¬´ ê¸´ ìƒ˜í”Œì„ ì œê±°í•˜ëŠ” ë‹¨ê³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae5cd938-efa0-4fe7-b24d-a5e2cdf3cf9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d649cadaf1f34ed1ac9ed772471372da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1b67d22bb94292b85e37a9b36df486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_sagemaker_faqs_dataset = sagemaker_faqs_dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"response\", \"text\"],\n",
    ")\n",
    "\n",
    "processed_dataset = encoded_sagemaker_faqs_dataset.filter(lambda rec: len(rec[\"input_ids\"]) < MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd86b08-e90e-4fad-9107-f9dcf63915ad",
   "metadata": {},
   "source": [
    "Split the dataset into `train` and `test` for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db5ba6bb-41a7-455d-a349-8219405eec98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 133\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 14\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset = processed_dataset.train_test_split(test_size=14, seed=0)\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b2da6-08d9-4a1d-bfa9-0e847e104ce9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## <a name=\"#step5\">5ë‹¨ê³„: íŠ¸ë ˆì´ë„ˆ ì •ì˜ ë° LLM ë¯¸ì„¸ ì¡°ì •</a>\n",
    "\n",
    "ì´ ì‹¤ìŠµì—ì„œëŠ” íš¨ìœ¨ì ì¸ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•´ [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. LoRAëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³ ì •í•˜ê³  í•™ìŠµ ê°€ëŠ¥í•œ ë­í¬ ë¶„í•´ í–‰ë ¬ì„ Transformer ì•„í‚¤í…ì²˜ì˜ ê° ë ˆì´ì–´ì— ì£¼ì…í•˜ì—¬ í•˜ìœ„ ì‘ì—…ì— í•„ìš”í•œ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤. Adamìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ GPT-3 175Bì™€ ë¹„êµí–ˆì„ ë•Œ, LoRAëŠ” í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ 10,000ë°° ì¤„ì´ê³  GPU ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ì„ 3ë°°ê¹Œì§€ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### <a name=\"#step5.1\">5.1ë‹¨ê³„: LoraConfig ì •ì˜ ë° LoRA ëª¨ë¸ ë¡œë“œ</a>\n",
    "\n",
    "[huggingface ğŸ¤— PEFT: ìµœì²¨ë‹¨ íŒŒë¼ë¯¸í„° íš¨ìœ¨ ë¯¸ì„¸ ì¡°ì •](https://github.com/huggingface/peft)ì˜ ë¹Œë“œëœ LoRA í´ë˜ìŠ¤ `LoraConfig`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. `LoraConfig` ë‚´ì—ì„œ ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `r`: ì €ì°¨ í–‰ë ¬ì˜ ì°¨ì›\n",
    "- `lora_alpha`: ì €ì°¨ í–‰ë ¬ì˜ ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜\n",
    "- `lora_dropout`: LoRA ë ˆì´ì–´ì˜ ë“œë¡­ì•„ì›ƒ í™•ë¥ \n",
    "\n",
    "---\n",
    "\n",
    "GPU ë©”ëª¨ë¦¬ë¥¼ ì•„ë¼ê¸° ìœ„í•´ ëª¨ë¸ì„ int8ë¡œ ì¤€ë¹„í•˜ê³ ,\n",
    "ëŒ€ë¶€ë¶„ì˜ ê°€ì¤‘ì¹˜ëŠ” ê³ ì •í•œ ì±„ ìƒìœ„ ì¼ë¶€ ë ˆì´ì–´ + LoRAë§Œ í•™ìŠµí•˜ëŠ”\n",
    "ê³ íš¨ìœ¨ ë¯¸ì„¸ì¡°ì • íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "- ëª¨ë¸ì˜ ê°€ì¥ ìœ„ìª½ ë ˆì´ì–´ 2ê°œë§Œ í•™ìŠµ í—ˆìš©\n",
    "- í•˜ìœ„ ë ˆì´ì–´:\n",
    "    - ì–¸ì–´ì˜ ê¸°ë³¸ ë¬¸ë²•, í† í° í†µê³„\n",
    "- ìƒìœ„ ë ˆì´ì–´:\n",
    "    - íƒœìŠ¤í¬ ì ì‘, ìŠ¤íƒ€ì¼, ë„ë©”ì¸ íŠ¹í™”\n",
    " \n",
    "\n",
    "```text\n",
    "Token Embedding\n",
    " â†“\n",
    "Layer 1~5   : ë¬¸ì/í† í° íŒ¨í„´\n",
    " â†“\n",
    "Layer 6~15  : êµ¬ë¬¸(Syntax), ë¬¸ì¥ êµ¬ì¡°\n",
    " â†“\n",
    "Layer 16~25 : ì˜ë¯¸(Semantics), ê´€ê³„\n",
    " â†“\n",
    "Layer 26~N  : íƒœìŠ¤í¬, ì˜ë„, ìŠ¤íƒ€ì¼\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d98cace8-19f6-41c0-9fa2-04b8bb987fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# First prepare the model for int8 training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# Then freeze all parameters\n",
    "for param in model.parameters():\n",
    "    if param.dtype == torch.float32 or param.dtype == torch.float16:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Unfreeze only the top N layers\n",
    "num_layers_to_unfreeze = 2  # Adjust this number as needed\n",
    "for i, layer in enumerate(model.gpt_neox.layers[-num_layers_to_unfreeze:]):\n",
    "    for param in layer.parameters():\n",
    "        if param.dtype == torch.float32 or param.dtype == torch.float16:\n",
    "            param.requires_grad = True\n",
    "        \n",
    "MICRO_BATCH_SIZE = 8\n",
    "BATCH_SIZE = 64\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LORA_R = 256 # 512\n",
    "LORA_ALPHA = 512 # 1024\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "        \n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "                 r=LORA_R,\n",
    "                 lora_alpha=LORA_ALPHA,\n",
    "                 lora_dropout=LORA_DROPOUT,\n",
    "                 bias=\"none\",\n",
    "                 task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166278b-9810-4d25-a5b2-95db0e376d76",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Use the `get_peft_model` function to initialize the model with the LoRA framework, configuring it based on the provided `lora_config` settings. This way, the model can incorporate the benefits and capabilities of the LoRA optimization approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6042bbe6-6815-4f19-95c1-c8946f6156d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 83886080 || all params: 2858977280 || trainable%: 2.9341289483769524\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f590f0f-e95e-4407-8716-e0a802ccc2b9",
   "metadata": {},
   "source": [
    "ë³´ì‹œëŠ” ë°”ì™€ ê°™ì´, LoRA ì „ìš© í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°ëŠ” ì „ì²´ ê°€ì¤‘ì¹˜ì˜ ì•½ 3%ì— ë¶ˆê³¼í•˜ì—¬ í›¨ì”¬ íš¨ìœ¨ì ì…ë‹ˆë‹¤.\n",
    "\n",
    "### <a name=\"#step5.2\">5.2ë‹¨ê³„: ë°ì´í„° ì½œë ˆì´í„° ì •ì˜</a>\n",
    "\n",
    "DataCollatorëŠ” ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œ ëª©ë¡ì„ ë°›ì•„ PyTorch í…ì„œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°°ì¹˜ë¡œ ì½œë ˆì´ì…˜í•˜ëŠ” transformers í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ê¸°ë³¸ `DataCollatorForLanguageModeling` í´ë˜ìŠ¤ì˜ ê¸°ëŠ¥ì„ í™•ì¥í•œ `DataCollatorForCompletionOnlyLM`ì„ ì‚¬ìš©í•˜ì„¸ìš”. ì´ ì‚¬ìš©ì ì§€ì • ì½œë ˆì´í„°ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ì— í”„ë¡¬í”„íŠ¸ ë‹¤ìŒì— ì‘ë‹µì´ ì˜¤ëŠ” ê²½ìš°ì™€ ê·¸ì— ë”°ë¼ ë ˆì´ë¸”ì´ ìˆ˜ì •ë˜ëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "êµ¬í˜„ì€ `utils/helpers.py`ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e5cef99-9c84-4478-be05-317b7f79de45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.helpers import MLUDataCollatorForCompletionOnlyLM\n",
    "\n",
    "data_collator = MLUDataCollatorForCompletionOnlyLM(\n",
    "        tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321ebc5-c2db-4c44-9442-961122da9205",
   "metadata": {},
   "source": [
    "### <a name=\"#step5.3\">Step 5.3: Define the trainer</a>\n",
    "\n",
    "To fine-tune the LLM, you must define a trainer. First, define the training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07571a85-2724-44ee-a2b0-24ed5866ce2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "MODEL_SAVE_FOLDER_NAME = \"dolly-3b-lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                    output_dir=MODEL_SAVE_FOLDER_NAME,\n",
    "                    fp16=True,\n",
    "                    per_device_train_batch_size=8,\n",
    "                    per_device_eval_batch_size=8,\n",
    "                    learning_rate=LEARNING_RATE,\n",
    "                    num_train_epochs=EPOCHS,\n",
    "                    logging_strategy=\"steps\",\n",
    "                    logging_steps=100,\n",
    "                    evaluation_strategy=\"steps\",\n",
    "                    eval_steps=100,\n",
    "                    save_strategy=\"steps\",\n",
    "                    save_steps=20000,\n",
    "                    save_total_limit=10,\n",
    "                    optim=\"adamw_torch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b13b8-0139-48f5-8619-acf459b78d33",
   "metadata": {},
   "source": [
    "This is where the magic happens! Initialize the trainer with the defined model, tokenizer, training arguments, data collator, and the train/eval datasets.\n",
    "\n",
    "The training takes about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a708616f-8fbc-4f1e-9f50-077ae198fd69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 09:53, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.254600</td>\n",
       "      <td>2.825284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=170, training_loss=0.8034087573780733, metrics={'train_runtime': 598.3092, 'train_samples_per_second': 2.223, 'train_steps_per_second': 0.284, 'total_flos': 4224342422077440.0, 'train_loss': 0.8034087573780733, 'epoch': 10.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=split_dataset['train'],\n",
    "        eval_dataset=split_dataset[\"test\"],\n",
    "        data_collator=data_collator,\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf5ffa-3997-415f-bf51-dd91d8c3d777",
   "metadata": {},
   "source": [
    "### <a name=\"#step5.4\">5.4ë‹¨ê³„: ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ ì €ì¥</a>\n",
    "\n",
    "í•™ìŠµì´ ì™„ë£Œë˜ë©´ `transformers.PreTrainedModel.save_pretrained` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë””ë ‰í† ë¦¬ì— ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ í•¨ìˆ˜ëŠ” í•™ìŠµëœ ì¦ë¶„ PEFT ê°€ì¤‘ì¹˜(adapter_model.bin)ë§Œ ì €ì¥í•˜ë¯€ë¡œ ëª¨ë¸ì„ ì €ì¥, ì „ì†¡ ë° ë¶ˆëŸ¬ì˜¤ëŠ” ë° ë§¤ìš° íš¨ìœ¨ì ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97f327e9-c6cd-4f2b-af25-e22420372e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9cc6d-98cf-4118-8218-6de6c9893570",
   "metadata": {},
   "source": [
    "If you want to save the full model that you just fine-tuned, you can use the [`transformers.trainer.save_model`] function. Meanwhile, save the training arguments together with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a112abbd-8b1b-4da7-80d4-35b92e4eadcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "198216a6-598e-4d56-af8d-682a71b187bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.model.config.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33f0f9-2604-45fb-a32d-6a1b2e311513",
   "metadata": {},
   "source": [
    "Save the tokenizer along with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56aac92b-8bcb-46dc-87d6-ff5069b0896c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dolly-3b-lora/tokenizer_config.json',\n",
       " 'dolly-3b-lora/special_tokens_map.json',\n",
       " 'dolly-3b-lora/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3c515-1c98-4282-9040-7af7d702fb8e",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "\n",
    "#\n",
    "## <a name=\"#step6\">6ë‹¨ê³„: ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ ë°°í¬</a>\n",
    "\n",
    "### <a name=\"step6\">ë°°í¬ ë§¤ê°œë³€ìˆ˜ ê°œìš”</a>\n",
    "\n",
    "Amazon SageMaker Python SDKì™€ Deep Java Library(DJL)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°í¬í•˜ë ¤ë©´ ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ `Model` í´ë˜ìŠ¤ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "```{python}\n",
    "model = Model(\n",
    "image_uri,\n",
    "model_data=...,\n",
    "predictor_cls=...,\n",
    "role=aws_role\n",
    ")\n",
    "```\n",
    "- `image_uri`: ì‚¬ìš©í•  ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ ë° ë²„ì „ì„ ë‚˜íƒ€ë‚´ëŠ” Docker ì´ë¯¸ì§€ URIì…ë‹ˆë‹¤.\n",
    "\n",
    "- `model_data`: Amazon Simple Storage Service(Amazon S3) ë²„í‚·ì— ìˆëŠ” ë¯¸ì„¸ ì¡°ì •ëœ LLM ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ì˜ ìœ„ì¹˜ì…ë‹ˆë‹¤. ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜, ì•„í‚¤í…ì²˜ ë° í•„ìš”í•œ ì•„í‹°íŒ©íŠ¸ê°€ í¬í•¨ëœ TAR GZ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `predictor_cls`: DJLê³¼ ê´€ë ¨ì´ ì—†ëŠ” JSON ì…ë ¥ JSON ì¶œë ¥ ì˜ˆì¸¡ê¸°ì…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [sagemaker.djl_inference.DJLPredictor](https://sagemaker.readthedocs.io/en/stable/frameworks/djl/sagemaker.djl_inference.html#djlpredictor)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n",
    "\n",
    "- `role`: ëª¨ë¸ ë°ì´í„°ê°€ í¬í•¨ëœ S3 ë²„í‚·ê³¼ ê°™ì€ ë¦¬ì†ŒìŠ¤ì— ì•¡ì„¸ìŠ¤í•˜ëŠ” ë° í•„ìš”í•œ ê¶Œí•œì„ ì œê³µí•˜ëŠ” AWS Identity and Access Management(IAM) ì—­í•  ARNì…ë‹ˆë‹¤.\n",
    "\n",
    "### <a name=\"step6.1\">6.1ë‹¨ê³„: SageMaker ë§¤ê°œë³€ìˆ˜ ì¸ìŠ¤í„´ìŠ¤í™”</a>\n",
    "\n",
    "Amazon SageMaker ì„¸ì…˜ì„ ì´ˆê¸°í™”í•˜ê³  SageMaker ì—­í•  ë° AWS ë¦¬ì „ê³¼ ê°™ì€ AWS í™˜ê²½ ê´€ë ¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ë˜í•œ SageMaker ì„¸ì…˜ì˜ ë¦¬ì „ì„ ì‚¬ìš©í•˜ì—¬ \"djl-deepspeed\" í”„ë ˆì„ì›Œí¬ì˜ íŠ¹ì • ë²„ì „ì— ëŒ€í•œ ì´ë¯¸ì§€ URIë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ URIëŠ” Amazon SageMaker ë˜ëŠ” Amazon Elastic Container Registry(Amazon ECR)ì™€ ê°™ì€ ë‹¤ì–‘í•œ AWS ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íŠ¹ì • Docker ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ì— ëŒ€í•œ ê³ ìœ  ì‹ë³„ìì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e147f60a-5209-460c-bb29-8d1b57d27e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting sagemaker==2.237.1\n",
      "  Downloading sagemaker-2.237.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting attrs<24,>=23.1.0 (from sagemaker==2.237.1)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: boto3<2.0,>=1.35.75 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (1.42.16)\n",
      "Collecting cloudpickle==2.2.1 (from sagemaker==2.237.1)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (7.1.0)\n",
      "Requirement already satisfied: fastapi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (0.127.0)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (4.25.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (1.26.4)\n",
      "Collecting omegaconf<2.3,>=2.2 (from sagemaker==2.237.1)\n",
      "  Downloading omegaconf-2.2.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (24.2)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (2.3.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (0.3.4)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (4.5.0)\n",
      "Collecting protobuf<6.0,>=3.12 (from sagemaker==2.237.1)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (7.1.3)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (6.0.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (2.32.5)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (1.0.72)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (0.7.8)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (4.67.1)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (2.5.0)\n",
      "Requirement already satisfied: uvicorn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.237.1) (0.40.0)\n",
      "Requirement already satisfied: botocore<1.43.0,>=1.42.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.35.75->sagemaker==2.237.1) (1.42.16)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.35.75->sagemaker==2.237.1) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.17.0,>=0.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.35.75->sagemaker==2.237.1) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.43.0,>=1.42.16->boto3<2.0,>=1.35.75->sagemaker==2.237.1) (2.9.0.post0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker==2.237.1) (3.23.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from omegaconf<2.3,>=2.2->sagemaker==2.237.1) (4.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.43.0,>=1.42.16->boto3<2.0,>=1.35.75->sagemaker==2.237.1) (1.17.0)\n",
      "Collecting pydantic<3.0.0,>=2.0.0 (from sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.237.1)\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: rich<15.0.0,>=13.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.237.1) (14.2.0)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.237.1) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker==2.237.1) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker==2.237.1) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker==2.237.1) (0.28.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.237.1) (0.7.0)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.237.1)\n",
      "  Using cached pydantic_core-2.41.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.237.1) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.237.1) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.237.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.237.1) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker==2.237.1) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker==2.237.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker==2.237.1) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker==2.237.1) (2025.10.5)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fastapi->sagemaker==2.237.1) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fastapi->sagemaker==2.237.1) (0.0.4)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from starlette<0.51.0,>=0.40.0->fastapi->sagemaker==2.237.1) (4.11.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi->sagemaker==2.237.1) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi->sagemaker==2.237.1) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker==2.237.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker==2.237.1) (2025.2)\n",
      "Requirement already satisfied: ppft>=1.7.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker==2.237.1) (1.7.7)\n",
      "Collecting dill>=0.4.0 (from pathos->sagemaker==2.237.1)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pox>=0.3.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker==2.237.1) (0.3.6)\n",
      "Collecting multiprocess>=0.70.18 (from pathos->sagemaker==2.237.1)\n",
      "  Using cached multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn->sagemaker==2.237.1) (8.3.0)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn->sagemaker==2.237.1) (0.16.0)\n",
      "Downloading sagemaker-2.237.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
      "Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached multiprocess-0.70.18-py310-none-any.whl (134 kB)\n",
      "Installing collected packages: pydantic-core, protobuf, omegaconf, dill, cloudpickle, attrs, pydantic, multiprocess, sagemaker\n",
      "\u001b[2K  Attempting uninstall: pydantic-core\n",
      "\u001b[2K    Found existing installation: pydantic_core 2.41.4\n",
      "\u001b[2K    Uninstalling pydantic_core-2.41.4:\n",
      "\u001b[2K      Successfully uninstalled pydantic_core-2.41.4\n",
      "\u001b[2K  Attempting uninstall: protobuf\n",
      "\u001b[2K    Found existing installation: protobuf 6.31.1\n",
      "\u001b[2K    Uninstalling protobuf-6.31.1:\n",
      "\u001b[2K      Successfully uninstalled protobuf-6.31.1\n",
      "\u001b[2K  Attempting uninstall: omegaconfâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/9\u001b[0m [protobuf]\n",
      "\u001b[2K    Found existing installation: omegaconf 2.3.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/9\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling omegaconf-2.3.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/9\u001b[0m [protobuf]\n",
      "\u001b[2K      Successfully uninstalled omegaconf-2.3.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/9\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: dillâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/9\u001b[0m [protobuf]\n",
      "\u001b[2K    Found existing installation: dill 0.3.8â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/9\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling dill-0.3.8:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/9\u001b[0m [protobuf]\n",
      "\u001b[2K      Successfully uninstalled dill-0.3.8â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/9\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: cloudpickle\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K    Found existing installation: cloudpickle 3.1.2â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K    Uninstalling cloudpickle-3.1.2:mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled cloudpickle-3.1.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K  Attempting uninstall: attrs0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K    Found existing installation: attrs 25.4.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K    Uninstalling attrs-25.4.0:m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled attrs-25.4.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K  Attempting uninstall: pydantic[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K    Found existing installation: pydantic 1.10.26â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K    Uninstalling pydantic-1.10.26:0mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled pydantic-1.10.26â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K  Attempting uninstall: multiprocess0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/9\u001b[0m [pydantic]\n",
      "\u001b[2K    Found existing installation: multiprocess 0.70.16â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/9\u001b[0m [pydantic]\n",
      "\u001b[2K    Uninstalling multiprocess-0.70.16:mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/9\u001b[0m [pydantic]\n",
      "\u001b[2K      Successfully uninstalled multiprocess-0.70.16â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/9\u001b[0m [pydantic]\n",
      "\u001b[2K  Attempting uninstall: sagemakerm\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/9\u001b[0m [pydantic]\n",
      "\u001b[2K    Found existing installation: sagemaker 2.255.0â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/9\u001b[0m [pydantic]\n",
      "\u001b[2K    Uninstalling sagemaker-2.255.0:â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[32m8/9\u001b[0m [sagemaker]\n",
      "\u001b[2K      Successfully uninstalled sagemaker-2.255.0â•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[32m8/9\u001b[0m [sagemaker]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9/9\u001b[0m [sagemaker]/9\u001b[0m [sagemaker]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab 4.4.10 requires httpx<1,>=0.25.0, which is not installed.\n",
      "spyder 5.2.2 requires pyqtwebengine<5.13, which is not installed.\n",
      "dask 2025.10.0 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
      "datasets 2.21.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.0 which is incompatible.\n",
      "distributed 2025.10.0 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
      "langchain 0.0.161 requires pydantic<2,>=1, but you have pydantic 2.12.5 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.3.3 which is incompatible.\n",
      "spyder 5.2.2 requires pyqt5<5.13, but you have pyqt5 5.15.11 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed attrs-23.2.0 cloudpickle-2.2.1 dill-0.4.0 multiprocess-0.70.18 omegaconf-2.2.3 protobuf-5.29.5 pydantic-2.12.5 pydantic-core-2.41.5 sagemaker-2.237.1\n"
     ]
    }
   ],
   "source": [
    "# installing sagemaker library\n",
    "!pip3 install sagemaker==2.237.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55f11d90-e6ac-42f4-a8ef-b74f55d8c15d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker_session:  <sagemaker.session.Session object at 0x7f90d5f089a0>\n",
      "aws_role:  arn:aws:iam::864506703836:role/sagemaker_notebook_role_54582730\n",
      "aws_region:  us-east-1\n",
      "image_uri:  763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.22.1-deepspeed0.9.2-cu118\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker.djl_inference\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import Model\n",
    "\n",
    "sagemaker_session = Session()\n",
    "print(\"sagemaker_session: \", sagemaker_session)\n",
    "\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "print(\"aws_role: \", aws_role)\n",
    "\n",
    "aws_region = boto3.Session().region_name\n",
    "print(\"aws_region: \", aws_region)\n",
    "\n",
    "image_uri = image_uris.retrieve(framework=\"djl-deepspeed\",\n",
    "                                version=\"0.22.1\",\n",
    "                                region=sagemaker_session._region_name)\n",
    "print(\"image_uri: \", image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3ae0e-6639-4400-9123-c38f1b79fd99",
   "metadata": {},
   "source": [
    "### <a name=\"step6.2\">6.2ë‹¨ê³„: ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ìƒì„±</a> ###\n",
    "\n",
    "ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë¥¼ S3 ë²„í‚·ì— ì—…ë¡œë“œí•˜ë ¤ë©´ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ê°€ í¬í•¨ëœ TAR GZ íŒŒì¼ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. ë¨¼ì € `lora_model`ì´ë¼ëŠ” ë””ë ‰í† ë¦¬ì™€ `dolly-3b-lora`ë¼ëŠ” í•˜ìœ„ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. \"-p\" ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ ì¤‘ê°„ ë””ë ‰í† ë¦¬ê°€ ì—†ëŠ” ê²½ìš° ëª…ë ¹ì´ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ LoRA ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì¸ `adapter_model.bin`ê³¼ `adapter_config.json`ì„ `dolly-3b-lora` ë””ë ‰í† ë¦¬ì— ë³µì‚¬í•©ë‹ˆë‹¤. ê¸°ë³¸ Dolly ëª¨ë¸ì€ ëŸ°íƒ€ì„ ì‹œ Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5685e182-95e0-4ec3-888d-bb573ebdd3db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf lora_model\n",
    "mkdir -p lora_model\n",
    "mkdir -p lora_model/dolly-3b-lora\n",
    "cp dolly-3b-lora/adapter_config.json lora_model/dolly-3b-lora/\n",
    "cp dolly-3b-lora/adapter_model.bin lora_model/dolly-3b-lora/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df3e690-c244-44b6-9ff7-7a51d2ee47e3",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ, `serving.properties` íŒŒì¼ì— [DJL Serving êµ¬ì„± ì˜µì…˜](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html)ì„ ì„¤ì •í•©ë‹ˆë‹¤. Jupyterì˜ `%%writefile` ë§¤ì§ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë‚´ìš©ì„ `lora_model/serving.properties`ë¼ëŠ” íŒŒì¼ì— ê¸°ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- `engine=Python`: ì´ ì¤„ì€ ì„œë¹™ì— ì‚¬ìš©í•  ì—”ì§„ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `option.entryPoint=model.py`: ì´ ì¤„ì€ ì„œë¹™ í”„ë¡œì„¸ìŠ¤ì˜ ì§„ì…ì ì„ ì§€ì •í•˜ë©°, `model.py`ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.\n",
    "\n",
    "- `option.adapter_checkpoint=dolly-3b-lora`: ì´ ì¤„ì€ ì–´ëŒ‘í„°ì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ `dolly-3b-lora`ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. ì²´í¬í¬ì¸íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ ë˜ëŠ” í•´ë‹¹ ë§¤ê°œë³€ìˆ˜ì˜ ì €ì¥ëœ ìƒíƒœë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    "- `option.adapter_name=dolly-lora`: ì´ ì¤„ì€ ì–´ëŒ‘í„°ì˜ ì´ë¦„ì„ dolly-loraë¡œ ì„¤ì •í•©ë‹ˆë‹¤. dolly-loraëŠ” ëª¨ë¸ê³¼ ì„œë¹„ìŠ¤ ì¸í”„ë¼ ê°„ì˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì§€ì›í•˜ëŠ” êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34fa2845-a04b-48aa-8381-a8097bce07b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lora_model/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile lora_model/serving.properties\n",
    "engine=Python\n",
    "option.entryPoint=model.py\n",
    "option.adapter_checkpoint=dolly-3b-lora\n",
    "option.adapter_name=dolly-lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91554e8a-c65d-409b-bf16-29c179a7a6df",
   "metadata": {},
   "source": [
    "You also need the environment requirement file in the model artifact. Create a file named `lora_model/requirements.txt` and write a list of Python package requirements, typically used with package managers such as `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c76b09e8-3080-4f79-bec8-7ebfb2feeb64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lora_model/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile lora_model/requirements.txt\n",
    "accelerate>=0.16.0,<1\n",
    "bitsandbytes==0.39.0\n",
    "click>=8.0.4,<9\n",
    "datasets>=2.10.0,<3\n",
    "deepspeed>=0.8.3,<0.9\n",
    "faiss-cpu==1.7.4\n",
    "ipykernel==6.22.0\n",
    "scipy==1.11.1\n",
    "torch>=2.0.0\n",
    "transformers==4.28.1\n",
    "peft==0.3.0\n",
    "pytest==7.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca79001-7dd7-49ca-80e1-3ee88a8034b8",
   "metadata": {},
   "source": [
    "### <a name=\"step6.3\">Step 6.3: Create the inference script</a>\n",
    "\n",
    "Similar to the fine-tuning notebook, a custom pipeline, `InstructionTextGenerationPipeline`, is defined. The code is provided in `utils/deployment_model.py`.\n",
    "\n",
    "You save these inference functions to `lora_model/model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8be0229e-0895-436e-b789-edbb3ba773cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp utils/deployment_model.py lora_model/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d7b1f-7b8d-444a-b185-a46053421536",
   "metadata": {},
   "source": [
    "### <a name=\"step6.4\">Step 6.4: Upload the model artifact to Amazon S3</a>\n",
    "\n",
    "Create a compressed tarball archive of the lora_model directory and save it as lora_model.tar.gz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86c84297-6f89-47b5-9f11-2cf1d2fba989",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_model/\n",
      "lora_model/requirements.txt\n",
      "lora_model/serving.properties\n",
      "lora_model/dolly-3b-lora/\n",
      "lora_model/dolly-3b-lora/adapter_model.bin\n",
      "lora_model/dolly-3b-lora/adapter_config.json\n",
      "lora_model/model.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tar -cvzf lora_model.tar.gz lora_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e15a5-832a-453c-b02d-b0e5c82ca8b5",
   "metadata": {},
   "source": [
    "Upload the lora_model.tar.gz file to the specified S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "383a3067-47f1-4ddc-9f81-0d6c33801d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact-54582730\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker.djl_inference\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import Model\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Get the name of the bucket with prefix lab-code\n",
    "for bucket in s3.buckets.all():\n",
    "    if bucket.name.startswith('artifact'):\n",
    "        mybucket = bucket.name\n",
    "        print(mybucket)\n",
    "\n",
    "response = s3_client.upload_file(\"lora_model.tar.gz\", mybucket, \"lora_model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d142b4-c420-416e-a0c5-3068a934d573",
   "metadata": {},
   "source": [
    "### <a name=\"step6.5\">6.5ë‹¨ê³„: ëª¨ë¸ ë°°í¬</a> ###\n",
    "\n",
    "ì´ì œ SageMaker Python SDKë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •ëœ LLM ëª¨ë¸ì„ ë°°í¬í•  ì°¨ë¡€ì…ë‹ˆë‹¤. SageMaker Python SDKì˜ `Model` í´ë˜ìŠ¤ëŠ” ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤í™”ë©ë‹ˆë‹¤.\n",
    "\n",
    "- `image_uri`: ì‚¬ìš©í•  ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ ë° ë²„ì „ì„ ë‚˜íƒ€ë‚´ëŠ” Docker ì´ë¯¸ì§€ URIì…ë‹ˆë‹¤.\n",
    "\n",
    "- `model_data`: S3 ë²„í‚·ì— ìˆëŠ” ë¯¸ì„¸ ì¡°ì •ëœ LLM ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ì˜ ìœ„ì¹˜ì…ë‹ˆë‹¤. ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜, ì•„í‚¤í…ì²˜ ë° í•„ìš”í•œ ì•„í‹°íŒ©íŠ¸ê°€ í¬í•¨ëœ TAR GZ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `predictor_cls`: ì´ëŠ” JSON ì…ë ¥, JSON ì¶œë ¥ ì˜ˆì¸¡ê¸°ì´ë©° DJLê³¼ëŠ” ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [sagemaker.djl_inference.DJLPredictor](https://sagemaker.readthedocs.io/en/stable/frameworks/djl/sagemaker.djl_inference.html#djlpredictor)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n",
    "- `role`: ëª¨ë¸ ë°ì´í„°ê°€ í¬í•¨ëœ S3 ë²„í‚·ê³¼ ê°™ì€ ë¦¬ì†ŒìŠ¤ì— ì•¡ì„¸ìŠ¤í•˜ëŠ” ë° í•„ìš”í•œ ê¶Œí•œì„ ì œê³µí•˜ëŠ” IAM ì—­í•  ARNì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c7f0b12-bf0f-4fd8-bb7d-a0000ee67b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data=\"s3://{}/lora_model.tar.gz\".format(mybucket)\n",
    "\n",
    "model = Model(image_uri=image_uri,\n",
    "              model_data=model_data,\n",
    "              predictor_cls=sagemaker.djl_inference.DJLPredictor,\n",
    "              role=aws_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d5263-2c04-4541-8126-e718ab7e55ce",
   "metadata": {},
   "source": [
    "NOTE: The deployment should be completed within 10 minutes. Any longer than that, your endpoint might have failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "269abf73-c194-48c0-8f5e-6c23547acb5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!CPU times: user 225 ms, sys: 81.1 ms, total: 306 ms\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = model.deploy(1, \"ml.g4dn.2xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6aff0-5525-4bc8-945b-96391b9fc1bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a name=\"step7\">Step 7: Test the deployed inference</a>\n",
    "\n",
    "Test the inference endpoint with [predictor.predict](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html#sagemaker.predictor.Predictor.predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20d371af-3486-441f-8d6f-ef65bf12395d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = predictor.predict({\"inputs\": \"What security measures does Amazon SageMaker have?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2df51a6-6e71-4c3a-b2be-116fc07851a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Amazon SageMaker is designed to protect your data and models from suspected attacks, including outside access, unauthorized backups, model retraining without consent, and more. For more information, see the SageMaker Security Page."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a2199-7616-4c05-8f3d-6a5020856332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
