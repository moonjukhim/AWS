{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6ccf5f",
   "metadata": {},
   "source": [
    "### loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6273a40c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('./test.txt', encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Install the beautifulsoup4 package:\n",
    "\n",
    "```bash\n",
    "pip install beautifulsoup4\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader('https://www.langchain.com/')\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc4bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the pdf parsing library !pip install pypdf\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('./test.pdf')\n",
    "pages = loader.load()\n",
    "\n",
    "print(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa622da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('./test.txt', encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splitted_docs = splitter.split_documents(docs)\n",
    "\n",
    "print(splitted_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e9ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "PYTHON_CODE = \"\"\" def hello_world(): print(\"Hello, World!\") # Call the function hello_world() \"\"\"\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "\n",
    "print(python_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "markdown_text = \"\"\" # ðŸ¦œðŸ”— LangChain âš¡ Building applications with LLMs through composability âš¡ ## Quick Install ```bash pip install langchain ``` As an open source project in a rapidly developing field, we are extremely open     to contributions. \"\"\"\n",
    "\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "\n",
    "md_docs = md_splitter.create_documents(\n",
    "    [markdown_text], [{\"source\": \"https://www.langchain.com\"}])\n",
    "\n",
    "print(md_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9721d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "embeddings = model.embed_documents([\n",
    "    \"Hi there!\",\n",
    "    \"Oh, hello!\",\n",
    "    \"What's your name?\",\n",
    "    \"My friends call me World\",\n",
    "    \"Hello World!\"\n",
    "])\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ded81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load the document\n",
    "loader = TextLoader(\"./test.txt\", encoding=\"utf-8\")\n",
    "doc = loader.load()\n",
    "\n",
    "# Split the document\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(doc)\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [chunk.page_content for chunk in chunks]\n",
    ")\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Ensure docker is installed and running (https://docs.docker.com/get-docker/)\n",
    "2. pip install -qU langchain_postgres\n",
    "3. Run the following command to start the postgres container:\n",
    "   \n",
    "docker run \\\n",
    "    --name pgvector-container \\\n",
    "    -e POSTGRES_USER=langchain \\\n",
    "    -e POSTGRES_PASSWORD=langchain \\\n",
    "    -e POSTGRES_DB=langchain \\\n",
    "    -p 6024:5432 \\\n",
    "    -d pgvector/pgvector:pg16\n",
    "4. Use the connection string below for the postgres container\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "\n",
    "\n",
    "# See docker command above to launch a postgres instance with pgvector enabled.\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "\n",
    "# Load the document, split it into chunks\n",
    "raw_documents = TextLoader('./test.txt', encoding=\"utf-8\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# Create embeddings for the documents\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "db = PGVector.from_documents(\n",
    "    documents, embeddings_model, connection=connection)\n",
    "\n",
    "results = db.similarity_search(\"query\", k=4)\n",
    "\n",
    "print(results)\n",
    "\n",
    "print(\"Adding documents to the vector store\")\n",
    "ids = [str(uuid.uuid4()), str(uuid.uuid4())]\n",
    "db.add_documents(\n",
    "    [\n",
    "        Document(\n",
    "            page_content=\"there are cats in the pond\",\n",
    "            metadata={\"location\": \"pond\", \"topic\": \"animals\"},\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"ducks are also found in the pond\",\n",
    "            metadata={\"location\": \"pond\", \"topic\": \"animals\"},\n",
    "        ),\n",
    "    ],\n",
    "    ids=ids,\n",
    ")\n",
    "\n",
    "print(\"Documents added successfully.\\n Fetched documents count:\",\n",
    "      len(db.get_by_ids(ids)))\n",
    "\n",
    "print(\"Deleting document with id\", ids[1])\n",
    "db.delete({\"ids\": ids})\n",
    "\n",
    "print(\"Document deleted successfully.\\n Fetched documents count:\",\n",
    "      len(db.get_by_ids(ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f1c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "collection_name = \"my_docs\"\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "namespace = \"my_docs_namespace\"\n",
    "\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings_model,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace,\n",
    "    db_url=\"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\",\n",
    ")\n",
    "\n",
    "# Create the schema if it doesn't exist\n",
    "record_manager.create_schema()\n",
    "\n",
    "# Create documents\n",
    "docs = [\n",
    "    Document(page_content='there are cats in the pond', metadata={\n",
    "             \"id\": 1, \"source\": \"cats.txt\"}),\n",
    "    Document(page_content='ducks are also found in the pond', metadata={\n",
    "             \"id\": 2, \"source\": \"ducks.txt\"}),\n",
    "]\n",
    "\n",
    "# Index the documents\n",
    "index_1 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",  # prevent duplicate documents\n",
    "    source_id_key=\"source\",  # use the source field as the source_id\n",
    ")\n",
    "\n",
    "print(\"Index attempt 1:\", index_1)\n",
    "\n",
    "# second time you attempt to index, it will not add the documents again\n",
    "index_2 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"source\",\n",
    ")\n",
    "\n",
    "print(\"Index attempt 2:\", index_2)\n",
    "\n",
    "# If we mutate a document, the new version will be written and all old versions sharing the same source will be deleted.\n",
    "\n",
    "docs[0].page_content = \"I just modified this document!\"\n",
    "\n",
    "index_3 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"source\",\n",
    ")\n",
    "\n",
    "print(\"Index attempt 3:\", index_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9884d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "import uuid\n",
    "\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "collection_name = \"summaries\"\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "# Load the document\n",
    "loader = TextLoader(\"./test.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"length of loaded docs: \", len(docs[0].page_content))\n",
    "# Split the document\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# The rest of your code remains the same, starting from:\n",
    "prompt_text = \"Summarize the following document:\\n\\n{doc}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "summarize_chain = {\n",
    "    \"doc\": lambda x: x.page_content} | prompt | llm | StrOutputParser()\n",
    "\n",
    "# batch the chain across the chunks\n",
    "summaries = summarize_chain.batch(chunks, {\"max_concurrency\": 5})\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings_model,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# indexing the summaries in our vector store, whilst retaining the original documents in our document store:\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Changed from summaries to chunks since we need same length as docs\n",
    "doc_ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "\n",
    "# Each summary is linked to the original document by the doc_id\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add the document summaries to the vector store for similarity search\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "\n",
    "# Store the original documents in the document store, linked to their summaries via doc_ids\n",
    "# This allows us to first search summaries efficiently, then fetch the full docs when needed\n",
    "retriever.docstore.mset(list(zip(doc_ids, chunks)))\n",
    "\n",
    "# vector store retrieves the summaries\n",
    "sub_docs = retriever.vectorstore.similarity_search(\n",
    "    \"chapter on philosophy\", k=2)\n",
    "\n",
    "print(\"sub docs: \", sub_docs[0].page_content)\n",
    "\n",
    "print(\"length of sub docs:\\n\", len(sub_docs[0].page_content))\n",
    "\n",
    "# Whereas the retriever will return the larger source document chunks:\n",
    "retrieved_docs = retriever.invoke(\"chapter on philosophy\")\n",
    "\n",
    "print(\"length of retrieved docs: \", len(retrieved_docs[0].page_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Windows is not supported. RAGatouille doesn't appear to work outside WSL and has issues with WSL1. Some users have had success running RAGatouille in WSL2.\n",
    "- Only on python.\n",
    "- Read full docs here: https://github.com/AnswerDotAI/RAGatouille/blob/8183aad64a9a6ba805d4066dcab489d97615d316/README.md\n",
    "\n",
    "- To install run:\n",
    "\n",
    "```bash\n",
    "pip install -U ragatouille transformers\n",
    "```\n",
    "\"\"\"\n",
    "from ragatouille import RAGPretrainedModel\n",
    "import requests\n",
    "\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    Retrieve the full text content of a Wikipedia page.\n",
    "    :param title: str - Title of the Wikipedia page.\n",
    "    :return: str - Full text content of the page as raw string.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
    "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1\"}\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "    # Extracting page content\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page[\"extract\"] if \"extract\" in page else None\n",
    "\n",
    "\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")\n",
    "# Create an index\n",
    "RAG.index(\n",
    "    collection=[full_document],\n",
    "    index_name=\"Miyazaki-123\",\n",
    "    max_document_length=180,\n",
    "    split_documents=True,\n",
    ")\n",
    "# query\n",
    "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
    "\n",
    "print(results)\n",
    "\n",
    "# Alternative: Utilize langchain retriever\n",
    "retriever = RAG.as_langchain_retriever(k=3)\n",
    "retriever.invoke(\"What animation studio did Miyazaki found?\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
