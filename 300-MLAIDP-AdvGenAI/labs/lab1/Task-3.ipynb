{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Build and evaluate Q&A Application using Amazon Bedrock Knowledge Bases using RAG Assessment (RAGAS) framework\n",
    "\n",
    "In this task, you build and evaluate a Q&A application using LangChain's *AmazonKnowledgeBasesRetriever* class, Chains, and the RAGAS framework for evaluating the responses. Here, you query the knowledge base to get the desired number of document chunks based on similarity search. Next, you prompt the text generation LLM by supplying the document chunks as context along with the query. Then you evaluate the responses using the following evaluation metrics: faithfulness, answer_relevancy, context_recall, context_precision, context_entity_recall, answer_similarity, answer_correctness, harmfulness, maliciousness, coherence, correctness and conciseness.\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-info-circle\" style=\"color:#007FAA\"></i> **Learn more:** Refer to *[Metrics](https://docs.ragas.io/en/latest/concepts/metrics/index.html)* for additional information about different metrics used with the Ragas framework.\n",
    "\n",
    "### Pattern\n",
    "\n",
    "You implement the solution using Retrieval Augmented Generation (RAG) pattern. RAG retrieves data from outside the language model and augments the prompts by adding the relevant retrieved data in context. In this task, you create responses to queries using the knowledge base that was already created during lab provisioning.\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "- You utilize RAGAS for evaluation on the following metrics:\n",
    "  - **Faithfulness:** This measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better.\n",
    "  - **Answer Relevance:** This metric focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy. This metric is computed using the question, the context and the answer. Please note, that even though in practice the score will range between 0 and 1 most of the time, this is not mathematically guaranteed, due to the nature of the cosine similarity ranging from -1 to 1.\n",
    "  - **Context Precision:** This is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks. This metric is computed using the question, ground_truth and the contexts, with values ranging between 0 and 1, where higher scores indicate better precision.\n",
    "  - **Context Recall:** This metric measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. It is computed based on the ground truth and the retrieved context, and the values range between 0 and 1, with higher values indicating better performance.\n",
    "  - **Context entities recall:** This metric gives the measure of recall of the retrieved context, based on the number of entities present in both ground_truths and contexts relative to the number of entities present in the ground_truths alone. Simply put, it is a measure of what fraction of entities are recalled from ground_truths. This metric is useful in fact-based use cases like tourism help desk, historical QA, etc. This metric can help evaluate the retrieval mechanism for entities, based on comparison with entities present in ground_truths, because in cases where entities matter, we need the contexts which cover them.\n",
    "  - **Answer Semantic Similarity:** The concept of Answer Semantic Similarity pertains to the assessment of the semantic resemblance between the generated answer and the ground truth. This evaluation is based on the ground truth and the answer, with values falling within the range of 0 to 1. A higher score signifies a better alignment between the generated answer and the ground truth.\n",
    "  - **Answer Correctness:** The assessment of Answer Correctness involves gauging the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness. Answer correctness encompasses two critical aspects: semantic similarity between the generated answer and the ground truth, as well as factual similarity. These aspects are combined using a weighted scheme to formulate the answer correctness score. Users also have the option to employ a ‘threshold’ value to round the resulting score to binary, if desired.\n",
    "  - **Aspect Critique:** This is designed to assess submissions based on predefined aspects such as harmlessness and correctness. The output of aspect critiques is binary, indicating whether the submission aligns with the defined aspect or not. This evaluation is performed using the ‘answer’ as input.\n",
    "\n",
    "In this task, you use AnyCompany's financial 10k reports (synthetically generated dataset) as a text corpus to perform Q&A on. This data is already ingested into the Knowledge Base in Amazon Bedrock.\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** In your specific use case, you can sync different files for different domain topics and query this notebook in the same manner to evaluate model responses using the retrieve API from knowledge bases.\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-exclamation-circle\" style=\"color:#7C5AED\"></i> **Caution:** It is recommended to run each code cell individually rather than using the **Run All Cells** option from the **Run** menu. Running all cells together can sometimes lead to unexpected behavior, such as the Kernel crashing or restarting. By executing cells one by one, you can better control the execution flow, catch potential errors early, and ensure that your code runs as intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1: Setup the environment\n",
    "\n",
    "To run this notebook you need to install dependencies, LangChain and RAGAS and updated boto3, botocore packages.\n",
    "\n",
    "You set up the necessary packages by following the steps provided below:\n",
    "\n",
    "- Import the necessary libraries for creating *bedrock-runtime* for invoking foundation models\n",
    "- Import LangChain related libraries\n",
    "- Initialize bedrock model **amazon.titan-text-premier-v1:0** as your large language model to perform query completions using the RAG pattern.\n",
    "- Initialize bedrock model **amazon.nova-lite-v1:0** as your large language model to perform RAG evaluation.\n",
    "- Initialize bedrock model **amazon.titan-embed-text-v2:0** as your large language embedding model to create embeddings for RAG evaluation. This is the same embedding model that was used to create the knowledge base.\n",
    "- Initialize LangChain retriever integrated with knowledge bases.\n",
    "- Later in the notebook you wrap the LLM and retriever as a chain for building your Q&A application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the following code cell to verify the ID for the existing Knowledge Base in Amazon Bedrock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base ID: W1FOJLAZNW\n"
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "\n",
    "session = boto3.Session()\n",
    "bedrock_client = session.client('bedrock-agent')\n",
    "\n",
    "try:\n",
    "    response = bedrock_client.list_knowledge_bases(\n",
    "        maxResults=1  # We only need to retrieve the first Knowledge Base\n",
    "    )\n",
    "    knowledge_base_summaries = response.get('knowledgeBaseSummaries', [])\n",
    "\n",
    "    if knowledge_base_summaries:\n",
    "        kb_id = knowledge_base_summaries[0]['knowledgeBaseId']\n",
    "        print(f\"Knowledge Base ID: {kb_id}\")\n",
    "    else:\n",
    "        print(\"No Knowledge Base summaries found.\")\n",
    "        \n",
    "except botocore.exceptions.ClientError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run the following code cell to install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pprint\n",
    "\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_aws.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "## 두 개의 에이전트를 사용\n",
    "llm_for_text_generation = ChatBedrock(model_id=\"amazon.nova-lite-v1:0\", client=bedrock_client)\n",
    "llm_for_evaluation = ChatBedrock(model_id=\"amazon.nova-lite-v1:0\", client=bedrock_client)\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\",client=bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2: Create *AmazonKnowledgeBasesRetriever* object from LangChain\n",
    "\n",
    "In this task, you create a *AmazonKnowledgeBasesRetriever* object from LangChain to search the knowledge base and return the relevant results, giving you more control to build custom workflows on top of the semantic search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run the following code cell to create a *AmazonKnowledgeBasesRetriever* object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "        knowledge_base_id=kb_id,\n",
    "        retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": 5}},\n",
    "        # endpoint_url=endpoint_url,\n",
    "        # region_name=\"us-east-1\",\n",
    "        # credentials_profile_name=\"<profile_name>\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3: Model Invocation and Response Generation using RetrievalQA chain \n",
    "\n",
    "In this task, you invoke the model and visualize the response using the following information:\n",
    "\n",
    "Question =\n",
    "\n",
    "```\n",
    "Provide a list of few risks for AnyCompany financial in numbered list without description.\"\n",
    "```\n",
    "\n",
    "Ground truth answer = \n",
    "\n",
    "```\n",
    "1. Commodity Prices\n",
    "2. Foreign Exchange Rates \n",
    "3. Equity Prices\n",
    "4. Credit Risk\n",
    "5. Liquidity Risk\n",
    "...\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run the following code cell to create a prompt with context and question as variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Human: You are a financial advisor AI system, and provides answers to questions by using fact based and statistical information when possible. \n",
    "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "Assistant:\"\"\"\n",
    "prompt = PromptTemplate(template=PROMPT_TEMPLATE, \n",
    "                               input_variables=[\"context\",\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run the following code cell to invoke the model using a pre-defined query and print the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Commodity Prices\n",
      "2. Foreign Exchange Rates\n",
      "3. Equity Prices\n",
      "4. Operational Risk\n",
      "5. Regulatory Risk\n",
      "6. Strategic Risk\n",
      "7. Reputation Risk\n",
      "8. Legal Risk\n",
      "9. Environmental, Social, and Governance (ESG) Risk\n",
      "10. Liquidity Risk\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs): #concatenate the text from the page_content field in the output from retriever.invoke\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm_for_text_generation\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"Provide a list of ten risks for AnyCompany financial as a numbered list. Do not include descriptions.\"\n",
    "\n",
    "response=chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.4: Prepare the evaluation data\n",
    "\n",
    "As RAGAS aims to be a reference-free evaluation framework, the required preparations of the evaluation dataset are minimal. In this task, you prepare the *question* and *ground_truths* pairs from which you can prepare the remaining information through inference as shown below.\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** If you are not interested in the *context_recall* metric, you don’t need to provide the *ground_truths* information. In this task, all you need to prepare are the *questions*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run the following code cell to prepare the *question* and *ground_truths* pairs for evaluation. This may run for a few minutes with retries in case of throttling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process questions...\n",
      "\n",
      "Processing question 1/5\n",
      "Query: What was the primary reason for the increase in net cash by operating activities for AnyCompany Fina...\n",
      "Successfully processed query on attempt 1\n",
      "Successfully processed question 1\n",
      "\n",
      "Processing question 2/5\n",
      "Query: Which year did AnyCompany Financial have the highest net cash used in investing activities, what was...\n",
      "Successfully processed query on attempt 1\n",
      "Successfully processed question 2\n",
      "\n",
      "Processing question 3/5\n",
      "Query: What was the primary source of cash inflows from financing activities for AnyCompany Financial in 20...\n",
      "Successfully processed query on attempt 1\n",
      "Successfully processed question 3\n",
      "\n",
      "Processing question 4/5\n",
      "Query: Calculate the year-over-year percentage change in cash and cash equivalents for AnyCompany Financial...\n",
      "Successfully processed query on attempt 1\n",
      "Successfully processed question 4\n",
      "\n",
      "Processing question 5/5\n",
      "Query: With the information provided, what can you infer about AnyCompany Financial's overall financial hea...\n",
      "Successfully processed query on attempt 1\n",
      "Successfully processed question 5\n",
      "\n",
      "Dataset Creation Summary:\n",
      "Total questions processed: 5 out of 5\n",
      "Columns available: ['question', 'ground_truth', 'answer', 'contexts']\n",
      "\n",
      "Sample Entry (First Question):\n",
      "Question: What was the primary reason for the increase in net cash by operating activities for AnyCompany Financial in 2021?\n",
      "Ground Truth: An increase in net cash provided by operating activities was primarily due to increases in net income and favorable changes in operating assets and liabilities.\n",
      "Model Answer: The primary reason for the increase in net cash provided by operating activities for AnyCompany Financial in 2021 was an increase in net income and favorable changes in operating assets and liabilities. Specifically, net cash provided by operating activities increased from $880 million in 2020 to $1,100 million in 2021. This growth was driven by the increase in net income, which rose from $1,125 million in 2020 to $1,200 million in 2021, and favorable changes in operating assets and liabilities.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Define questions and ground truths for RAGAS evaluation\n",
    "questions = [\n",
    "    \"What was the primary reason for the increase in net cash by operating activities for AnyCompany Financial in 2021?\",\n",
    "    \"Which year did AnyCompany Financial have the highest net cash used in investing activities, what was the primary reason?\",\n",
    "    \"What was the primary source of cash inflows from financing activities for AnyCompany Financial in 2021?\",\n",
    "    \"Calculate the year-over-year percentage change in cash and cash equivalents for AnyCompany Financial from 2020 to 2021.\",\n",
    "    \"With the information provided, what can you infer about AnyCompany Financial's overall financial health and growth prospects?\"\n",
    "]\n",
    "\n",
    "ground_truth = [\n",
    "    \"An increase in net cash provided by operating activities was primarily due to increases in net income and favorable changes in operating assets and liabilities.\",\n",
    "    \"AnyCompany Financial had the highest net cash used in investing activities in 2021, at $360 million, compared to $290 million in 2020 and $240 million in 2019. The primary reason was an increase in purchases of property, plant, and equipment and marketable securities.\",\n",
    "    \"The primary source of cash inflows from financing activities for AnyCompany Financial in 2021 was an increase in proceeds from the issuance of common stock and long-term debt.\",\n",
    "    \"To calculate the year-over-year percentage change in cash and cash equivalents from 2020 to 2021: \\\n",
    "    2020 cash and cash equivalents: $350 million \\\n",
    "    2021 cash and cash equivalents: $480 million \\\n",
    "    Percentage change = (2021 value - 2020 value) / 2020 value * 100 \\\n",
    "    = ($480 million - $350 million) / $350 million * 100 \\\n",
    "    = 37.14% increase\",\n",
    "    \"Based on information provided, AnyCompany Financial appears to be in a healthy financial position with good growth prospects. The company increased its net cash provided by operating activities, indicating strong profitability and efficient management of working capital. AnyCompany Financial has been investing in long-term assets, such as property, plant, and equipment, and marketable securities, which suggests plans for future growth and expansion. The company was able to finance its growth through the issuance of common stock and long-term debt, indicating confidence from investors and lenders. Overall, AnyCompany Financial's steady increase in cash and cash equivalents over the past three years provides a strong foundation for future growth and investment opportunities.\"\n",
    "]\n",
    "\n",
    "def get_model_response(query, chain, retriever, max_retries=5, wait_time=15):\n",
    "    \"\"\"Get response from the model with fixed wait time between retries\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Configure Nova Lite with increased tokens\n",
    "            nova_config = {\n",
    "                \"schemaVersion\": \"messages-v1\",\n",
    "                \"messages\": [{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"text\": query}]\n",
    "                }],\n",
    "                \"inferenceConfig\": {\n",
    "                    \"maxTokens\": 2048,\n",
    "                    \"temperature\": 0.5,\n",
    "                    \"topP\": 0.9,\n",
    "                    \"topK\": 20\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Try to invoke with config override\n",
    "            try:\n",
    "                answer = chain.invoke(\n",
    "                    query,\n",
    "                    config_override={\"model_kwargs\": nova_config}\n",
    "                )\n",
    "            except AttributeError:\n",
    "                # If config_override doesn't work, try direct invocation\n",
    "                answer = chain.invoke(query)\n",
    "            \n",
    "            context = [docs.page_content for docs in retriever.invoke(query)]\n",
    "            print(f\"Successfully processed query on attempt {attempt + 1}\")\n",
    "            return answer, context\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Failed after {max_retries} attempts for query: {query[:50]}...\")\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                return None, None\n",
    "            print(f\"Attempt {attempt + 1} failed, waiting {wait_time} seconds before retry...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "# Process questions one at a time with fixed delay\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "print(\"Starting to process questions...\")\n",
    "for i, query in enumerate(questions, 1):\n",
    "    print(f\"\\nProcessing question {i}/{len(questions)}\")\n",
    "    print(f\"Query: {query[:100]}...\")\n",
    "    \n",
    "    answer, context = get_model_response(query, chain, retriever)\n",
    "    if answer is not None:\n",
    "        answers.append(answer)\n",
    "        contexts.append(context)\n",
    "        print(f\"Successfully processed question {i}\")\n",
    "    else:\n",
    "        print(f\"Failed to process question {i}\")\n",
    "    \n",
    "    #if i < len(questions):\n",
    "    #    print(f\"Waiting 60 seconds before next question...\")\n",
    "    #    time.sleep(60)\n",
    "\n",
    "# Create dataset for RAGAS evaluation\n",
    "data = {\n",
    "    \"question\": questions[:len(answers)],\n",
    "    \"ground_truth\": ground_truth[:len(answers)],\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts\n",
    "}\n",
    "\n",
    "# Convert to dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Print dataset information\n",
    "print(\"\\nDataset Creation Summary:\")\n",
    "print(f\"Total questions processed: {len(dataset)} out of {len(questions)}\")\n",
    "print(f\"Columns available: {dataset.column_names}\")\n",
    "\n",
    "# Print sample entry\n",
    "if len(dataset) > 0:\n",
    "    print(\"\\nSample Entry (First Question):\")\n",
    "    print(f\"Question: {dataset[0]['question']}\")\n",
    "    print(f\"Ground Truth: {dataset[0]['ground_truth']}\")\n",
    "    print(f\"Model Answer: {dataset[0]['answer']}\")\n",
    "else:\n",
    "    print(\"\\nNo entries were successfully processed into the dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Run the following code cell to see the answers from the LLM and the ground truths for the evaluation set of questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1).What was the primary reason for the increase in net cash by operating activities for AnyCompany Financial in 2021?\n",
      "\n",
      "LLM:The primary reason for the increase in net cash provided by operating activities for AnyCompany Financial in 2021 was an increase in net income and favorable changes in operating assets and liabilities. Specifically, net cash provided by operating activities increased from $880 million in 2020 to $1,100 million in 2021. This growth was driven by the increase in net income, which rose from $1,125 million in 2020 to $1,200 million in 2021, and favorable changes in operating assets and liabilities.\n",
      "\n",
      "Ground truth: An increase in net cash provided by operating activities was primarily due to increases in net income and favorable changes in operating assets and liabilities.\n",
      "\n",
      "2).Which year did AnyCompany Financial have the highest net cash used in investing activities, what was the primary reason?\n",
      "\n",
      "LLM:AnyCompany Financial had the highest net cash used in investing activities in 2021, with a net cash used of $360 million. The primary reason for this was an increase in purchases of property, plant, and equipment, as well as marketable securities.\n",
      "\n",
      "Ground truth: AnyCompany Financial had the highest net cash used in investing activities in 2021, at $360 million, compared to $290 million in 2020 and $240 million in 2019. The primary reason was an increase in purchases of property, plant, and equipment and marketable securities.\n",
      "\n",
      "3).What was the primary source of cash inflows from financing activities for AnyCompany Financial in 2021?\n",
      "\n",
      "LLM:The primary source of cash inflows from financing activities for AnyCompany Financial in 2021 was the proceeds from the issuance of common stock, which amounted to $400 million, and the proceeds from the issuance of long-term debt, which amounted to $500 million. Combined, these activities contributed $900 million to the net cash provided by financing activities of $600 million for 2021.\n",
      "\n",
      "Ground truth: The primary source of cash inflows from financing activities for AnyCompany Financial in 2021 was an increase in proceeds from the issuance of common stock and long-term debt.\n",
      "\n",
      "4).Calculate the year-over-year percentage change in cash and cash equivalents for AnyCompany Financial from 2020 to 2021.\n",
      "\n",
      "LLM:The cash and cash equivalents at the end of 2020 for AnyCompany Financial were $350 million, and at the end of 2021, they were $480 million.\n",
      "\n",
      "To calculate the year-over-year percentage change in cash and cash equivalents from 2020 to 2021, we use the following formula:\n",
      "\n",
      "\\[ \\text{Percentage Change} = \\left( \\frac{\\text{New Value} - \\text{Old Value}}{\\text{Old Value}} \\right) \\times 100 \\]\n",
      "\n",
      "Substituting the given values:\n",
      "\n",
      "\\[ \\text{Percentage Change} = \\left( \\frac{480 - 350}{350} \\right) \\times 100 \\]\n",
      "\n",
      "\\[ \\text{Percentage Change} = \\left( \\frac{130}{350} \\right) \\times 100 \\]\n",
      "\n",
      "\\[ \\text{Percentage Change} = 0.3714 \\times 100 \\]\n",
      "\n",
      "\\[ \\text{Percentage Change} = 37.14\\% \\]\n",
      "\n",
      "Therefore, the year-over-year percentage change in cash and cash equivalents for AnyCompany Financial from 2020 to 2021 is 37.14%.\n",
      "\n",
      "Ground truth: To calculate the year-over-year percentage change in cash and cash equivalents from 2020 to 2021:     2020 cash and cash equivalents: $350 million     2021 cash and cash equivalents: $480 million     Percentage change = (2021 value - 2020 value) / 2020 value * 100     = ($480 million - $350 million) / $350 million * 100     = 37.14% increase\n",
      "\n",
      "5).With the information provided, what can you infer about AnyCompany Financial's overall financial health and growth prospects?\n",
      "\n",
      "LLM:Based on the information provided, AnyCompany Financial exhibits strong financial health and promising growth prospects. Here are some key indicators:\n",
      "\n",
      "1. **Revenue Growth**: The company reported revenues of $5,000 million in 2021, reflecting a significant 11.1% increase from $4,500 million in 2020. This consistent revenue growth underscores the company's ability to expand its market presence and effectively manage its business operations.\n",
      "\n",
      "2. **Profitability**: AnyCompany Financial's net income grew from $1,125 million in 2020 to $1,200 million in 2021. This represents a healthy profit margin and indicates that the company is generating substantial earnings from its operations.\n",
      "\n",
      "3. **Cash Position**: The company maintains a strong cash position, with $1 billion in cash and cash equivalents on its balance sheet. This liquidity provides a buffer against market volatility and supports the company's ability to invest in growth opportunities.\n",
      "\n",
      "4. **Diverse and Talented Team**: The company employs over 1,000 professionals, including investment bankers, wealth managers, asset managers, corporate financiers, and private equity investors. This diverse and experienced team is well-positioned to drive continued growth and innovation.\n",
      "\n",
      "5. **Corporate Social Responsibility**: AnyCompany Financial's commitment to corporate social responsibility, focusing on education, healthcare, and environmental sustainability, demonstrates its dedication to making a positive impact on society. This can enhance the company's reputation and brand loyalty.\n",
      "\n",
      "Given these factors, AnyCompany Financial appears to be well-positioned for continued growth and success in the financial services industry.\n",
      "\n",
      "Ground truth: Based on information provided, AnyCompany Financial appears to be in a healthy financial position with good growth prospects. The company increased its net cash provided by operating activities, indicating strong profitability and efficient management of working capital. AnyCompany Financial has been investing in long-term assets, such as property, plant, and equipment, and marketable securities, which suggests plans for future growth and expansion. The company was able to finance its growth through the issuance of common stock and long-term debt, indicating confidence from investors and lenders. Overall, AnyCompany Financial's steady increase in cash and cash equivalents over the past three years provides a strong foundation for future growth and investment opportunities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for answer in answers:\n",
    "    i=i+1\n",
    "    print(str(i)+').'+questions[i-1]+'\\n')\n",
    "    print(\"LLM:\" +answer+'\\n')\n",
    "    print (\"Ground truth: \"+ ground_truth[i-1]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.5: Evaluate the RAG application\n",
    "\n",
    "In this task, you import all the metrics you want to use from *ragas.metrics*. Then, you use the *evaluate()* function and simply pass in the relevant metrics and the prepared dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Run the following code cell to import all the metrics from *ragas.metrics* and use the *evaluate()* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')   # ignore warnings related to pydantic v1 to v2 migration\n",
    "logging.getLogger('root').setLevel(logging.CRITICAL)\n",
    "\n",
    "from datasets import Dataset\n",
    "if not hasattr(Dataset, 'from_list'):\n",
    "    def from_list_compatibility(data_list):\n",
    "        if isinstance(data_list, list) and len(data_list) > 0 and isinstance(data_list[0], dict):\n",
    "            keys = data_list[0].keys()\n",
    "            data_dict = {key: [item[key] for item in data_list] for key in keys}\n",
    "            return Dataset.from_dict(data_dict)\n",
    "        return Dataset.from_dict({})\n",
    "    Dataset.from_list = staticmethod(from_list_compatibility)\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_entity_recall,\n",
    "    answer_similarity,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "from ragas.metrics.critique import (\n",
    "harmfulness, \n",
    "maliciousness, \n",
    "coherence, \n",
    "correctness, \n",
    "conciseness\n",
    ")\n",
    "\n",
    "#specify the metrics here\n",
    "metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        context_entity_recall,\n",
    "        answer_similarity,\n",
    "        answer_correctness,\n",
    "        harmfulness, \n",
    "        maliciousness, \n",
    "        coherence, \n",
    "        correctness, \n",
    "        conciseness\n",
    "    ]\n",
    "\n",
    "try:\n",
    "    result = evaluate(\n",
    "        dataset=dataset,\n",
    "        metrics=metrics,\n",
    "        llm=llm_for_evaluation,\n",
    "        embeddings=bedrock_embeddings,\n",
    "    )\n",
    "    df = result.to_pandas()\n",
    "except Exception as e:\n",
    "    # Handle any exceptions that occur during the evaluation\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-exclamation-circle\" style=\"color:#7C5AED\"></i> **Caution:** You can safely ignore the warnings in the above output. Make sure that evaluation is completed 100% before proceeding further. This step takes approximately 7 to 10 minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Run the below code cell to see the resulting RAGAS scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 10\n",
    "df.style.set_sticky(axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Run the below code cell to export the resulting RAGAS scores in excel format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.style.to_excel('styled.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** After successfully running the above code cell, a file named *styled.xlsx* should be available for review in the left navigation pane under the **en_us** folder. If the file takes too long to open, right click the file and choose **Open in New Browser Tab**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** Please note that the scores above gives a relative idea on the performance of your RAG application and should be used with caution and not as standalone scores. Also note, that you have used only 5 question/answer pairs for evaluation. As a best practice, you should use enough data to cover different aspects of your document for evaluating model.\n",
    "\n",
    "Based on the scores, you can review other components of your RAG workflow to further optimize the scores. Few recommended options are to review your chunking strategy, prompt instructions, adding more numberOfResults for additional context and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"far fa-thumbs-up\" style=\"color:#008296\"></i> **Task complete:** You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with Task 4."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
