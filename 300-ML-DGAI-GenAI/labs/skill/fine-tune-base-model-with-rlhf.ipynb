{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f10b19-c061-42b2-8b42-e3cbafa3b1da",
   "metadata": {},
   "source": [
    "# Fine-Tune FLAN-T5 with Reinforcement Learning (PPO) and PEFT to Generate Less Toxic Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef668a-9c51-489b-be47-a07a09ef2289",
   "metadata": {},
   "source": [
    "본 실습에서는 Meta AI의 혐오 발언 보상 모델을 사용하여 FLAN-T5 모델을 미세 조정하여 유해성을 줄입니다. 이 보상 모델은 주어진 텍스트에 대해 *혐오* 또는 *혐오 아님*으로 예측하는 이진 분류기입니다. 근접 정책 최적화(PPO)를 사용하여 모델의 유해성을 미세 조정하고 줄입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3e8d26-1ea7-4ba7-adbf-f5c2e838d103",
   "metadata": {},
   "source": [
    "#### _Code Cell 1_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c46008-542c-45f1-a80a-7a35d739f97e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torch==2.4.1.post100\n",
    "%pip install transformers==4.47.0\n",
    "%pip install datasets==3.2.0\n",
    "%pip install accelerate==1.2.0\n",
    "%pip install evaluate==0.4.3\n",
    "%pip install trl==0.8.0\n",
    "%pip install rouge_score==0.1.2\n",
    "%pip install loralib==0.1.2\n",
    "%pip install peft==0.14.0\n",
    "%pip install -q awswrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab0784-cd59-4505-aaed-ff7d75ff978b",
   "metadata": {},
   "source": [
    "#### _Code Cell 2_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c20bed-6a30-4847-a507-02969ecb4465",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 04:56:45.979087: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
    "\n",
    "# trl: Transformer Reinforcement Learning library\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import peft\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# tqdm library makes the loops show a smart progress meter.\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76eea84-8e3a-4487-9692-613977e6c8e3",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 – Load the FLAN-T5 model, and prepare the reward model and toxicity evaluator\n",
    "<a name='2.1'></a>\n",
    "### 2.1 – Load the dataset and the FLAN-T5-BASE model. The model has been trained and fine-tuned to follow prompted instructions.\n",
    "#### _Code Cell 3_ ####\n",
    "Hugging Face의 DialogSum 데이터셋을 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b058b52b-ec4d-4426-8d71-91e898f727f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1737d1f75d3948d98c6c9da81e3d409c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3257ab32ff7c4f3295022f0573ed02c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57d5a4efd8442b4a7d9b1fea9ba5f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce209ac79994c0081d70a9859d698c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4e8f498dd547728abc0a1debabc355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f393cd3ebb194a77ae2a1c5a80f6926e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d586bbbc27c45d9a5e34bf777b55bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "model_name=\"google/flan-t5-base\"\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset_original = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d30d6-6f81-4e52-a81a-3057163ddb0e",
   "metadata": {},
   "source": [
    "다음 단계는 데이터셋을 전처리하는 것입니다. 데이터셋의 일부만 가져와 특정 길이의 대화만 필터링합니다(예시를 충분히 길면서도 읽기 쉽게 만들기 위해). 그런 다음 각 대화를 명령어로 감싸고 프롬프트를 토큰화합니다. 토큰 ID는 `*input_ids*` 필드에, 디코딩된 프롬프트는 `*query*` 필드에 저장합니다.\n",
    "\n",
    "이 모든 과정을 아래 셀에 단계별로 작성할 수도 있지만, 모든 작업을 `*build_dataset*` 함수로 구성하는 것이 좋습니다.\n",
    "\n",
    "#### _코드 셀 4_ ####\n",
    "\n",
    "```text\n",
    "Hugging Face Dataset 로드\n",
    " → 대화 길이 필터링\n",
    " → Instruction Prompt로 감싸기\n",
    " → Tokenize (input_ids, query 생성)\n",
    " → PyTorch 텐서 변환\n",
    " → Train / Test Split\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51469abe-4d72-4093-a6c6-8e04e19f09eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c16b861c2cb443d9e98f58d486cd97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30652b6974fe404f90fc8d1c03f83894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5571fbe702d49638bf9d99bcdb50daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97cac20343974452b9aeb44efcaa2c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c912f5ae094041aa698685549a1aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7835fefaed4643dfa46626a986f7b1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18e1e6c684d45139e07846e9a840a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f391564df7493a8b8d2eac8d1d30c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 8017\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 2005\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "\n",
    "def build_dataset(model_name,\n",
    "                  dataset_name,\n",
    "                  input_min_text_length, \n",
    "                  input_max_text_length):\n",
    "\n",
    "    # Load dataset (the \"train\" part only is enough for this lab).\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    \n",
    "    # Filter the dialogues of length between input_min_text_length and input_max_text_length characters.\n",
    "    # 대화 길이 기준 필터링\n",
    "    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
    "\n",
    "    # Prepare the tokenizer. Setting device_map=\"auto\" allows to switch between GPU and CPU automatically.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\",force_download=True)\n",
    "    \n",
    "    def tokenize(sample):\n",
    "        \n",
    "        # Wrap each dialogue with the instruction.\n",
    "        prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{sample[\"dialogue\"]}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
    "        \n",
    "        # This must be called \"query\", which is a requirement of our PPO library.\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    # Tokenize each dialogue.\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    dataset.set_format(type=\"torch\")\n",
    "    \n",
    "    # Split the dataset into train and test parts.\n",
    "    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n",
    "\n",
    "    return dataset_splits\n",
    "\n",
    "dataset = build_dataset(model_name=model_name,\n",
    "                        dataset_name=huggingface_dataset_name,\n",
    "                        input_min_text_length=200, \n",
    "                        input_max_text_length=1000)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d03155e-649b-45bb-a5a0-94edd682c069",
   "metadata": {
    "tags": []
   },
   "source": [
    "To save time in this lab, we fine-tuned the model by using PEFT with summarization instructions. The training in the notebook was done on a subset of data. We downloaded this model and saved it to the model-checkpoint-files.zip archive. Next, unzip that file and use it as a checkpoint.\n",
    "\n",
    "#### _Code Cell 5_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d44a53-ea1f-4fa5-89e7-d46e37d19935",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  model-checkpoint-files.zip\n",
      "  inflating: peft-dialogue-summary-checkpoint-from-s3/adapter_config.json  \n",
      "  inflating: peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin  \n",
      "  inflating: peft-dialogue-summary-checkpoint-from-s3/special_tokens_map.json  \n",
      "  inflating: peft-dialogue-summary-checkpoint-from-s3/tokenizer_config.json  \n",
      "  inflating: peft-dialogue-summary-checkpoint-from-s3/tokenizer.json  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!unzip -o model-checkpoint-files.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8bea4-addd-4b29-b3af-6db6ea2baeb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "List the model item and check its size. (It's less than 15 Mb.)\n",
    "#### _Code Cell 6_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4288240d-764b-4c49-8df7-b30b9277adbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--. 1 sagemaker-user users 14M Jun 15  2023 ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4226923-67c0-4ea6-8e47-030136b2f191",
   "metadata": {},
   "source": [
    "Prepare a function to pull out the number of model parameters.\n",
    "#### _Code Cell 7_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f06806-a194-4c14-b64d-e31afd7b658c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e06a57-fb80-4f8c-a967-4c7c42a7bfda",
   "metadata": {
    "tags": []
   },
   "source": [
    "Add the adapter to the original FLAN-T5 model. Now, you must pass both to the constructed PEFT model, also putting `is_trainable=True`.\n",
    "\n",
    "#### _Code Cell 8_ ####\n",
    "\n",
    "대형 Seq2Seq 모델(FLAN-T5)을 전체 파인튜닝하지 않고,\n",
    "LoRA 어댑터만 학습 가능 상태로 붙여서 효율적으로 미세조정하는 코드\n",
    "\n",
    "LoRA란 : Low-Rank Adaptation\n",
    "\n",
    "|파라미터|의미|\n",
    "|---|---|\n",
    "|r=32 | LoRA rank (저랭크 차원) |\n",
    "|lora_alpha=32 | LoRA scaling factor (출력 스케일) |\n",
    "|target_modules=[\"q\",\"v\"] | Attention의 Query / Value projection에만 LoRA 적용 |\n",
    "|lora_dropout=0.05 | LoRA 경로에 dropout |\n",
    "|bias=\"none\" | 기존 bias 파라미터는 학습 안 함 |\n",
    "|task_type=SEQ_2_SEQ_LM | Encoder-Decoder 모델 (T5 계열) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1a94b14-b375-45e7-9e49-a7f2c341b4ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66214b935d9484383705501e4ec7e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea86a8ff190409c8e37fd0a710edaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40df4c4be4a84e268c22864f7c7e7a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, \n",
    "                                              torch_dtype=torch.bfloat16)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model, \n",
    "                                       './peft-dialogue-summary-checkpoint-from-s3/', \n",
    "                                       lora_config=lora_config,\n",
    "                                       torch_dtype=torch.bfloat16, \n",
    "                                       device_map=\"auto\",                                       \n",
    "                                       is_trainable=True)\n",
    "\n",
    "print(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a950ae8a-76b9-4951-9c78-9ac7a6349e17",
   "metadata": {},
   "source": [
    "이 실습에서는 강화 학습(RL)을 사용하여 대규모 언어 모델(LLM)을 미세 조정하는 준비를 합니다. RL은 실습의 다음 섹션에서 간략하게 다루지만, 이 단계에서는 명령으로 미세 조정된 PEFT 모델을 입력으로 사용하여 근접 정책 최적화(PPO) 모델만 준비하면 됩니다. PPO는 보상 모델에 대해 RL 정책을 최적화하는 데 사용됩니다.\n",
    "\n",
    "#### _코드 셀 9_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e86bab0-6dee-4dff-a754-b584ed962723",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO model parameters to be updated (ValueHead + 769 params):\n",
      "\n",
      "trainable model parameters: 3539713\n",
      "all model parameters: 251117569\n",
      "percentage of trainable model parameters: 1.41%\n",
      "\n",
      "ValueHead(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (summary): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,                                                               \n",
    "                                                               torch_dtype=torch.bfloat16,\n",
    "                                                               is_trainable=True)\n",
    "\n",
    "print(f'PPO model parameters to be updated (ValueHead + 769 params):\\n{print_number_of_trainable_model_parameters(ppo_model)}\\n')\n",
    "print(ppo_model.v_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913ef05-737e-4cdf-9bac-467ee6cf9f76",
   "metadata": {},
   "source": [
    "During PPO, only a few parameters are updated; specifically, the parameters of the `ValueHead.` For more information about this class of models, see the [Hugging Face documentation](https://huggingface.co/docs/trl/main/en/models#trl.create_reference_model). The number of trainable parameters can be computed as $(n+1)*m$, where $n$ is the number of input units (here $n=768$) and $m$ is the number of output units (you have $m=1$). The $+1$ term in the equation takes into account the bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7e2df-0c75-4bd0-bf8d-1f1545ef864e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, create a frozen copy of the PPO, which will be a reference model and not fine-tuned. The reference model represents the LLM before detoxification. None of the parameters of the reference model are updated during PPO training. This is on purpose.\n",
    "\n",
    "#### _Code Cell 10_ ####\n",
    "\n",
    "**PPO 기반 RLHF 학습에서 사용되는 “Reference Model(참조 모델)”을 생성하고,\n",
    "그 모델이 실제로 학습되지 않는다는 것(= 파라미터가 업데이트되지 않음)**을 확인하는 단계\n",
    "\n",
    "PPO 학습 중, 정책 모델이 “너무 멀리 벗어나지 않도록” 기준점 역할을 하는\n",
    "고정된(Freeze) 참조 모델을 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18a9b30a-ad14-4189-8088-d4de447fe247",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 0\n",
      "all model parameters: 251117569\n",
      "percentage of trainable model parameters: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref_model = create_reference_model(ppo_model)\n",
    "\n",
    "print(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a14848-e83d-4fdd-bc68-eb770c5951d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Everything is set! You are ready to prepare the reward model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfdf1f7-3509-4adc-812a-2b22bd330137",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 – 보상 모델 준비\n",
    "\n",
    "*강화 학습(RL)*은 에이전트가 누적 보상을 최대화하는 것을 목표로 하는 환경에서 행동을 취하는 머신 러닝 유형 중 하나입니다. 에이전트의 행동은 *정책*으로 정의됩니다. RL의 목표는 에이전트가 *보상 함수*를 최대화하는 최적 또는 거의 최적의 정책을 학습하는 것입니다.\n",
    "\n",
    "[이전 섹션](#2.1)에서 초기 정책은 PEFT 모델을 기반으로 합니다. 이는 해독 전의 LLM입니다. 그런 다음 사람 라벨러에게 출력의 독성에 대한 피드백을 요청할 수 있지만, 사람 라벨러는 전체 미세 조정 과정에 많은 비용이 들 수 있습니다. 이러한 비용을 피하는 실용적인 방법은 에이전트가 대화 요약의 독성을 제거하도록 유도하는 보상 모델을 사용하는 것입니다. 직관적인 접근 방식은 '혐오하지 않음(nothate)'과 '혐오(hate)'라는 두 가지 범주에 대한 감정 분석을 수행하고, '혐오하지 않음'이 출력될 확률이 높을수록 더 높은 보상을 제공하는 것입니다.\n",
    "\n",
    "이 실습에서는 보상 모델로 [Meta AI의 RoBERTa 기반 혐오 발언 모델](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target)을 사용합니다. 이 모델은 로짓 값을 출력하고, 두 범주('혐오하지 않음'과 '혐오')에 대한 확률을 예측합니다. 출력값이 '혐오하지 않음'일 경우의 로짓 값은 긍정적인 보상으로 간주됩니다. 그런 다음, 이러한 보상 값을 사용하여 PPO(Preferred Probability of Ethics)를 통해 모델을 미세 조정합니다.\n",
    "\n",
    "RoBERTa 모델에 필요한 모델 클래스의 인스턴스를 생성하세요. 모델 테스트를 위해 토크나이저도 로드해야 합니다. 모델 레이블 *0*은 '혐오하지 않음' 클래스에, 레이블 *1*은 '혐오' 클래스에 해당합니다.\n",
    "\n",
    "#### _코드 셀 11_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f038a9f-e04b-49bc-8923-8ef3816919ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c938e6bfc624fa994c2bf50c45217a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575910b09fb14d5e8cc22666b0bff867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2d0c0c514a4a87adfce40fab04eb43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8390eb9bd1f641cba589f824e12bcfc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6140ed51804681b7cef4b1566311a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/816 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f5d7492e614ab2a5992073b14cc793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'nothate', 1: 'hate'}\n"
     ]
    }
   ],
   "source": [
    "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d68799-a6e8-42d7-8d61-002e47210c18",
   "metadata": {
    "tags": []
   },
   "source": [
    "Take some non-toxic text, tokenize it, and pass it to the model. Print the output logits, probabilities, and the corresponding reward that will be used for fine-tuning.\n",
    "#### _Code Cell 12_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f4e6a05-2398-4ca7-a176-d5a1ff27fe39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [4.6532111167907715, -4.178226947784424]\n",
      "probabilities [not hate, hate]: [0.9998539686203003, 0.0001460467028664425]\n",
      "reward (value of \"not hate\" logit): [4.6532111167907715]\n"
     ]
    }
   ],
   "source": [
    "non_toxic_text = \"You are a great person and i like you.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# get the logits for \"not hate\" - this is the reward!\n",
    "not_hate_index = 0\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward (value of \"not hate\" logit): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f729c5-98c3-4745-96e8-3484670215db",
   "metadata": {},
   "source": [
    "The following comment will have a low reward because it is more toxic.\n",
    "\n",
    "#### _Code Cell 13_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ffc81e4-b220-4f4e-95ec-98ac418612d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [-2.064443349838257, 1.665043830871582]\n",
      "probabilities [not hate, hate]: [0.023442404344677925, 0.9765575528144836]\n",
      "reward (value of \"not hate\" logit): [-2.064443349838257]\n"
     ]
    }
   ],
   "source": [
    "toxic_text = \"You are a terrible person and i hate you.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logits = toxicity_model(toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# Get the logits for \"not hate\" - this is the reward!\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist() \n",
    "print(f'reward (value of \"not hate\" logit): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6e656e-fefe-4623-8bbb-472a8cf1c3c5",
   "metadata": {},
   "source": [
    "Set up the Hugging Face inference pipeline to streamline the code for the toxicity reward model.\n",
    "\n",
    "#### _Code Cell 14_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73aab8c6-33eb-4bf4-a816-3866fc3460af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model output for non-toxic text:\n",
      "[{'label': 'nothate', 'score': 4.6532111167907715}, {'label': 'hate', 'score': -4.178226947784424}]\n",
      "[{'label': 'nothate', 'score': 0.9998539686203003}, {'label': 'hate', 'score': 0.0001460467028664425}]\n",
      "\n",
      "Reward model output for toxic text:\n",
      "[{'label': 'hate', 'score': 1.665043830871582}, {'label': 'nothate', 'score': -2.064443349838257}]\n",
      "[{'label': 'hate', 'score': 0.9765575528144836}, {'label': 'nothate', 'score': 0.023442404344677925}]\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", \n",
    "                          model=toxicity_model_name, \n",
    "                          device=device)\n",
    "reward_logits_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "reward_probabilities_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"softmax\", # Set to \"softmax\" to apply softmax and retrieve probabilities.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "print(\"Reward model output for non-toxic text:\")\n",
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\n",
    "print(\"\\nReward model output for toxic text:\")\n",
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21302d74-59d8-451f-b287-e86245bf3324",
   "metadata": {},
   "source": [
    "The outputs are the logits for both *nothate* (positive) and *hate* (negative) classes. But PPO uses logits only from the *nothate* class as the positive reward signal to help detoxify the LLM outputs.\n",
    "\n",
    "#### _Code Cell 15_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36ff3925-70c4-495c-ae26-68e2fc36296b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'nothate', 'score': 4.6532111167907715}, {'label': 'hate', 'score': -4.178226947784424}]\n",
      "[{'label': 'nothate', 'score': 0.9998539686203003}, {'label': 'hate', 'score': 0.0001460467028664425}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3161a-ab3e-4c80-a322-036f5a983d62",
   "metadata": {},
   "source": [
    "#### _Code Cell 16_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d11618b-5887-489a-b390-2139e364987f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'hate', 'score': 1.665043830871582}, {'label': 'nothate', 'score': -2.064443349838257}]\n",
      "[{'label': 'hate', 'score': 0.9765575528144836}, {'label': 'nothate', 'score': 0.023442404344677925}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56513033-9bb1-41d5-81e2-54d1249c5c89",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3 – Evaluate toxicity\n",
    "\n",
    "To evaluate the model before and after fine-tuning (detoxification) you must set up the [toxicity evaluation metric](https://huggingface.co/spaces/evaluate-measurement/toxicity). The *toxicity score* is a decimal value between 0 and 1 where 1 is the highest toxicity.\n",
    "\n",
    "#### _Code Cell 17_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7de8e99d-60ea-48a2-bdf5-817f80b48979",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5987ec43d69c418d937d5ef5aa9d0b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "toxicity_evaluator = evaluate.load(\"toxicity\", \n",
    "                                    toxicity_model_name,\n",
    "                                    module_type=\"measurement\",\n",
    "                                    toxic_label=\"hate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840fbc47-c5c2-469a-b5f2-6407e8f0bfde",
   "metadata": {
    "tags": []
   },
   "source": [
    "Using the same sentences as in section [2.2](#2.2), try to calculate their toxicity. It's no surprise that the toxicity scores are the probabilities of the *hate* class returned directly from the reward model.\n",
    "\n",
    "#### _Code Cell 18_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5298f91c-30d1-4d17-95a7-553952ac97b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity score for non-toxic text:\n",
      "[0.0001460467028664425]\n",
      "\n",
      "Toxicity score for toxic text:\n",
      "[0.9765575528144836]\n"
     ]
    }
   ],
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    non_toxic_text\n",
    "])\n",
    "\n",
    "print(\"Toxicity score for non-toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])\n",
    "\n",
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    toxic_text\n",
    "])\n",
    "\n",
    "print(\"\\nToxicity score for toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e835b-14b9-4646-b1c9-c975ef3ea944",
   "metadata": {
    "tags": []
   },
   "source": [
    "This evaluator can be used to compute the toxicity of the dialogues prepared in section [2.1](#2.1). You must pass the test dataset, `dataset[\"test\"]` (the same tokenizer that was used in that section), the frozen PEFT model prepared in section [2.2](#2.2), and the toxicity evaluator. For convenience, you can wrap the required steps in the function, `evaluate_toxicity`. \n",
    "#### _Code Cell 19_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "316ab128-33ff-4a1e-8936-47bfa29d48a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_toxicity(model, \n",
    "                      toxicity_evaluator, \n",
    "                      tokenizer, \n",
    "                      dataset, \n",
    "                      num_samples):\n",
    "\n",
    "    max_new_tokens=100\n",
    "\n",
    "    toxicities = []\n",
    "    input_texts = []\n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        input_text = sample[\"query\"]\n",
    "\n",
    "        if i > num_samples:\n",
    "            break\n",
    "            \n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids\n",
    "        \n",
    "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
    "                                             tok_k=0.0,\n",
    "                                             top_p=1.0,\n",
    "                                             do_sample=True)\n",
    "\n",
    "        response_token_ids = model.generate(input_ids=input_ids,\n",
    "                                            generation_config=generation_config)\n",
    "        \n",
    "        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n",
    "\n",
    "        toxicities.extend(toxicity_score[\"toxicity\"])\n",
    "\n",
    "    # Compute mean and std using np.\n",
    "    mean = np.mean(toxicities)\n",
    "    std = np.std(toxicities)\n",
    "        \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed269c3-dbd7-4d45-bc44-c6ab6d4ae141",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, perform the calculation of the model toxicity before fine-tuning (detoxification).\n",
    "\n",
    "#### _Code Cell 20_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c11ede15-dc1a-4a7e-a60d-b9cadfc7d876",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:18,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity [mean, std] before detox: [0.03055911746130071, 0.0324480148665644]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "mean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model, \n",
    "                                                                          toxicity_evaluator=toxicity_evaluator, \n",
    "                                                                          tokenizer=tokenizer, \n",
    "                                                                          dataset=dataset[\"test\"], \n",
    "                                                                          num_samples=10)\n",
    "\n",
    "print(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba81c90-1ac8-4403-ac1a-d4c75c6df4f0",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 – Perform fine-tuning to detoxify the summaries\n",
    "Optimize an RL policy against the reward model by using Proximal Policy Optimization (PPO)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5516e318-8fce-4ca7-bf19-b7baf5255480",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 – Initialize the PPOTrainer\n",
    " \n",
    "Set up the configuration parameters. Load the `ppo_model` and the tokenizer. You will also load a frozen version of the model, `ref_model`. The first model is optimized, and the second model serves as a reference to calculate the KL-divergence from the starting point. This works as an additional reward signal in the PPO training to ensure that the optimized model does not deviate too much from the original LLM.\n",
    "\n",
    "#### _Code Cell 21_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b7be1c0-382a-4fe2-8174-470f3e333e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate=1.41e-5\n",
    "max_ppo_epochs=1\n",
    "mini_batch_size=4\n",
    "batch_size=16\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=model_name,    \n",
    "    learning_rate=learning_rate,\n",
    "    ppo_epochs=max_ppo_epochs,\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "ppo_trainer = PPOTrainer(config=config, \n",
    "                         model=ppo_model, \n",
    "                         ref_model=ref_model, \n",
    "                         tokenizer=tokenizer, \n",
    "                         dataset=dataset[\"train\"], \n",
    "                         data_collator=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c7352d-53fd-41d8-b438-cfcc9979b0c7",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 – Fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cac21fb-fea5-4e80-a741-87f35ae72c62",
   "metadata": {},
   "source": [
    "The fine-tuning loop consists of the following main steps:\n",
    "1. Get the query responses from the policy LLM (PEFT model).\n",
    "2. Get sentiments for query/responses from the hate speech RoBERTa model.\n",
    "3. Optimize the policy with PPO by using the (query, response, reward) triplet.\n",
    "\n",
    "The operation is running if the following metrics appear:\n",
    "* `objective/kl`: minimize kl divergence\n",
    "* `ppo/returns/mean`: maximize mean returns\n",
    "* `ppo/policy/advantages_mean`: maximize advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c66ed-a702-4aa8-b473-92d7fa106ff8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### _Code Cell 22 (This code cell can take up to 10 minutes to be completed.)_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bc55397-92b8-4f61-9ec2-c8b39d5f8962",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "1it [01:06, 66.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 35.69118881225586\n",
      "ppo/returns/mean: -0.7093673348426819\n",
      "ppo/policy/advantages_mean: 0.018999043852090836\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [02:09, 64.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 30.212993621826172\n",
      "ppo/returns/mean: -0.46174338459968567\n",
      "ppo/policy/advantages_mean: 0.0044395942240953445\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [03:10, 62.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 28.143619537353516\n",
      "ppo/returns/mean: -0.42218393087387085\n",
      "ppo/policy/advantages_mean: 0.023562896996736526\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [04:04, 59.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 23.815454483032227\n",
      "ppo/returns/mean: -0.2045554220676422\n",
      "ppo/policy/advantages_mean: 0.019497783854603767\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [05:01, 58.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 23.983890533447266\n",
      "ppo/returns/mean: -0.018388643860816956\n",
      "ppo/policy/advantages_mean: 0.00619332492351532\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [06:05, 60.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 24.368545532226562\n",
      "ppo/returns/mean: -0.09612344950437546\n",
      "ppo/policy/advantages_mean: -0.004671149887144566\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [07:05, 60.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 25.428083419799805\n",
      "ppo/returns/mean: -0.16490083932876587\n",
      "ppo/policy/advantages_mean: 0.010492168366909027\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [08:01, 59.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 27.015817642211914\n",
      "ppo/returns/mean: -0.2909148335456848\n",
      "ppo/policy/advantages_mean: -0.0071167126297950745\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [09:07, 61.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 25.713504791259766\n",
      "ppo/returns/mean: -0.16972775757312775\n",
      "ppo/policy/advantages_mean: 0.017213433980941772\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [10:06, 60.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 25.09076499938965\n",
      "ppo/returns/mean: -0.27405688166618347\n",
      "ppo/policy/advantages_mean: 0.015224728733301163\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_min_length = 100\n",
    "output_max_length = 400\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 5,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "\n",
    "reward_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"none\", # You want the raw logits without softmax.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "max_ppo_steps = 10\n",
    "\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    # Break when you reach max_steps.\n",
    "    if step >= max_ppo_steps:\n",
    "        break   \n",
    "\n",
    "    prompt_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from FLAN-T5/PEFT LLM.\n",
    "    summary_tensors = []\n",
    "\n",
    "    for prompt_tensor in prompt_tensors:\n",
    "        max_new_tokens = output_length_sampler()        \n",
    "            \n",
    "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
    "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
    "        \n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "        \n",
    "    # This needs to be called \"response\".\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
    "\n",
    "    # Compute reward outputs.\n",
    "    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]    \n",
    "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
    "\n",
    "    # You use the \"nothate\" item because this is the score for the positive \"nothate\" class.\n",
    "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]    \n",
    "\n",
    "    # Run the PPO step.\n",
    "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
    "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
    "    \n",
    "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
    "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
    "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
    "    print('-'.join('' for x in range(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903f5df-a9de-41eb-b239-38bc367b5654",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "### 3.3 – Evaluate the model quantitatively\n",
    "\n",
    "Load the PPO/PEFT model back in from the disk, and use the test dataset split to evaluate the toxicity score of the RL-fine-tuned model.\n",
    "\n",
    "#### _Code Cell 23_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b093d43-6197-4cc0-b933-29030479a7d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:17,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity [mean, std] after detox: [0.05145865214184265, 0.08639719215614429]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_after_detoxification, std_after_detoxification = evaluate_toxicity(model=ppo_model, \n",
    "                                                                        toxicity_evaluator=toxicity_evaluator, \n",
    "                                                                        tokenizer=tokenizer, \n",
    "                                                                        dataset=dataset[\"test\"], \n",
    "                                                                        num_samples=10)\n",
    "print(f'toxicity [mean, std] after detox: [{mean_after_detoxification}, {std_after_detoxification}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42895cc-7bbf-45e1-a7c7-78ee29cd8009",
   "metadata": {
    "tags": []
   },
   "source": [
    "Compare the toxicity scores of the reference model (before detoxification) and the fine-tuned model (after detoxification).\n",
    "\n",
    "#### _Code Cell 24_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77cc3af2-6600-4673-874b-917c05247ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage improvement of toxicity score after detoxification:\n",
      "mean: 68.39%\n",
      "std: 166.26%\n"
     ]
    }
   ],
   "source": [
    "mean_improvement = (mean_after_detoxification - mean_before_detoxification) / mean_before_detoxification\n",
    "std_improvement = (std_after_detoxification - std_before_detoxification) / std_before_detoxification\n",
    "\n",
    "\n",
    "print(f'Percentage improvement of toxicity score after detoxification:')\n",
    "print(f'mean: {mean_improvement*100:.2f}%')\n",
    "print(f'std: {std_improvement*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66030581-b6f7-41d7-a7e6-2466226833be",
   "metadata": {},
   "source": [
    "<a name='3.4'></a>\n",
    "### 3.4 – Evaluate the model qualitatively\n",
    "\n",
    "Here are some examples to inspect from the test dataset. You can compare the original `ref_model` to the fine-tuned (detoxified) `ppo_model` by using the toxicity evaluator.\n",
    "\n",
    "#### _Code Cell 25_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22cc8313-20ae-4d32-855e-9b2866fa3085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:03<00:00,  3.15s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "compare_results = {}\n",
    "\n",
    "df_batch = dataset[\"test\"][0:batch_size]\n",
    "\n",
    "compare_results[\"query\"] = df_batch[\"query\"]\n",
    "prompt_tensors = df_batch[\"input_ids\"]\n",
    "\n",
    "summary_tensors_ref = []\n",
    "summary_tensors = []\n",
    "\n",
    "# Get response from the PPO and base model.\n",
    "for i in tqdm(range(batch_size)):\n",
    "    gen_len = output_length_sampler()\n",
    "    generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "    \n",
    "    summary = ref_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors_ref.append(summary)\n",
    "\n",
    "    summary = ppo_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors.append(summary)\n",
    "\n",
    "# Decode responses.\n",
    "compare_results[\"response_before\"] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\n",
    "compare_results[\"response_after\"] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n",
    "\n",
    "# Sentiment analysis of query/response pairs before/after.\n",
    "texts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\n",
    "rewards_before = sentiment_pipe(texts_before, **reward_kwargs)\n",
    "compare_results[\"reward_before\"] = [reward[not_hate_index][\"score\"] for reward in rewards_before]\n",
    "\n",
    "texts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\n",
    "rewards_after = sentiment_pipe(texts_after, **reward_kwargs)\n",
    "compare_results[\"reward_after\"] = [reward[not_hate_index][\"score\"] for reward in rewards_after]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b70892-4c22-4bed-9d1e-9da3f4f0c97f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Store and review the results in a DataFrame.\n",
    "\n",
    "#### _Code Cell 26_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e06fff0f-9dba-4517-9424-a5ebd81e8f49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response_before</th>\n",
       "      <th>response_after</th>\n",
       "      <th>reward_before</th>\n",
       "      <th>reward_after</th>\n",
       "      <th>reward_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summarize the following conversation. #Person1#: I'd like to have this cashed, please. #Person2#: Please put you name and address here. May I see your passport? #Person1#: Yes. #Person2#: How would you like it? #Person1#: Ten hundreds and ten twenties, and the rest in small change, please. #Person2#: OK. Here you are. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# desires to have their work cashed and sends their passport to #Person2# as an addition to their paycheck.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to have a cashed from online shop with 500 guest, who cashed at last printing. #Person2# will inve\"ng it.&lt;/s&gt;</td>\n",
       "      <td>1.416823</td>\n",
       "      <td>2.081782</td>\n",
       "      <td>0.664959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summarize the following conversation. #Person1#: How much are you asking for this? #Person2#: I'm offering them to you at 150 yuan a piece. Is that all right? #Person1#: Is tax already included in their price? #Person2#: Yes. Our price can't be matched. #Person1#: Would you consider a volume discount? #Person2#: If you buy 1, 000 or more, you'll get a 10 % discount. #Person1#: I'll accept your offer. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2#'s offering 100 yuan a piece and lets #Person1# look for a volume discount. #Person1# will accept the offer and accepts the offer.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# is making a purchase from #Person2# for 150 yuan with include-pay.&lt;/s&gt;</td>\n",
       "      <td>2.454588</td>\n",
       "      <td>3.005260</td>\n",
       "      <td>0.550672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Could you help me, Sir? My flight got in 15 minutes ago. Everyone else has picked up the luggage but mine hasn't come through. #Person2#: I'm sorry, Madam, I'll go and find out if there is any more to come. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to let #Person2# help who hasn't picked up his luggage yet.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; The flight to #Person1# doesn't make it through as it hasn't happened.&lt;/s&gt;</td>\n",
       "      <td>2.284972</td>\n",
       "      <td>2.834608</td>\n",
       "      <td>0.549636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Summarize the following conversation. #Person1#: I'm forming a music band. #Person2#: Do you already know how to play an instrument? #Person1#: Uh. . . Yeah! I'Ve told you a thousand times that I'm learning to play the drums. Now that I know how to play well, I would like to form a rock band. #Person2#: Aside from yourself, who are the other members of the band? #Person1#: We have a guy who plays guitar, and another who plays bass. Although we still haven't found anyone to be our singer. You...</td>\n",
       "      <td>&lt;pad&gt; #Person1# is forming a music band and wants to form a rock band with their other musicians. #Person1# also knows dogs and coaches. #Person2# can audition at #Person1#'s house this weekend but #Person1# does not have enough room.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; people are forming a rock band and they want to know their other players and sometal...&lt;/s&gt;</td>\n",
       "      <td>2.730146</td>\n",
       "      <td>2.942207</td>\n",
       "      <td>0.212061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Summarize the following conversation. #Person1#: It smells like an ashtray in here! #Person2#: Hi honey! What's wrong? Why do you have that look on your face? #Person1#: What's wrong? I thought we agreed that you were gonna quit smoking. #Person2#: No! I said I was going to cut down which is very different. You can't just expect me to go cold turkey overnight! #Person1#: Look, there are other ways to quit. You can try the nicotine patch, or nicotine chewing gum. We spend a fortune on cigaret...</td>\n",
       "      <td>&lt;pad&gt; #Person2# doesn't want to quit smoking, though #Person1# advises her to look for the other options. #Person2#'s thinking of quitting only because she doesn't have the willpower to quit smoking.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# smells like an ashtray with a look that looks like an ashtray. #Person2# tells #Person1# he's afraid of smoking and wants a divorce.&lt;/s&gt;</td>\n",
       "      <td>1.486577</td>\n",
       "      <td>1.666340</td>\n",
       "      <td>0.179763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Summarize the following conversation. #Person1#: So how did you like the restaurant? #Person2#: Actually, it could have been better. #Person1#: What didn't you like about it? #Person2#: It is a new restaurant. I don't think they have their act together yet. #Person1#: What did you think about the food? #Person2#: I felt that the food was pretty mediocre. #Person1#: The service wasn't that great, either. #Person2#: I agree. The service was not good. #Person1#: Do you think that you want to tr...</td>\n",
       "      <td>&lt;pad&gt; #Person2# loves the restaurant. #Person1# asks #Person2# what #Person2# does and #Person2# disagrees about the service and the food. #Person2#'s satisfied with this restaurant.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# looks for him. He says he liked the restaurant that was good and also regrets the food and the host, but he thinks too late.&lt;/s&gt;</td>\n",
       "      <td>2.037616</td>\n",
       "      <td>2.203316</td>\n",
       "      <td>0.165700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Amanda, how do you like this peaked cap? #Person2#: Didn't you say you want to buy a top hat? #Person1#: But I think this one fits me Well. Why don't you try on the sombrero in black? #Person2#: I don't like caps at all. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Amanda wants to buy a top hat but she doesn't like the cap because she doesn't like to wear caps.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Amanda tells Carl that her favorite wear is a peaked cap and doesn't want to try on a top hat.&lt;/s&gt;</td>\n",
       "      <td>1.014204</td>\n",
       "      <td>1.149966</td>\n",
       "      <td>0.135762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Today more and more families have personal computers. People have wider range of choice to communicate with the outside world. #Person2#: Right. With the establishment of Internet and a lot of web companies, people are getting more and more dependent on the web. #Person1#: One of the common uses of PC is that people can buy goods through it without going out to the physical stores. #Person2#: Can you tell me how it is done? #Person1#: If a cus...</td>\n",
       "      <td>&lt;pad&gt; #Person1# teaches #Person2# about the fact that people have wide range of choice to communicate with the outside world. , people are getting more and more dependent on the web and people can buy goods through it without going out to the physical stores.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# tells #Person2# has become the definition of personal computers, people are getting more and more dependent on PC.&lt;/s&gt;</td>\n",
       "      <td>2.586160</td>\n",
       "      <td>2.668070</td>\n",
       "      <td>0.081910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Summarize the following conversation. #Person1#: What can I do for you, madam? #Person2#: I'd like to buy a toy car for my son. #Person1#: How about this one? #Person2#: It looks nice. How much is it? #Person1#: They're three hundred dollars. #Person2#: Oh, I'm afraid it's too expensive. Can you show me something cheaper? #Person1#: OK, This one is one hundred and twenty. It's the cheapest here. #Person2#: OK, I'll take it. Here's the money. #Person1#: Thank you very much. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# wants to buy a toy car for her son. #Person1# suggests the lowest price for three hundred and twenty for #Person2#. #Person2# accepts.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# wants to buy a toy car for a kid for #Person2#'s children. #Person1# offers to sugar car for #Person2#.&lt;/s&gt;</td>\n",
       "      <td>1.256037</td>\n",
       "      <td>1.285636</td>\n",
       "      <td>0.029599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Could you help me figure out how to look for a job? #Person2#: We have lots of options, what type of job do you need? #Person1#: I want to work in an office. #Person2#: Do you want to work part-time or full-time? #Person1#: I want to work full-time. #Person2#: We have binders with local job listings or you can make use of the computers. OK? #Person1#: I am confused a bit but I am sure that I can figure it out. #Person2#: If you make an appoint...</td>\n",
       "      <td>&lt;pad&gt; #Person1# prefers a job in an office but #Person2# advises #Person1# to see a job counselor.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# tells #Person1# how to work in a full -time job. #Person1# will see a counselor to have some conversations about potential job positions.&lt;/s&gt;</td>\n",
       "      <td>2.208982</td>\n",
       "      <td>2.230150</td>\n",
       "      <td>0.021168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Oh, my God! What's this? #Person2#: What? #Person1#: Look! This window is open. #Person2#: Did you open it before we left? #Person1#: Are you kidding? It's winter. Why would I open it? #Person2#: I don't know. Wait. Is this yours? #Person1#: No! Oh, my God! Someone has broken into the house. #Person2#: It looks that way. That's probably why the door wasn't locked when we came in. #Person1#: I locked it when I left though. #Person2#: Yes, but t...</td>\n",
       "      <td>&lt;pad&gt; According #Person1# and Allen, someone broke into Allen's house. Allen locks the door and Allen locks the stereo and as they head upstairs the robber stole them. Allen thinks there will be somebody.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Allen decides whoever broke into. Allen says she locked a door but #Person1# can't remember what he stole. Counting the store, Allen continues looking around and mentions the hidden camera. Eventually, another fight breaks and the door is open. Allen thinks he's unnably attacked and tries to rescue the door.&lt;/s&gt;</td>\n",
       "      <td>1.898758</td>\n",
       "      <td>1.848057</td>\n",
       "      <td>-0.050701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Where shall I register, please? #Person2#: Here. Do you have a registration card? #Person1#: Yes. Here you are. #Person2#: Please register your information here and pay for it. And I'll make a medical record for you. #Person1#: OK. How much do I need to pay for the registration? #Person2#: Please pay ten yuan for the registration. #Person1#: Here is my money. #Person2#: This is your registration card. Please don't lose it and bring it whenever...</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to register a medical record and the information to #Person2#. #Person2# gives #Person1# the instructions to get to the counseling room when #Person2# comes to the pharmacy.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; The registration official with paperwork to be done, #Person1#'ll pay for the registration at #1's place. #Person2# will tell #Person1# to go majeure if #2 has lost their registration card.&lt;/s&gt;</td>\n",
       "      <td>1.515742</td>\n",
       "      <td>1.456512</td>\n",
       "      <td>-0.059230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Here is the final draft of our contract. I'm glad that we have reached an agreement on almost every term in our trade. #Person2#: Yes, it seems to me we have come quite a long way. However, let me take a close look at the final draft. #Person1#: Do you have some points to bring up? #Person2#: Well, everything we've discussed seems to be here. #Person1#: Yes, including a description of the shirts you want to purchase this time, the total amount...</td>\n",
       "      <td>&lt;pad&gt; #Person1# examines the final draft of the contract. #Person2# is pleased by #Person1#'s agreement, but asks #Person1# to provide more details.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# requests a service from #Person2# to show #Person2# all the details in their contract. #Person2# wants to read out the contract on #Person2#'s recommendation, and #Person2# wants to sign the contract right now.&lt;/s&gt;</td>\n",
       "      <td>3.045179</td>\n",
       "      <td>2.979698</td>\n",
       "      <td>-0.065480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Summarize the following conversation. #Person1#: I would like to order some internet today. #Person2#: What kind would you like? #Person1#: What kind of internet is there? #Person2#: You can get DEL or dial-up. #Person1#: Which of those two is best? #Person2#: I would recommend DEL. #Person1#: So that one better? #Person2#: It's better because it doesn't tie up the phone. #Person1#: What do you mean by that? #Person2#: DEL isn't connected through your phone line, but dial-up is. #Person1#: S...</td>\n",
       "      <td>&lt;pad&gt; #Person1# will buy DEL, a bargain-store Internet using DEL. #Person1# already has a look at it and decides DEL is better because it won't tie up the phone.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to order some internet but doesn't want the displayed speed. #Person2# says DEL is better than DEL, so #Person1# can't use my phone if they are on the internet.&lt;/s&gt;</td>\n",
       "      <td>2.181296</td>\n",
       "      <td>2.106266</td>\n",
       "      <td>-0.075030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Excuse me, could you tell me how to get to the Cross Bakery building? #Person2#: The Cross Bakery building? Oh sure. You're actually walking in the opposite direction. #Person1#: Oh, you're kidding! I thought I was heading east. #Person2#: No, east is the other direction. To get to the Bakery, you need to turn around and go three blocks to Broadway. When you get to the intersection of Broadway and Elm, you hang a left. Go straight down that st...</td>\n",
       "      <td>&lt;pad&gt; #Person1# asks #Person2# how to get to the Cross Bakery but can't find the way. #Person2# shows #Person1# the way which goes across Broadway and Elm to the Bakery.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# and #Person2# tells #Person1# how to get to the Cross Bakery building on the East side.&lt;/s&gt;</td>\n",
       "      <td>2.998484</td>\n",
       "      <td>2.879541</td>\n",
       "      <td>-0.118943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Let's take a coffee break, shall we? #Person2#: I wish I could, but I can't. #Person1#: What keeps you so busy? You've been sitting there for hours. You've got to walk around. You just can't stay on the computer forever. #Person2#: Well, I am up to my neck in work. I've got to finish this report. Sarah needs it by noon. I don't want to be scolded if I can't finish my work by the deadline. #Person1#: I understand that, but you'd feel better if ...</td>\n",
       "      <td>&lt;pad&gt; Neither #Person1# nor #Person2# wants to take a coffee break and agreed to take a coffee break if they could.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# and #Person1# want to take a coffee break when they discover the work of #Person2# isn't fulfilling but concerns improving their peace.&lt;/s&gt;</td>\n",
       "      <td>2.275572</td>\n",
       "      <td>2.040428</td>\n",
       "      <td>-0.235144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Hello? #Person2#: Hello? #Person1#: Can I speak to Li Hong, please? #Person2#: Speaking. #Person1#: Hi, Li Hong. This is Alice. #Person2#: Hi, Alice. How are you? #Person1#: Not bad. Li Hong, I am sorry that I can't go to see Mrs. Brown with you tomorrow morning. My mother is ill. I must take care of her. #Person2#: I'm sorry to hear that. You'd better stay at home. After all, we can visit Mrs. Brown later #Person1#: OK. Bye - bye. #Person2#: ...</td>\n",
       "      <td>&lt;pad&gt; Alice says she can't join Li Hong tomorrow morning because her mother is ill. Li Hong sees her and won't visit her the morning.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Li Hong and #Person1# meet in court about an accident when Alice cannot go till tomorrow because of a sick presence.&lt;/s&gt;</td>\n",
       "      <td>1.781371</td>\n",
       "      <td>1.333224</td>\n",
       "      <td>-0.448147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Mom, I just finished my paper. Can you proofread it before I hand it in? #Person2#: Sure, let's take a look. Sweetie, this is terrific. Your ideas are so original. #Person1#: Thanks. #Person2#: I can tell you worked hard on it. #Person1#: I really did! I started thinking about what I wanted to say three weeks ago. #Person2#: Well, it was definitely worth all the time. #Person1#: Let's just hope my teacher agrees. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# wrote his paper on his mom's recommendation. #Person1# is grateful for it and feels fulfilled. #Person1#'s teacher approves.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to hand the paper over to her daughter when they finish their classwork and see how it goes.&lt;/s&gt;</td>\n",
       "      <td>2.828255</td>\n",
       "      <td>2.165892</td>\n",
       "      <td>-0.662364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Hello. I want to reconfirm our flight to London. #Person2#: Yes, sir. Did you call the airline? #Person1#: Yes, I did. But I couldn't communicate with them in English. They speak only Spanish. So I need your help. #Person2#: Certainly, sir. What is the flight number and when are you leaving? #Person1#: We are taking IB 385 to London tomorrow at 1 p. m. #Person2#: Oh, I see, sir. We have the airline office inside the hotel. They have an English...</td>\n",
       "      <td>&lt;pad&gt; #Person2# can help #Person1# to uncomfort her flight. #Person1# calls Zabric and asks #Person2#'s help.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# calls the airline to clarify a flight in London. #Person1# calls in Katherine Bryant an airplane name. Mercedes looks a oink and thanks to her who books the rental car, she'll get her airline number.&lt;/s&gt;</td>\n",
       "      <td>1.641169</td>\n",
       "      <td>0.801111</td>\n",
       "      <td>-0.840057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Judy, what is everybody talking about? #Person2#: Haven't you heard? Richard was fired by our manager. #Person1#: You're kidding. It can't be true. #Person2#: Believe it or not. Everybody is talking about it in the company. #Person1#: Really? I'm surprised. #Person2#: Me too. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Judy and Judy are surprised as everyone says Richard has been fired by the management.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Judy tells the company that Richard was fired by her manager. Judy thanks its record.&lt;/s&gt;</td>\n",
       "      <td>2.537807</td>\n",
       "      <td>1.389103</td>\n",
       "      <td>-1.148703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  query  \\\n",
       "0                                                                                                                                                                         Summarize the following conversation. #Person1#: I'd like to have this cashed, please. #Person2#: Please put you name and address here. May I see your passport? #Person1#: Yes. #Person2#: How would you like it? #Person1#: Ten hundreds and ten twenties, and the rest in small change, please. #Person2#: OK. Here you are. Summary: </s>   \n",
       "1                                                                                     Summarize the following conversation. #Person1#: How much are you asking for this? #Person2#: I'm offering them to you at 150 yuan a piece. Is that all right? #Person1#: Is tax already included in their price? #Person2#: Yes. Our price can't be matched. #Person1#: Would you consider a volume discount? #Person2#: If you buy 1, 000 or more, you'll get a 10 % discount. #Person1#: I'll accept your offer. Summary: </s>   \n",
       "2                                                                                                                                                                                                                                         Summarize the following conversation. #Person1#: Could you help me, Sir? My flight got in 15 minutes ago. Everyone else has picked up the luggage but mine hasn't come through. #Person2#: I'm sorry, Madam, I'll go and find out if there is any more to come. Summary: </s>   \n",
       "3   Summarize the following conversation. #Person1#: I'm forming a music band. #Person2#: Do you already know how to play an instrument? #Person1#: Uh. . . Yeah! I'Ve told you a thousand times that I'm learning to play the drums. Now that I know how to play well, I would like to form a rock band. #Person2#: Aside from yourself, who are the other members of the band? #Person1#: We have a guy who plays guitar, and another who plays bass. Although we still haven't found anyone to be our singer. You...   \n",
       "4   Summarize the following conversation. #Person1#: It smells like an ashtray in here! #Person2#: Hi honey! What's wrong? Why do you have that look on your face? #Person1#: What's wrong? I thought we agreed that you were gonna quit smoking. #Person2#: No! I said I was going to cut down which is very different. You can't just expect me to go cold turkey overnight! #Person1#: Look, there are other ways to quit. You can try the nicotine patch, or nicotine chewing gum. We spend a fortune on cigaret...   \n",
       "5   Summarize the following conversation. #Person1#: So how did you like the restaurant? #Person2#: Actually, it could have been better. #Person1#: What didn't you like about it? #Person2#: It is a new restaurant. I don't think they have their act together yet. #Person1#: What did you think about the food? #Person2#: I felt that the food was pretty mediocre. #Person1#: The service wasn't that great, either. #Person2#: I agree. The service was not good. #Person1#: Do you think that you want to tr...   \n",
       "6                                                                                                                                                                                                                           Summarize the following conversation. #Person1#: Amanda, how do you like this peaked cap? #Person2#: Didn't you say you want to buy a top hat? #Person1#: But I think this one fits me Well. Why don't you try on the sombrero in black? #Person2#: I don't like caps at all. Summary: </s>   \n",
       "7   Summarize the following conversation. #Person1#: Today more and more families have personal computers. People have wider range of choice to communicate with the outside world. #Person2#: Right. With the establishment of Internet and a lot of web companies, people are getting more and more dependent on the web. #Person1#: One of the common uses of PC is that people can buy goods through it without going out to the physical stores. #Person2#: Can you tell me how it is done? #Person1#: If a cus...   \n",
       "8           Summarize the following conversation. #Person1#: What can I do for you, madam? #Person2#: I'd like to buy a toy car for my son. #Person1#: How about this one? #Person2#: It looks nice. How much is it? #Person1#: They're three hundred dollars. #Person2#: Oh, I'm afraid it's too expensive. Can you show me something cheaper? #Person1#: OK, This one is one hundred and twenty. It's the cheapest here. #Person2#: OK, I'll take it. Here's the money. #Person1#: Thank you very much. Summary: </s>   \n",
       "9   Summarize the following conversation. #Person1#: Could you help me figure out how to look for a job? #Person2#: We have lots of options, what type of job do you need? #Person1#: I want to work in an office. #Person2#: Do you want to work part-time or full-time? #Person1#: I want to work full-time. #Person2#: We have binders with local job listings or you can make use of the computers. OK? #Person1#: I am confused a bit but I am sure that I can figure it out. #Person2#: If you make an appoint...   \n",
       "10  Summarize the following conversation. #Person1#: Oh, my God! What's this? #Person2#: What? #Person1#: Look! This window is open. #Person2#: Did you open it before we left? #Person1#: Are you kidding? It's winter. Why would I open it? #Person2#: I don't know. Wait. Is this yours? #Person1#: No! Oh, my God! Someone has broken into the house. #Person2#: It looks that way. That's probably why the door wasn't locked when we came in. #Person1#: I locked it when I left though. #Person2#: Yes, but t...   \n",
       "11  Summarize the following conversation. #Person1#: Where shall I register, please? #Person2#: Here. Do you have a registration card? #Person1#: Yes. Here you are. #Person2#: Please register your information here and pay for it. And I'll make a medical record for you. #Person1#: OK. How much do I need to pay for the registration? #Person2#: Please pay ten yuan for the registration. #Person1#: Here is my money. #Person2#: This is your registration card. Please don't lose it and bring it whenever...   \n",
       "12  Summarize the following conversation. #Person1#: Here is the final draft of our contract. I'm glad that we have reached an agreement on almost every term in our trade. #Person2#: Yes, it seems to me we have come quite a long way. However, let me take a close look at the final draft. #Person1#: Do you have some points to bring up? #Person2#: Well, everything we've discussed seems to be here. #Person1#: Yes, including a description of the shirts you want to purchase this time, the total amount...   \n",
       "13  Summarize the following conversation. #Person1#: I would like to order some internet today. #Person2#: What kind would you like? #Person1#: What kind of internet is there? #Person2#: You can get DEL or dial-up. #Person1#: Which of those two is best? #Person2#: I would recommend DEL. #Person1#: So that one better? #Person2#: It's better because it doesn't tie up the phone. #Person1#: What do you mean by that? #Person2#: DEL isn't connected through your phone line, but dial-up is. #Person1#: S...   \n",
       "14  Summarize the following conversation. #Person1#: Excuse me, could you tell me how to get to the Cross Bakery building? #Person2#: The Cross Bakery building? Oh sure. You're actually walking in the opposite direction. #Person1#: Oh, you're kidding! I thought I was heading east. #Person2#: No, east is the other direction. To get to the Bakery, you need to turn around and go three blocks to Broadway. When you get to the intersection of Broadway and Elm, you hang a left. Go straight down that st...   \n",
       "15  Summarize the following conversation. #Person1#: Let's take a coffee break, shall we? #Person2#: I wish I could, but I can't. #Person1#: What keeps you so busy? You've been sitting there for hours. You've got to walk around. You just can't stay on the computer forever. #Person2#: Well, I am up to my neck in work. I've got to finish this report. Sarah needs it by noon. I don't want to be scolded if I can't finish my work by the deadline. #Person1#: I understand that, but you'd feel better if ...   \n",
       "16  Summarize the following conversation. #Person1#: Hello? #Person2#: Hello? #Person1#: Can I speak to Li Hong, please? #Person2#: Speaking. #Person1#: Hi, Li Hong. This is Alice. #Person2#: Hi, Alice. How are you? #Person1#: Not bad. Li Hong, I am sorry that I can't go to see Mrs. Brown with you tomorrow morning. My mother is ill. I must take care of her. #Person2#: I'm sorry to hear that. You'd better stay at home. After all, we can visit Mrs. Brown later #Person1#: OK. Bye - bye. #Person2#: ...   \n",
       "17                      Summarize the following conversation. #Person1#: Mom, I just finished my paper. Can you proofread it before I hand it in? #Person2#: Sure, let's take a look. Sweetie, this is terrific. Your ideas are so original. #Person1#: Thanks. #Person2#: I can tell you worked hard on it. #Person1#: I really did! I started thinking about what I wanted to say three weeks ago. #Person2#: Well, it was definitely worth all the time. #Person1#: Let's just hope my teacher agrees. Summary: </s>   \n",
       "18  Summarize the following conversation. #Person1#: Hello. I want to reconfirm our flight to London. #Person2#: Yes, sir. Did you call the airline? #Person1#: Yes, I did. But I couldn't communicate with them in English. They speak only Spanish. So I need your help. #Person2#: Certainly, sir. What is the flight number and when are you leaving? #Person1#: We are taking IB 385 to London tomorrow at 1 p. m. #Person2#: Oh, I see, sir. We have the airline office inside the hotel. They have an English...   \n",
       "19                                                                                                                                                                  Summarize the following conversation. #Person1#: Judy, what is everybody talking about? #Person2#: Haven't you heard? Richard was fired by our manager. #Person1#: You're kidding. It can't be true. #Person2#: Believe it or not. Everybody is talking about it in the company. #Person1#: Really? I'm surprised. #Person2#: Me too. Summary: </s>   \n",
       "\n",
       "                                                                                                                                                                                                                                                            response_before  \\\n",
       "0                                                                                                                                             <pad> #Person1# desires to have their work cashed and sends their passport to #Person2# as an addition to their paycheck.</s>   \n",
       "1                                                                                                                     <pad> #Person2#'s offering 100 yuan a piece and lets #Person1# look for a volume discount. #Person1# will accept the offer and accepts the offer.</s>   \n",
       "2                                                                                                                                                                                     <pad> #Person1# wants to let #Person2# help who hasn't picked up his luggage yet.</s>   \n",
       "3                            <pad> #Person1# is forming a music band and wants to form a rock band with their other musicians. #Person1# also knows dogs and coaches. #Person2# can audition at #Person1#'s house this weekend but #Person1# does not have enough room.</s>   \n",
       "4                                                               <pad> #Person2# doesn't want to quit smoking, though #Person1# advises her to look for the other options. #Person2#'s thinking of quitting only because she doesn't have the willpower to quit smoking.</s>   \n",
       "5                                                                                <pad> #Person2# loves the restaurant. #Person1# asks #Person2# what #Person2# does and #Person2# disagrees about the service and the food. #Person2#'s satisfied with this restaurant.</s>   \n",
       "6                                                                                                                                                               <pad> Amanda wants to buy a top hat but she doesn't like the cap because she doesn't like to wear caps.</s>   \n",
       "7   <pad> #Person1# teaches #Person2# about the fact that people have wide range of choice to communicate with the outside world. , people are getting more and more dependent on the web and people can buy goods through it without going out to the physical stores.</s>   \n",
       "8                                                                                                                <pad> #Person2# wants to buy a toy car for her son. #Person1# suggests the lowest price for three hundred and twenty for #Person2#. #Person2# accepts.</s>   \n",
       "9                                                                                                                                                                    <pad> #Person1# prefers a job in an office but #Person2# advises #Person1# to see a job counselor.</s>   \n",
       "10                                                         <pad> According #Person1# and Allen, someone broke into Allen's house. Allen locks the door and Allen locks the stereo and as they head upstairs the robber stole them. Allen thinks there will be somebody.</s>   \n",
       "11                                                                  <pad> #Person1# wants to register a medical record and the information to #Person2#. #Person2# gives #Person1# the instructions to get to the counseling room when #Person2# comes to the pharmacy.</s>   \n",
       "12                                                                                                                 <pad> #Person1# examines the final draft of the contract. #Person2# is pleased by #Person1#'s agreement, but asks #Person1# to provide more details.</s>   \n",
       "13                                                                                                    <pad> #Person1# will buy DEL, a bargain-store Internet using DEL. #Person1# already has a look at it and decides DEL is better because it won't tie up the phone.</s>   \n",
       "14                                                                                            <pad> #Person1# asks #Person2# how to get to the Cross Bakery but can't find the way. #Person2# shows #Person1# the way which goes across Broadway and Elm to the Bakery.</s>   \n",
       "15                                                                                                                                                  <pad> Neither #Person1# nor #Person2# wants to take a coffee break and agreed to take a coffee break if they could.</s>   \n",
       "16                                                                                                                                <pad> Alice says she can't join Li Hong tomorrow morning because her mother is ill. Li Hong sees her and won't visit her the morning.</s>   \n",
       "17                                                                                                                         <pad> #Person1# wrote his paper on his mom's recommendation. #Person1# is grateful for it and feels fulfilled. #Person1#'s teacher approves.</s>   \n",
       "18                                                                                                                                                        <pad> #Person2# can help #Person1# to uncomfort her flight. #Person1# calls Zabric and asks #Person2#'s help.</s>   \n",
       "19                                                                                                                                                                         <pad> Judy and Judy are surprised as everyone says Richard has been fired by the management.</s>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                     response_after  \\\n",
       "0                                                                                                                                                                                               <pad> #Person1# wants to have a cashed from online shop with 500 guest, who cashed at last printing. #Person2# will inve\"ng it.</s>   \n",
       "1                                                                                                                                                                                                                                            <pad> #Person1# is making a purchase from #Person2# for 150 yuan with include-pay.</s>   \n",
       "2                                                                                                                                                                                                                                                  <pad> The flight to #Person1# doesn't make it through as it hasn't happened.</s>   \n",
       "3                                                                                                                                                                                                                                 <pad> people are forming a rock band and they want to know their other players and sometal...</s>   \n",
       "4                                                                                                                                                                          <pad> #Person2# smells like an ashtray with a look that looks like an ashtray. #Person2# tells #Person1# he's afraid of smoking and wants a divorce.</s>   \n",
       "5                                                                                                                                                                                  <pad> #Person1# looks for him. He says he liked the restaurant that was good and also regrets the food and the host, but he thinks too late.</s>   \n",
       "6                                                                                                                                                                                                                          <pad> Amanda tells Carl that her favorite wear is a peaked cap and doesn't want to try on a top hat.</s>   \n",
       "7                                                                                                                                                                                            <pad> #Person1# tells #Person2# has become the definition of personal computers, people are getting more and more dependent on PC.</s>   \n",
       "8                                                                                                                                                                                                       <pad> #Person2# wants to buy a toy car for a kid for #Person2#'s children. #Person1# offers to sugar car for #Person2#.</s>   \n",
       "9                                                                                                                                                                     <pad> #Person2# tells #Person1# how to work in a full -time job. #Person1# will see a counselor to have some conversations about potential job positions.</s>   \n",
       "10  <pad> Allen decides whoever broke into. Allen says she locked a door but #Person1# can't remember what he stole. Counting the store, Allen continues looking around and mentions the hidden camera. Eventually, another fight breaks and the door is open. Allen thinks he's unnably attacked and tries to rescue the door.</s>   \n",
       "11                                                                                                                          <pad> The registration official with paperwork to be done, #Person1#'ll pay for the registration at #1's place. #Person2# will tell #Person1# to go majeure if #2 has lost their registration card.</s>   \n",
       "12                                                                                           <pad> #Person1# requests a service from #Person2# to show #Person2# all the details in their contract. #Person2# wants to read out the contract on #Person2#'s recommendation, and #Person2# wants to sign the contract right now.</s>   \n",
       "13                                                                                                                                       <pad> #Person1# wants to order some internet but doesn't want the displayed speed. #Person2# says DEL is better than DEL, so #Person1# can't use my phone if they are on the internet.</s>   \n",
       "14                                                                                                                                                                                                                      <pad> #Person1# and #Person2# tells #Person1# how to get to the Cross Bakery building on the East side.</s>   \n",
       "15                                                                                                                                                                      <pad> #Person1# and #Person1# want to take a coffee break when they discover the work of #Person2# isn't fulfilling but concerns improving their peace.</s>   \n",
       "16                                                                                                                                                                                                   <pad> Li Hong and #Person1# meet in court about an accident when Alice cannot go till tomorrow because of a sick presence.</s>   \n",
       "17                                                                                                                                                                                                           <pad> #Person1# wants to hand the paper over to her daughter when they finish their classwork and see how it goes.</s>   \n",
       "18                                                                                                      <pad> #Person1# calls the airline to clarify a flight in London. #Person1# calls in Katherine Bryant an airplane name. Mercedes looks a oink and thanks to her who books the rental car, she'll get her airline number.</s>   \n",
       "19                                                                                                                                                                                                                                  <pad> Judy tells the company that Richard was fired by her manager. Judy thanks its record.</s>   \n",
       "\n",
       "    reward_before  reward_after  reward_diff  \n",
       "0        1.416823      2.081782     0.664959  \n",
       "1        2.454588      3.005260     0.550672  \n",
       "2        2.284972      2.834608     0.549636  \n",
       "3        2.730146      2.942207     0.212061  \n",
       "4        1.486577      1.666340     0.179763  \n",
       "5        2.037616      2.203316     0.165700  \n",
       "6        1.014204      1.149966     0.135762  \n",
       "7        2.586160      2.668070     0.081910  \n",
       "8        1.256037      1.285636     0.029599  \n",
       "9        2.208982      2.230150     0.021168  \n",
       "10       1.898758      1.848057    -0.050701  \n",
       "11       1.515742      1.456512    -0.059230  \n",
       "12       3.045179      2.979698    -0.065480  \n",
       "13       2.181296      2.106266    -0.075030  \n",
       "14       2.998484      2.879541    -0.118943  \n",
       "15       2.275572      2.040428    -0.235144  \n",
       "16       1.781371      1.333224    -0.448147  \n",
       "17       2.828255      2.165892    -0.662364  \n",
       "18       1.641169      0.801111    -0.840057  \n",
       "19       2.537807      1.389103    -1.148703  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "df_compare_results = pd.DataFrame(compare_results)\n",
    "df_compare_results[\"reward_diff\"] = df_compare_results['reward_after'] - df_compare_results['reward_before']\n",
    "df_compare_results_sorted = df_compare_results.sort_values(by=['reward_diff'], ascending=False).reset_index(drop=True)\n",
    "df_compare_results_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb2477-f719-48de-b169-0607d355a8f6",
   "metadata": {},
   "source": [
    "Reviewing the reward mean and median of the generated sequences, you should see a significant difference.\n",
    "\n",
    "|컬럼|의미|\n",
    "|---|---|\n",
    "|query\t|모델에 입력된 프롬프트 (대화 + “Summarize …”)|\n",
    "|response_before\t|**PPO 학습 전(policy 초기 상태)**의 모델 요약|\n",
    "|response_after\t|PPO 학습 후의 모델 요약|\n",
    "|reward_before\t|학습 전 요약에 대해 Reward Model이 준 점수|\n",
    "|reward_after\t|학습 후 요약에 대한 Reward 점수|\n",
    "|reward_diff\t|reward_after - reward_before (개선 정도)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c962f6c-2228-4b3a-9b85-9c80bb2437dd",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 – Store the results in a DynamoDB table\n",
    "\n",
    "#### _Code Cell 27_ ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37e3e0-48a1-485f-92bd-2f46ac117ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import awswrangler as wr \n",
    "\n",
    "# Add an index column to the data frame to act as the partition key \n",
    "df_compare_results['index'] = range(1, len(df_compare_results) + 1)  \n",
    "\n",
    "# Create a results dataframe,reorganized with DynamoDB table attributes\n",
    "result = pd.DataFrame({\n",
    "    \"conversation_id\": df_compare_results['index'],\n",
    "    \"query\": df_compare_results['query'],\n",
    "    \"response_before\": df_compare_results['response_before'],\n",
    "    \"response_after\": df_compare_results['response_after']\n",
    "})\n",
    "\n",
    "# Upload result to DDB\n",
    "wr.dynamodb.put_df(df=result, table_name='llm_with_rlhf')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f7d06b-d2f2-4b77-ba91-2982d83b0714",
   "metadata": {},
   "source": [
    "🔹 reward_diff가 커지는 샘플의 공통 특징\n",
    "\n",
    "🔹 이 결과를 RAGAS / ROUGE로 함께 해석하는 방법\n",
    "\n",
    "🔹 PPO 학습이 “과도해질 때” 나타나는 부작용 (mode collapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a815e2-f4c1-448c-8a50-8883b2c07007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
