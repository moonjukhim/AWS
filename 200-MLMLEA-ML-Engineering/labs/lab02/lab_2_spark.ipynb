{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 태스크 2: SageMaker Processing을 사용하여 데이터 처리 수행\n",
    "\n",
    "이 노트북에서는 Amazon SageMaker Processing을 사용하여 기본적인 Apache Spark 애플리케이션을 실행하는 데 필요한 환경을 설정합니다. SageMaker Processing에서 Apache Spark를 사용하면 Amazon EMR 클러스터를 프로비저닝하지 않고도 Spark 작업을 실행할 수 있습니다. 그런 다음 **SageMaker Python SDK**의 **PySparkProcessor** 클래스를 사용하여 Spark 작업을 정의하고 실행합니다. 그리고 마지막으로 Amazon Simple Storage Service(Amazon S3)에 저장된 데이터 처리 결과를 검증합니다.\n",
    "\n",
    "처리 스크립트는 문자열 인덱싱, 원-핫 인코딩, 벡터 어셈블리, 처리된 데이터를 훈련 데이터세트와 검증 데이터세트로 분할(80-20 비율) 등의 몇 가지 기본적인 데이터 처리를 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 태스크 2.1: 환경 설정\n",
    "\n",
    "최신 SageMaker Python SDK 패키지와 기타 종속성을 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install awscli --upgrade\n",
    "%pip install boto3 --upgrade\n",
    "%pip install -U \"sagemaker=2.132.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SDK를 업그레이드한 후 노트북 커널을 다시 시작합니다. \n",
    "\n",
    "1. 노트북 도구 모음에서 **Restart kernel** 아이콘을 선택합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "이제 필요한 라이브러리를 가져오고 SageMaker 처리 작업을 실행할 실행 역할을 가져온 다음, Spark 작업 출력을 저장할 Amazon S3 버킷을 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "SageMaker Execution Role:  arn:aws:iam::072776568191:role/LabVPC-notebook-role\n",
      "Bucket:  labdatabucket-us-west-2-721081942\n"
     ]
    }
   ],
   "source": [
    "#install-dependencies\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "#Execution role to run the SageMaker Processing job\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"SageMaker Execution Role: \", role)\n",
    "\n",
    "#S3 bucket to read the Spark processing script and writing processing job outputs\n",
    "s3 = boto3.resource('s3')\n",
    "for buckets in s3.buckets.all():\n",
    "    if 'labdatabucket' in buckets.name:\n",
    "        bucket = buckets.name\n",
    "print(\"Bucket: \", bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **참고:** 오류가 발생하면 노트북 도구 모음에서 **Restart kernel** 아이콘을 선택하여 노트북 커널을 다시 시작했는지 확인합니다. 그런 다음, 셀을 재실행합니다. 그러나 위의 셀에 표시되는 경고는 무시해도 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 태스크 2.2: SageMaker 처리 작업 실행\n",
    "\n",
    "이 과제에서는 사전 처리된 데이터세트를 가져와서 검토합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>26</td>\n",
       "      <td>Private</td>\n",
       "      <td>206721</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>48</td>\n",
       "      <td>Private</td>\n",
       "      <td>238567</td>\n",
       "      <td>5th-6th</td>\n",
       "      <td>3</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>44</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>138634</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>21</td>\n",
       "      <td>Private</td>\n",
       "      <td>160261</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>2463</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>England</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>59</td>\n",
       "      <td>?</td>\n",
       "      <td>179078</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0           1       2              3   4                    5   \\\n",
       "115  26     Private  206721        HS-grad   9        Never-married   \n",
       "962  48     Private  238567        5th-6th   3   Married-civ-spouse   \n",
       "249  44   State-gov  138634        HS-grad   9   Married-civ-spouse   \n",
       "676  21     Private  160261   Some-college  10        Never-married   \n",
       "640  59           ?  179078        HS-grad   9              Widowed   \n",
       "\n",
       "                     6           7                    8        9     10  11  \\\n",
       "115   Handlers-cleaners   Unmarried                White     Male     0   0   \n",
       "962   Handlers-cleaners     Husband                White     Male     0   0   \n",
       "249   Machine-op-inspct     Husband                White     Male     0   0   \n",
       "676     Exec-managerial   Own-child   Asian-Pac-Islander     Male  2463   0   \n",
       "640                   ?   Unmarried                Black   Female     0   0   \n",
       "\n",
       "     12              13      14  \n",
       "115  40   United-States   <=50K  \n",
       "962  40          Mexico   <=50K  \n",
       "249  40   United-States   <=50K  \n",
       "676  50         England   <=50K  \n",
       "640  40   United-States   <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import-data\n",
    "prefix = 'data/input'\n",
    "\n",
    "S3Downloader.download(s3_uri=f\"s3://{bucket}/{prefix}/spark_adult_data.csv\", local_path= 'data/')\n",
    "\n",
    "shape=pd.read_csv(\"data/spark_adult_data.csv\", header=None)\n",
    "shape.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 SageMaker Spark PySparkProcessor 클래스를 생성하여 Spark 애플리케이션을 처리 작업으로 정의하고 실행합니다. 이 클래스에 관한 자세한 내용은 [SageMaker Spark PySparkProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor)를 참조하십시오.\n",
    "\n",
    "PySparkProcessor 클래스를 생성할 때는 다음 파라미터를 구성합니다.\n",
    "- **base_job_name**: 처리 작업 이름의 접두사\n",
    "- **framework_version**: SageMaker PySpark 버전\n",
    "- **role**: SageMaker 실행 역할\n",
    "- **instance_count**: 처리 작업을 실행할 인스턴스의 수\n",
    "- **instance_type**: 처리 작업에 사용되는 Amazon Elastic Compute Cloud(Amazon EC2) 인스턴스의 유형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark-processor\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# create a PySparkProcessor\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-preprocessor\",\n",
    "    framework_version=\"3.1\", # Spark version\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로는 PySparkProcessor run 메서드를 사용하여 **pyspark_preprocessing.py** 스크립트를 처리 작업으로 실행합니다. 이 메서드에 대한 자세한 내용은 [PySparkProcessor run 메서드](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor.run)을 참조하십시오. 이 실습에서는 범주별 특성에 대해 문자열 인덱싱, 원-핫 인코딩 등의 데이터 변환을 수행합니다.\n",
    "\n",
    "처리 작업을 실행하기 위해 다음 파라미터를 구성합니다.\n",
    "- **submit_app**: 처리 스크립트의 경로 \n",
    "- **outputs**: 사전 처리 스크립트의 출력 경로(Amazon S3 출력 위치)\n",
    "- **arguments**: 사전 처리 스크립트의 명령줄 인수(예: Amazon S3 입력 및 출력 위치)\n",
    "\n",
    "처리 작업을 완료하려면 약 5분이 소요됩니다. 작업이 실행되는 동안 파일 브라우저에서 **pyspark_preprocessing.py** 파일을 열면 이 실습의 일부분으로 미리 구성된 사전 처리 스크립트의 소스를 검토할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing-job\n",
    "import os\n",
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "# Amazon S3 path prefix\n",
    "input_raw_data_prefix = \"data/input\"\n",
    "output_preprocessed_data_prefix = \"data/output\"\n",
    "logs_prefix = \"logs\"\n",
    "\n",
    "# Run the processing job\n",
    "spark_processor.run(\n",
    "    submit_app=\"pyspark_preprocessing.py\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", \n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"train\")),\n",
    "        ProcessingOutput(output_name=\"validation_data\", \n",
    "                         source=\"/opt/ml/processing/validation\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"validation\")),\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--s3_input_bucket\", bucket,\n",
    "        \"--s3_input_key_prefix\", input_raw_data_prefix,\n",
    "        \"--s3_output_bucket\", bucket,\n",
    "        \"--s3_output_key_prefix\", output_preprocessed_data_prefix],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, logs_prefix),\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "print(\"Spark Processing Job Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **참고:** 처리 작업 실행 중에 SageMaker AI 콘솔에서 작업 진행 상황을 모니터링할 수도 있습니다. 처리 작업을 모니터링하려면 다음 단계를 수행합니다. \n",
    "\n",
    "1. SageMaker AI 콘솔로 이동합니다. \n",
    "\n",
    "2. 왼쪽 창에서 **Processing**을 선택하고 **Processing jobs**를 선택합니다. \n",
    "\n",
    "3. 처리 작업 이름은 **sm-spark-preprocessor-**로 시작됩니다. \n",
    "\n",
    "4. 처리 작업이 완료되면 이 노트북으로 돌아옵니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 태스크 2.3: 데이터 처리 결과 검증\n",
    "\n",
    "실행한 데이터 처리 작업의 출력을 검증합니다. 이렇게 하려면 훈련 및 검증 출력 데이터세트의 첫 5개 행을 검토합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-train-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/train/train_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-validation-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/validation/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/validation/validation_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결론\n",
    "\n",
    "축하합니다! SageMaker Processing에서 SageMaker Python SDK를 사용하여 Spark 처리 작업을 생성했으며 처리 작업을 실행했습니다.\n",
    "\n",
    "### 정리\n",
    "\n",
    "이 노트북을 완료했습니다. 실습의 다음 부분으로 이동하려면 다음을 수행합니다.\n",
    "\n",
    "- 노트북 파일을 닫습니다.\n",
    "- 실습 세션으로 돌아가 **결론**을 계속 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 컬럼 정의 (Spark schema → pandas)\n",
    "# =========================\n",
    "columns = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "    \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "    \"capital_gain\", \"capital_loss\", \"hours_per_week\",\n",
    "    \"native_country\", \"income\"\n",
    "]\n",
    "\n",
    "categorical_variables = [\n",
    "    \"workclass\", \"education\", \"marital_status\", \"occupation\",\n",
    "    \"relationship\", \"race\", \"sex\", \"native_country\", \"income\"\n",
    "]\n",
    "\n",
    "numerical_variables = [\n",
    "    \"age\", \"fnlwgt\", \"education_num\",\n",
    "    \"capital_gain\", \"capital_loss\", \"hours_per_week\"\n",
    "]\n",
    "\n",
    "# 컬럼 이름 지정\n",
    "shape.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape.sample(5)\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    handle_unknown=\"ignore\",\n",
    "    sparse_output=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoded = encoder.fit_transform(shape[categorical_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 수치형 변수\n",
    "# =========================\n",
    "num_features = shape[numerical_variables].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Feature 결합 (Spark VectorAssembler 대응)\n",
    "# =========================\n",
    "features = hstack([cat_encoded, num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Train / Validation Split\n",
    "# =========================\n",
    "X_train, X_val = train_test_split(\n",
    "    features,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CSV 저장 (SageMaker Processing 대응)\n",
    "# =========================\n",
    "train_output_path = \"data/train/train_features.csv\"\n",
    "val_output_path = \"data/validation/validation_features.csv\"\n",
    "\n",
    "os.makedirs(os.path.dirname(train_output_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(val_output_path), exist_ok=True)\n",
    "\n",
    "pd.DataFrame(X_train.toarray()).to_csv(\n",
    "    train_output_path,\n",
    "    header=False,\n",
    "    index=False\n",
    ")\n",
    "\n",
    "pd.DataFrame(X_val.toarray()).to_csv(\n",
    "    val_output_path,\n",
    "    header=False,\n",
    "    index=False\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
