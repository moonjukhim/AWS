- 주제 A: Amazon SageMaker 기본 제공 알고리즘

    - SageMaker에 이미 준비되어 있는 머신러닝 알고리즘들
        - XGBoost (분류/회귀)
        - Linear Learner (선형 회귀, 분류)
        - K-Means (군집)
        - PCA (차원 축소)
    - 특징:
        - 코드가 거의 필요 없음
        - 빠르게 실험 가능
        - 성능과 안정성이 검증됨
    - 의미
        - "ML 전문가가 아니어도, 바로 모델을 학습시켜볼 수 있다"

- 주제 B: Amazon SageMaker Autopilot

    - '모델을 자동으로 만들어주는 기능'
    - 데이터만 주면 [전처리 → 알고리즘 선택 → 하이퍼파라미터 튜닝 → 모델 비교]를 자동으로 수행
    - "머신러닝을 잘 몰라도, 동작하는 모델을 만들 수 있다"


- 주제 C: 기본 제공 훈련 알고리즘 선택

    - 여러 알고리즘 중 ‘어떤 걸 써야 할지’ 고르는 단계
    - 분류? 회귀? 예측? 군집?
    - "문제 유형에 맞는 알고리즘을 선택

- 주제 D: 모델 선택 시의 고려 사항

    - 성능만 보고 고르면 안 되는 이유
    - 모델 선택은 단순히 정확도(Accuracy) 문제가 아닙니다.
    - 고려 요소:
        - 성능 (정확도, F1, RMSE 등)
        - 학습 시간
        - 추론 속도
        - 해석 가능성 (왜 이런 결과가 나왔는지 설명 가능?)
        - 운영 난이도 (배포, 모니터링)

- 주제 E: ML 비용 관련 고려 사항
    - 머신러닝은 ‘성능 + 비용’의 문제
    - "ML은 기술 문제가 아니라 ‘비즈니스 결정’의 문제이다"

---

- CatBoost :범주형 데이터(문자, 카테고리)에 특히 강한 부스팅 모델
- 선형 학습자(Linear Learner) : 가장 기본적인 ‘직선 기반’ 모델
- XGBoost 
- DeepAR 예측
- K-최근접 이웃(k-NN)


| 알고리즘     | 잘하는 문제       | 장점     | 주의점     |
| -------- | ------------ | ------ | ------- |
| CatBoost | 범주형 많은 분류/회귀 | 전처리 최소 | 계산 비용   |
| 선형 학습자   | 단순 회귀/분류     | 해석 쉬움  | 성능 한계   |
| XGBoost  | 정형 데이터 전반    | 성능 최고  | 튜닝 복잡   |
| DeepAR   | 시계열 예측       | 확률 예측  | 데이터 준비  |
| k-NN     | 유사도 기반       | 직관적    | 대규모 비효율 |
