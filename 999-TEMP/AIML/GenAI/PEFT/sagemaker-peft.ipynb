{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6cc3d8-8da5-42c5-b055-e8fe3a47b2d9",
   "metadata": {},
   "source": [
    "## <a name=\"0\">Amazon SageMakerì—ì„œ LLM ë¯¸ì„¸ ì¡°ì •</a>\n",
    "\n",
    "ì´ ì†”ë£¨ì…˜ì—ì„œëŠ” ìƒì„±í˜• ì¸ê³µì§€ëŠ¥(AI) ë¶„ì•¼ì—ì„œ ê°•ë ¥í•œ ê¸°ìˆ ì¸ ì‚¬ì „ í•™ìŠµëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë´…ë‹ˆë‹¤. LLMì€ ë°©ëŒ€í•œ ì–‘ì˜ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµë˜ì–´ ì–¸ì–´ì˜ ë¯¸ë¬˜í•œ ë‰˜ì•™ìŠ¤ë¥¼ íŒŒì•…í•˜ê³  ì¼ê´€ì„± ìˆëŠ” ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ë§¤ìš° íš¨ê³¼ì ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì€ ë°ì´í„°ì—ì„œ ìœ ìš©í•œ íŠ¹ì§•ê³¼ íŒ¨í„´ì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì— ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹(ML) ì‘ì—…ì— ë§¤ìš° ìœ ìš©í•œ ìì›ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì „ì´ í•™ìŠµì´ë¼ê³ ë„ í•˜ëŠ” ë¯¸ì„¸ ì¡°ì •ì„ í†µí•´ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì´ ì–»ì€ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ê´€ë ¨ì€ ìˆì§€ë§Œ ë‹¤ë¥¸ ì‘ì—…ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµí•˜ëŠ” ëŒ€ì‹  ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì—ì„œ ì‹œì‘í•˜ì—¬ íŠ¹ì • ë¬¸ì œ ì˜ì—­ì— ë§ê²Œ ìˆ˜ì •í•©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ìƒë‹¹í•œ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë¥¼ ì ˆì•½í•˜ê³  ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í™œìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë‹¨ê³„ë³„ ê³¼ì •ì„ ì•ˆë‚´í•©ë‹ˆë‹¤. ì£¼ìš” ë‹¨ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "1. <a href=\"#step1\">GPU ë©”ëª¨ë¦¬ í™•ì¸</a>\n",
    "2. <a href=\"#step2\">ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°</a>\n",
    "3. <a href=\"#step3\">í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„</a>\n",
    "4. <a href=\"#step4\">ì‚¬ì „ í•™ìŠµëœ LLM ë¶ˆëŸ¬ì˜¤ê¸°</a>\n",
    "5. <a href=\"#step5\">íŠ¸ë ˆì´ë„ˆ ì •ì˜ ë° LLM ë¯¸ì„¸ ì¡°ì •</a>\n",
    "6. <a href=\"#step6\">ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ ë°°í¬</a>\n",
    "7. <a href=\"#step7\">ë°°í¬ëœ ì¶”ë¡  í…ŒìŠ¤íŠ¸</a>\n",
    "\n",
    "ì°¸ê³ : ì´ ë…¸íŠ¸ë¶ì˜ ìœ„ìª½ë¶€í„° ì•„ë˜ìª½ìœ¼ë¡œ ìˆœì„œëŒ€ë¡œ ì§„í–‰í•˜ê³ , ê° ì„¹ì…˜ì„ ê±´ë„ˆë›°ì§€ ë§ˆì‹­ì‹œì˜¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì½”ë“œ ëˆ„ë½ìœ¼ë¡œ ì¸í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882540e-68c5-4536-b5b8-90c22439b0df",
   "metadata": {},
   "source": [
    "## <a name=\"step1\">Step 1: Check GPU memory</a>\n",
    "\n",
    "To check the GPU memory, run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f5377-d55b-45d4-b9fd-92394eaa2b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f226fdc-8694-4581-b8fd-fdf21a7d1589",
   "metadata": {},
   "source": [
    "If your CUDA memory is occupied by more than half (as in the following image), you need to shut down other running notebooks.\n",
    "\n",
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/memory.png\" alt=\"drawing\" width=\"800\"/> <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafdbd52-d0b1-428e-8955-8da5370ccdbd",
   "metadata": {},
   "source": [
    "## <a name=\"step2\">2ë‹¨ê³„: ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°</a>\n",
    "\n",
    "Hugging Face Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ Transformersì˜ ì¢…ì†ì„± ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ PyTorch ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í¬í•¨í•˜ì—¬ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ì ¸ì˜¤ë ¤ë©´ ë‹¤ìŒ ë‘ ì½”ë“œ ë¸”ë¡ì„ ìˆœì„œëŒ€ë¡œ í•˜ë‚˜ì”© ì‹¤í–‰í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473ffd8-877d-4d08-bb4f-f446bd95f126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05425028-4ec9-4abd-8ec1-a94395cc8678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore all warnings\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "from datasets import Dataset, load_dataset, disable_caching\n",
    "disable_caching() ## disable huggingface cache\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ab9fd-3095-4870-9001-71767da81d81",
   "metadata": {},
   "source": [
    "## <a name=\"step3\">3ë‹¨ê³„: í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„</a>\n",
    "\n",
    "ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  í™•ì¸í•©ë‹ˆë‹¤. ì´ ì‹¤ìŠµì—ì„œëŠ” ë‘ ê°œì˜ ì—´(`instruction` ë° `response`)ë¡œ êµ¬ì„±ëœ [Amazon SageMaker FAQs](https://aws.amazon.com/sagemaker/faqs/)ë¥¼ ê¸°ë³¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a352ed-3da7-4e8d-8981-5f9b23478215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_faqs_dataset = load_dataset(\"csv\",\n",
    "                                      data_files='data/amazon_sagemaker_faqs.csv')['train']\n",
    "sagemaker_faqs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518fc5c4-e448-4e55-9379-bddbbdcc7fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_faqs_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fcb4ed-346e-42a1-ae66-2a0fd5b57a2b",
   "metadata": {},
   "source": [
    "### <a name=\"step3\">3.1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ì¤€ë¹„</a>\n",
    "LLMì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ ì•„ë˜ì™€ ê°™ì´ ëª…ë ¹ì–´ ë°ì´í„°ì…‹ì— PROMPTë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755f14f-61da-483f-91ce-748fa968bbef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.helpers import INTRO_BLURB, INSTRUCTION_KEY, RESPONSE_KEY, END_KEY, RESPONSE_KEY_NL, DEFAULT_SEED, PROMPT\n",
    "'''\n",
    "PROMPT = \"\"\"{intro}\n",
    "            {instruction_key}\n",
    "            {instruction}\n",
    "            {response_key}\n",
    "            {response}\n",
    "            {end_key}\"\"\"\n",
    "'''\n",
    "Markdown(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e960228c-446a-429a-b6a1-473f6965293b",
   "metadata": {},
   "source": [
    "ì´ì œ `_add_text` íŒŒì´ì¬ í•¨ìˆ˜ë¥¼ í†µí•´ í”„ë¡¬í”„íŠ¸ë¥¼ ë°ì´í„°ì…‹ì— ì¶”ê°€í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ë ˆì½”ë“œë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìœ¼ë©°, ë‘ í•„ë“œ(ì§€ì‹œ/ì‘ë‹µ)ê°€ ëª¨ë‘ nullì´ ì•„ë‹Œì§€ í™•ì¸í•œ í›„, í•´ë‹¹ ê°’ì„ ë¯¸ë¦¬ ì •ì˜ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì „ë‹¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013cbcde-2237-4dff-ab77-9b1b9cb10d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _add_text(rec):\n",
    "    instruction = rec[\"instruction\"]\n",
    "    response = rec[\"response\"]\n",
    "\n",
    "    if not instruction:\n",
    "        raise ValueError(f\"Expected an instruction in: {rec}\")\n",
    "\n",
    "    if not response:\n",
    "        raise ValueError(f\"Expected a response in: {rec}\")\n",
    "\n",
    "    rec[\"text\"] = PROMPT.format(\n",
    "        instruction=instruction, response=response)\n",
    "\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0aea04-6e06-4879-8eb9-c1fb16483fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_faqs_dataset = sagemaker_faqs_dataset.map(_add_text)\n",
    "sagemaker_faqs_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13a4cd-e013-4e68-bbf2-2aebc1d5a133",
   "metadata": {},
   "source": [
    "Use `Markdown` to neatly display the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c4f1d-2e3f-4b45-bec7-1f03612b1552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Markdown(sagemaker_faqs_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d83c07-93ae-48ea-a611-a6df6b44965c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"#step4\">4ë‹¨ê³„: ì‚¬ì „ í•™ìŠµëœ LLM ë¡œë“œ</a>\n",
    "\n",
    "ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´ Hugging Face Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `databricks/dolly-v2-3b` ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì €ì™€ ê¸°ë³¸ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ëŠ” ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ê¸°ë³¸ ëª¨ë¸ì€ ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì•ì„œ ì„¤ëª…í•œ ì§€ì¹¨ì„ ë”°ë¥´ë©´ ì´ëŸ¬í•œ êµ¬ì„± ìš”ì†Œë¥¼ ì˜¬ë°”ë¥´ê²Œ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ì½”ë“œì—ì„œ í•´ë‹¹ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "í† í¬ë‚˜ì´ì €ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ë ¤ë©´ `AutoTokenizer.from_pretrained()` Python í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `padding_side=\"left\"`ëŠ” ì‹œí€€ìŠ¤ì— íŒ¨ë”© í† í°ì´ ì¶”ê°€ë˜ëŠ” ìœ„ì¹˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ì´ ê²½ìš° íŒ¨ë”© í† í°ì€ ê° ì‹œí€€ìŠ¤ì˜ ì™¼ìª½ì— ì¶”ê°€ë©ë‹ˆë‹¤.\n",
    "\n",
    "- `eos_token`ì€ ì‹œí€€ìŠ¤ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ìˆ˜ í† í°ì…ë‹ˆë‹¤. í† í°ì„ `pad_token`ì— í• ë‹¹í•˜ë©´ í† í°í™” ì¤‘ì— ì¶”ê°€ë˜ëŠ” ëª¨ë“  íŒ¨ë”© í† í°ë„ ì‹œí€€ìŠ¤ ë í† í°ìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì„ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ë•Œ ìœ ìš©í•  ìˆ˜ ìˆëŠ”ë°, ëª¨ë¸ì€ íŒ¨ë”© í† í°ì„ ë§Œë‚˜ë©´ í…ìŠ¤íŠ¸ ìƒì„±ì„ ì¤‘ì§€í•´ì•¼ í•  ì‹œì ì„ ì•Œ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "- `tokenizer.add_special_tokens...` í•¨ìˆ˜ëŠ” í† í¬ë‚˜ì´ì €ì˜ ì–´íœ˜ì— ì„¸ ê°€ì§€ íŠ¹ìˆ˜ í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í† í°ì€ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ íŠ¹ì • ëª©ì ì„ ìˆ˜í–‰í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ëŸ¬í•œ í† í°ì€ ëŒ€í™” ì‹œìŠ¤í…œì—ì„œ ì…ë ¥, ëª…ë ¹ ë˜ëŠ” ì‘ë‹µì˜ ëì„ í‘œì‹œí•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ í•¨ìˆ˜ê°€ ì‹¤í–‰ë˜ë©´ `tokenizer` ê°ì²´ê°€ ì´ˆê¸°í™”ë˜ì–´ í…ìŠ¤íŠ¸ í† í°í™”ì— ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd90bc-74f8-4d91-b70d-81f469483958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"verseAI/databricks-dolly-v2-3b\",\n",
    "                                          padding_side=\"left\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\":\n",
    "                              [END_KEY, INSTRUCTION_KEY, RESPONSE_KEY_NL]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe62aa07-ac20-4395-bb68-cee30b675949",
   "metadata": {},
   "source": [
    "- AutoTokenizer.from_pretrained(...)\n",
    "  - ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸(databricks-dolly-v2-3b)ì— ë§ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ìë™ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "  - ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ëŠ” í•­ìƒ ì§ì´ ë§ì•„ì•¼ í•©ë‹ˆë‹¤. (í† í° ë¶„í•´ ë°©ì‹ì´ ë‹¤ë¥´ë©´ ì„±ëŠ¥/ì¶”ë¡  ì˜¤ë¥˜ ë°œìƒ)\n",
    "\n",
    "- padding_side=\"left\"\n",
    "  - ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ íŒ¨ë”©(pad)ì„ ì™¼ìª½ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "  - ì´ëŠ” Decoder-only ê³„ì—´ LLM (GPT ê³„ì—´) ì—ì„œ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd4d817-2008-42c2-84d5-13867c8053cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"verseAI/databricks-dolly-v2-3b\",\n",
    "    # use_cache=False,\n",
    "    device_map=\"auto\", #\"balanced\",\n",
    "    load_in_8bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d9354-c68b-4637-8985-8f781fadfae0",
   "metadata": {},
   "source": [
    "### <a name=\"#step4.1\">4.1ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ì¤€ë¹„</a>\n",
    "ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ ë¯¸ì„¸ ì¡°ì •(PEFT)ì„ ì‚¬ìš©í•˜ì—¬ INT8 ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ì „ì— ëª‡ ê°€ì§€ ì „ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ `prepare_model_for_int8_training`ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "\n",
    "- ì•ˆì •ì„±ì„ ìœ„í•´ ëª¨ë“  ë¹„ INT8 ëª¨ë“ˆì„ ì „ì²´ ì •ë°€ë„(FP32)ë¡œ ìºìŠ¤íŒ…í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ì…ë ¥ ì€ë‹‰ ìƒíƒœì˜ ê¸°ìš¸ê¸° ê³„ì‚°ì„ í™œì„±í™”í•˜ê¸° ìœ„í•´ ì…ë ¥ ì„ë² ë”© ë ˆì´ì–´ì— forward_hookì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ„í•´ ê¸°ìš¸ê¸° ì²´í¬í¬ì¸íŒ…ì„ í™œì„±í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df096cfc-4c37-4155-bb77-e2c7164e3bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc7ea0-89b3-4164-a8d5-3ced6179645f",
   "metadata": {},
   "source": [
    "`preprocess_batch` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ë°ì´í„°ì˜ í…ìŠ¤íŠ¸ í•„ë“œë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì§€ì •ëœ ìµœëŒ€ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í† í°í™”, ì˜ë¦¼ ë° ê¸°íƒ€ ê´€ë ¨ ì—°ì‚°ì„ ì ìš©í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ë°ì´í„° ë°°ì¹˜, í† í¬ë‚˜ì´ì € ë° ìµœëŒ€ ê¸¸ì´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ìì„¸í•œ ë‚´ìš©ì€ `mlu_utils/helpers.py` íŒŒì¼ì„ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de28ac-e6cb-46f3-8410-8d82923b1270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from utils.helpers import mlu_preprocess_batch\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "_preprocessing_function = partial(mlu_preprocess_batch, max_length=MAX_LENGTH, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a47b1-03c4-45b9-82b6-14bbf3c46bd0",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ, ë°ì´í„°ì…‹ì˜ ê° ë°°ì¹˜ì— ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í•„ë“œë¥¼ ì ì ˆíˆ ìˆ˜ì •í•©ë‹ˆë‹¤. ë§¤í•‘ ì‘ì—…ì€ ë°°ì¹˜ ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ë˜ë©°, ì§€ì‹œì‚¬í•­, ì‘ë‹µ ë° í…ìŠ¤íŠ¸ ì—´ì´ ë°ì´í„°ì…‹ì—ì„œ ì œê±°ë©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, `sagemaker_faqs_dataset`ì—ì„œ `input_ids` í•„ë“œì˜ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§í•˜ì—¬ ì§€ì •ëœ `MAX_LENGTH` ë‚´ì— ë§ë„ë¡ í•˜ì—¬ `processed_dataset`ì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5cd938-efa0-4fe7-b24d-a5e2cdf3cf9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_sagemaker_faqs_dataset = sagemaker_faqs_dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"response\", \"text\"],\n",
    ")\n",
    "\n",
    "processed_dataset = encoded_sagemaker_faqs_dataset.filter(lambda rec: len(rec[\"input_ids\"]) < MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd86b08-e90e-4fad-9107-f9dcf63915ad",
   "metadata": {},
   "source": [
    "Split the dataset into `train` and `test` for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ba6bb-41a7-455d-a349-8219405eec98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_dataset = processed_dataset.train_test_split(test_size=14, seed=0)\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b2da6-08d9-4a1d-bfa9-0e847e104ce9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"#step5\">5ë‹¨ê³„: íŠ¸ë ˆì´ë„ˆ ì •ì˜ ë° LLM ë¯¸ì„¸ ì¡°ì •</a>\n",
    "\n",
    "ì´ ì‹¤ìŠµì—ì„œëŠ” íš¨ìœ¨ì ì¸ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•´ [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. LoRAëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³ ì •í•˜ê³  í•™ìŠµ ê°€ëŠ¥í•œ ë­í¬ ë¶„í•´ í–‰ë ¬ì„ Transformer ì•„í‚¤í…ì²˜ì˜ ê° ë ˆì´ì–´ì— ì£¼ì…í•˜ì—¬ í•˜ìœ„ ì‘ì—…ì— í•„ìš”í•œ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤. Adamìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ GPT-3 175Bì™€ ë¹„êµí–ˆì„ ë•Œ, LoRAëŠ” í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ 10,000ë°° ì¤„ì´ê³  GPU ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ì„ 3ë°°ê¹Œì§€ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### <a name=\"#step5.1\">5.1ë‹¨ê³„: LoraConfig ì •ì˜ ë° LoRA ëª¨ë¸ ë¡œë“œ</a>\n",
    "\n",
    "[huggingface ğŸ¤— PEFT: ìµœì²¨ë‹¨ íŒŒë¼ë¯¸í„° íš¨ìœ¨ ë¯¸ì„¸ ì¡°ì •](https://github.com/huggingface/peft)ì˜ ë¹Œë“œëœ LoRA í´ë˜ìŠ¤ `LoraConfig`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. `LoraConfig` ë‚´ì—ì„œ ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `r`: ì €ì°¨ í–‰ë ¬ì˜ ì°¨ì›\n",
    "- `lora_alpha`: ì €ì°¨ í–‰ë ¬ì˜ ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜\n",
    "- `lora_dropout`: LoRA ë ˆì´ì–´ì˜ ë“œë¡­ì•„ì›ƒ í™•ë¥ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98cace8-19f6-41c0-9fa2-04b8bb987fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# First prepare the model for int8 training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# Then freeze all parameters\n",
    "for param in model.parameters():\n",
    "    if param.dtype == torch.float32 or param.dtype == torch.float16:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Unfreeze only the top N layers\n",
    "num_layers_to_unfreeze = 2  # Adjust this number as needed\n",
    "for i, layer in enumerate(model.gpt_neox.layers[-num_layers_to_unfreeze:]):\n",
    "    for param in layer.parameters():\n",
    "        if param.dtype == torch.float32 or param.dtype == torch.float16:\n",
    "            param.requires_grad = True\n",
    "        \n",
    "MICRO_BATCH_SIZE = 8\n",
    "BATCH_SIZE = 64\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LORA_R = 256 # 512\n",
    "LORA_ALPHA = 512 # 1024\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "        \n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "                 r=LORA_R,\n",
    "                 lora_alpha=LORA_ALPHA,\n",
    "                 lora_dropout=LORA_DROPOUT,\n",
    "                 bias=\"none\",\n",
    "                 task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166278b-9810-4d25-a5b2-95db0e376d76",
   "metadata": {},
   "source": [
    "Use the `get_peft_model` function to initialize the model with the LoRA framework, configuring it based on the provided `lora_config` settings. This way, the model can incorporate the benefits and capabilities of the LoRA optimization approach.\n",
    "\n",
    "â€œ3Bê¸‰ LLMì„ int8ë¡œ ë¡œë“œí•œ ë’¤,\n",
    "ì „ì²´ íŒŒë¼ë¯¸í„°ëŠ” ê³ ì •í•˜ê³ \n",
    "ìƒìœ„ ë ˆì´ì–´ + LoRA ì–´ëŒ‘í„°ë§Œ í•™ìŠµí•´ì„œ\n",
    "ì ì€ GPUë¡œ ì•ˆì •ì ì¸ Instruction Fine-tuningì„ í•˜ê² ë‹¤.â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6042bbe6-6815-4f19-95c1-c8946f6156d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f590f0f-e95e-4407-8716-e0a802ccc2b9",
   "metadata": {},
   "source": [
    "ë³´ì‹œëŠ” ë°”ì™€ ê°™ì´, LoRA ì „ìš© í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°ëŠ” ì „ì²´ ê°€ì¤‘ì¹˜ì˜ ì•½ 3%ì— ë¶ˆê³¼í•˜ì—¬ í›¨ì”¬ íš¨ìœ¨ì ì…ë‹ˆë‹¤.\n",
    "\n",
    "### <a name=\"#step5.2\">5.2ë‹¨ê³„: ë°ì´í„° ì½œë ˆì´í„° ì •ì˜</a>\n",
    "\n",
    "DataCollatorëŠ” ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œ ëª©ë¡ì„ ë°›ì•„ PyTorch í…ì„œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°°ì¹˜ë¡œ ì½œë ˆì´ì…˜í•˜ëŠ” transformers í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ê¸°ë³¸ `DataCollatorForLanguageModeling` í´ë˜ìŠ¤ì˜ ê¸°ëŠ¥ì„ í™•ì¥í•œ `DataCollatorForCompletionOnlyLM`ì„ ì‚¬ìš©í•˜ì„¸ìš”. ì´ ì‚¬ìš©ì ì§€ì • ì½œë ˆì´í„°ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ì— í”„ë¡¬í”„íŠ¸ ë‹¤ìŒì— ì‘ë‹µì´ ì˜¤ëŠ” ê²½ìš°ì™€ ê·¸ì— ë”°ë¼ ë ˆì´ë¸”ì´ ìˆ˜ì •ë˜ëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "êµ¬í˜„ì€ `utils/helpers.py`ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5cef99-9c84-4478-be05-317b7f79de45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.helpers import MLUDataCollatorForCompletionOnlyLM\n",
    "\n",
    "data_collator = MLUDataCollatorForCompletionOnlyLM(\n",
    "        tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321ebc5-c2db-4c44-9442-961122da9205",
   "metadata": {},
   "source": [
    "### <a name=\"#step5.3\">5.3ë‹¨ê³„: íŠ¸ë ˆì´ë„ˆ ì •ì˜</a>\n",
    "\n",
    "LLMì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ íŠ¸ë ˆì´ë„ˆë¥¼ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ë¨¼ì € í›ˆë ¨ ì¸ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07571a85-2724-44ee-a2b0-24ed5866ce2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "MODEL_SAVE_FOLDER_NAME = \"dolly-3b-lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                    output_dir=MODEL_SAVE_FOLDER_NAME,\n",
    "                    fp16=True,\n",
    "                    per_device_train_batch_size=8,\n",
    "                    per_device_eval_batch_size=8,\n",
    "                    learning_rate=LEARNING_RATE,\n",
    "                    num_train_epochs=EPOCHS,\n",
    "                    logging_strategy=\"steps\",\n",
    "                    logging_steps=100,\n",
    "                    evaluation_strategy=\"steps\",\n",
    "                    eval_steps=100,\n",
    "                    save_strategy=\"steps\",\n",
    "                    save_steps=20000,\n",
    "                    save_total_limit=10,\n",
    "                    optim=\"adamw_torch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b13b8-0139-48f5-8619-acf459b78d33",
   "metadata": {},
   "source": [
    "This is where the magic happens! Initialize the trainer with the defined model, tokenizer, training arguments, data collator, and the train/eval datasets.\n",
    "\n",
    "The training takes about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708616f-8fbc-4f1e-9f50-077ae198fd69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=split_dataset['train'],\n",
    "        eval_dataset=split_dataset[\"test\"],\n",
    "        data_collator=data_collator,\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf5ffa-3997-415f-bf51-dd91d8c3d777",
   "metadata": {},
   "source": [
    "### <a name=\"#step5.4\">5.4ë‹¨ê³„: ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ ì €ì¥</a>\n",
    "\n",
    "í•™ìŠµì´ ì™„ë£Œë˜ë©´ `transformers.PreTrainedModel.save_pretrained` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë””ë ‰í† ë¦¬ì— ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ í•¨ìˆ˜ëŠ” í•™ìŠµëœ ì¦ë¶„ PEFT ê°€ì¤‘ì¹˜(adapter_model.bin)ë§Œ ì €ì¥í•˜ë¯€ë¡œ ëª¨ë¸ì„ ì €ì¥, ì „ì†¡ ë° ë¶ˆëŸ¬ì˜¤ëŠ” ë° ë§¤ìš° íš¨ìœ¨ì ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f327e9-c6cd-4f2b-af25-e22420372e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9cc6d-98cf-4118-8218-6de6c9893570",
   "metadata": {},
   "source": [
    "If you want to save the full model that you just fine-tuned, you can use the [`transformers.trainer.save_model`] function. Meanwhile, save the training arguments together with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112abbd-8b1b-4da7-80d4-35b92e4eadcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198216a6-598e-4d56-af8d-682a71b187bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.model.config.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33f0f9-2604-45fb-a32d-6a1b2e311513",
   "metadata": {},
   "source": [
    "Save the tokenizer along with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aac92b-8bcb-46dc-87d6-ff5069b0896c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3c515-1c98-4282-9040-7af7d702fb8e",
   "metadata": {},
   "source": [
    "## <a name=\"#step6\">6ë‹¨ê³„: ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ ë°°í¬</a>\n",
    "\n",
    "### <a name=\"step6\">ë°°í¬ ë§¤ê°œë³€ìˆ˜ ê°œìš”</a>\n",
    "\n",
    "Amazon SageMaker Python SDKì™€ Deep Java Library(DJL)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°í¬í•˜ë ¤ë©´ ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ `Model` í´ë˜ìŠ¤ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "```{python}\n",
    "model = Model(\n",
    "image_uri,\n",
    "model_data=...,\n",
    "predictor_cls=...,\n",
    "role=aws_role\n",
    ")\n",
    "```\n",
    "- `image_uri`: ì‚¬ìš©í•  ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ ë° ë²„ì „ì„ ë‚˜íƒ€ë‚´ëŠ” Docker ì´ë¯¸ì§€ URIì…ë‹ˆë‹¤.\n",
    "\n",
    "- `model_data`: Amazon Simple Storage Service(Amazon S3) ë²„í‚·ì— ìˆëŠ” ë¯¸ì„¸ ì¡°ì •ëœ LLM ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ì˜ ìœ„ì¹˜ì…ë‹ˆë‹¤. ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜, ì•„í‚¤í…ì²˜ ë° í•„ìš”í•œ ì•„í‹°íŒ©íŠ¸ê°€ í¬í•¨ëœ TAR GZ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `predictor_cls`: DJLê³¼ ê´€ë ¨ì´ ì—†ëŠ” JSON ì…ë ¥ JSON ì¶œë ¥ ì˜ˆì¸¡ê¸°ì…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [sagemaker.djl_inference.DJLPredictor](https://sagemaker.readthedocs.io/en/stable/frameworks/djl/sagemaker.djl_inference.html#djlpredictor)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n",
    "\n",
    "- `role`: ëª¨ë¸ ë°ì´í„°ê°€ í¬í•¨ëœ S3 ë²„í‚·ê³¼ ê°™ì€ ë¦¬ì†ŒìŠ¤ì— ì•¡ì„¸ìŠ¤í•˜ëŠ” ë° í•„ìš”í•œ ê¶Œí•œì„ ì œê³µí•˜ëŠ” AWS Identity and Access Management(IAM) ì—­í•  ARNì…ë‹ˆë‹¤.\n",
    "\n",
    "### <a name=\"step6.1\">6.1ë‹¨ê³„: SageMaker ë§¤ê°œë³€ìˆ˜ ì¸ìŠ¤í„´ìŠ¤í™”</a>\n",
    "\n",
    "Amazon SageMaker ì„¸ì…˜ì„ ì´ˆê¸°í™”í•˜ê³  SageMaker ì—­í•  ë° AWS ë¦¬ì „ê³¼ ê°™ì€ AWS í™˜ê²½ ê´€ë ¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ë˜í•œ SageMaker ì„¸ì…˜ì˜ ë¦¬ì „ì„ ì‚¬ìš©í•˜ì—¬ \"djl-deepspeed\" í”„ë ˆì„ì›Œí¬ì˜ íŠ¹ì • ë²„ì „ì— ëŒ€í•œ ì´ë¯¸ì§€ URIë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ URIëŠ” Amazon SageMaker ë˜ëŠ” Amazon Elastic Container Registry(Amazon ECR)ì™€ ê°™ì€ ë‹¤ì–‘í•œ AWS ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íŠ¹ì • Docker ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ì— ëŒ€í•œ ê³ ìœ  ì‹ë³„ìì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147f60a-5209-460c-bb29-8d1b57d27e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing sagemaker library\n",
    "!pip3 install sagemaker==2.237.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f11d90-e6ac-42f4-a8ef-b74f55d8c15d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker.djl_inference\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import Model\n",
    "\n",
    "sagemaker_session = Session()\n",
    "print(\"sagemaker_session: \", sagemaker_session)\n",
    "\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "print(\"aws_role: \", aws_role)\n",
    "\n",
    "aws_region = boto3.Session().region_name\n",
    "print(\"aws_region: \", aws_region)\n",
    "\n",
    "image_uri = image_uris.retrieve(framework=\"djl-deepspeed\",\n",
    "                                version=\"0.22.1\",\n",
    "                                region=sagemaker_session._region_name)\n",
    "print(\"image_uri: \", image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3ae0e-6639-4400-9123-c38f1b79fd99",
   "metadata": {},
   "source": [
    "### <a name=\"step6.2\">6.2ë‹¨ê³„: ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ìƒì„±</a> ###\n",
    "\n",
    "ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë¥¼ S3 ë²„í‚·ì— ì—…ë¡œë“œí•˜ë ¤ë©´ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ê°€ í¬í•¨ëœ TAR GZ íŒŒì¼ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. ë¨¼ì € `lora_model`ì´ë¼ëŠ” ë””ë ‰í† ë¦¬ì™€ `dolly-3b-lora`ë¼ëŠ” í•˜ìœ„ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. \"-p\" ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ ì¤‘ê°„ ë””ë ‰í† ë¦¬ê°€ ì—†ëŠ” ê²½ìš° ëª…ë ¹ì´ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ LoRA ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì¸ `adapter_model.bin`ê³¼ `adapter_config.json`ì„ `dolly-3b-lora` ë””ë ‰í† ë¦¬ì— ë³µì‚¬í•©ë‹ˆë‹¤. ê¸°ë³¸ Dolly ëª¨ë¸ì€ ëŸ°íƒ€ì„ ì‹œ Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5685e182-95e0-4ec3-888d-bb573ebdd3db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf lora_model\n",
    "mkdir -p lora_model\n",
    "mkdir -p lora_model/dolly-3b-lora\n",
    "cp dolly-3b-lora/adapter_config.json lora_model/dolly-3b-lora/\n",
    "cp dolly-3b-lora/adapter_model.bin lora_model/dolly-3b-lora/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df3e690-c244-44b6-9ff7-7a51d2ee47e3",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ, `serving.properties` íŒŒì¼ì— [DJL Serving êµ¬ì„± ì˜µì…˜](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html)ì„ ì„¤ì •í•©ë‹ˆë‹¤. Jupyterì˜ `%%writefile` ë§¤ì§ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë‚´ìš©ì„ `lora_model/serving.properties`ë¼ëŠ” íŒŒì¼ì— ê¸°ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- `engine=Python`: ì´ ì¤„ì€ ì„œë¹™ì— ì‚¬ìš©í•  ì—”ì§„ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `option.entryPoint=model.py`: ì´ ì¤„ì€ ì„œë¹™ í”„ë¡œì„¸ìŠ¤ì˜ ì§„ì…ì ì„ ì§€ì •í•˜ë©°, `model.py`ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.\n",
    "\n",
    "- `option.adapter_checkpoint=dolly-3b-lora`: ì´ ì¤„ì€ ì–´ëŒ‘í„°ì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ `dolly-3b-lora`ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. ì²´í¬í¬ì¸íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ ë˜ëŠ” í•´ë‹¹ ë§¤ê°œë³€ìˆ˜ì˜ ì €ì¥ëœ ìƒíƒœë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    "- `option.adapter_name=dolly-lora`: ì´ ì¤„ì€ ì–´ëŒ‘í„°ì˜ ì´ë¦„ì„ dolly-loraë¡œ ì„¤ì •í•©ë‹ˆë‹¤. dolly-loraëŠ” ëª¨ë¸ê³¼ ì„œë¹„ìŠ¤ ì¸í”„ë¼ ê°„ì˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì§€ì›í•˜ëŠ” êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa2845-a04b-48aa-8381-a8097bce07b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile lora_model/serving.properties\n",
    "engine=Python\n",
    "option.entryPoint=model.py\n",
    "option.adapter_checkpoint=dolly-3b-lora\n",
    "option.adapter_name=dolly-lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91554e8a-c65d-409b-bf16-29c179a7a6df",
   "metadata": {},
   "source": [
    "You also need the environment requirement file in the model artifact. Create a file named `lora_model/requirements.txt` and write a list of Python package requirements, typically used with package managers such as `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b09e8-3080-4f79-bec8-7ebfb2feeb64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile lora_model/requirements.txt\n",
    "accelerate>=0.16.0,<1\n",
    "bitsandbytes==0.39.0\n",
    "click>=8.0.4,<9\n",
    "datasets>=2.10.0,<3\n",
    "deepspeed>=0.8.3,<0.9\n",
    "faiss-cpu==1.7.4\n",
    "ipykernel==6.22.0\n",
    "scipy==1.11.1\n",
    "torch>=2.0.0\n",
    "transformers==4.28.1\n",
    "peft==0.3.0\n",
    "pytest==7.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca79001-7dd7-49ca-80e1-3ee88a8034b8",
   "metadata": {},
   "source": [
    "### <a name=\"step6.3\">6.3ë‹¨ê³„: ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ ìƒì„±</a>\n",
    "\n",
    "ë¯¸ì„¸ ì¡°ì • ë…¸íŠ¸ë¶ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, `InstructionTextGenerationPipeline`ì´ë¼ëŠ” ì‚¬ìš©ì ì§€ì • íŒŒì´í”„ë¼ì¸ì´ ì •ì˜ë©ë‹ˆë‹¤. ì½”ë“œëŠ” `utils/deployment_model.py`ì— ì œê³µë©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŸ¬í•œ ì¶”ë¡  í•¨ìˆ˜ë“¤ì„ `lora_model/model.py`ì— ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0229e-0895-436e-b789-edbb3ba773cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp utils/deployment_model.py lora_model/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d7b1f-7b8d-444a-b185-a46053421536",
   "metadata": {},
   "source": [
    "### <a name=\"step6.4\">6.4ë‹¨ê³„: ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë¥¼ Amazon S3ì— ì—…ë¡œë“œ</a>\n",
    "\n",
    "lora_model ë””ë ‰í† ë¦¬ë¥¼ ì••ì¶•ëœ tarball ì•„ì¹´ì´ë¸Œë¡œ ìƒì„±í•˜ê³  lora_model.tar.gzë¡œ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c84297-6f89-47b5-9f11-2cf1d2fba989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "tar -cvzf lora_model.tar.gz lora_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e15a5-832a-453c-b02d-b0e5c82ca8b5",
   "metadata": {},
   "source": [
    "Upload the lora_model.tar.gz file to the specified S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a3067-47f1-4ddc-9f81-0d6c33801d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker.djl_inference\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import Model\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Get the name of the bucket with prefix lab-code\n",
    "for bucket in s3.buckets.all():\n",
    "    if bucket.name.startswith('artifact'):\n",
    "        mybucket = bucket.name\n",
    "        print(mybucket)\n",
    "\n",
    "response = s3_client.upload_file(\"lora_model.tar.gz\", mybucket, \"lora_model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d142b4-c420-416e-a0c5-3068a934d573",
   "metadata": {},
   "source": [
    "### <a name=\"step6.5\">6.5ë‹¨ê³„: ëª¨ë¸ ë°°í¬</a> ###\n",
    "\n",
    "ì´ì œ SageMaker Python SDKë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •ëœ LLM ëª¨ë¸ì„ ë°°í¬í•  ì°¨ë¡€ì…ë‹ˆë‹¤. SageMaker Python SDKì˜ `Model` í´ë˜ìŠ¤ëŠ” ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤í™”ë©ë‹ˆë‹¤.\n",
    "\n",
    "- `image_uri`: ì‚¬ìš©í•  ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ ë° ë²„ì „ì„ ë‚˜íƒ€ë‚´ëŠ” Docker ì´ë¯¸ì§€ URIì…ë‹ˆë‹¤.\n",
    "\n",
    "- `model_data`: S3 ë²„í‚·ì— ìˆëŠ” ë¯¸ì„¸ ì¡°ì •ëœ LLM ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ì˜ ìœ„ì¹˜ì…ë‹ˆë‹¤. ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜, ì•„í‚¤í…ì²˜ ë° í•„ìš”í•œ ì•„í‹°íŒ©íŠ¸ê°€ í¬í•¨ëœ TAR GZ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `predictor_cls`: ì´ëŠ” JSON ì…ë ¥, JSON ì¶œë ¥ ì˜ˆì¸¡ê¸°ì´ë©° DJLê³¼ëŠ” ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [sagemaker.djl_inference.DJLPredictor](https://sagemaker.readthedocs.io/en/stable/frameworks/djl/sagemaker.djl_inference.html#djlpredictor)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n",
    "- `role`: ëª¨ë¸ ë°ì´í„°ê°€ í¬í•¨ëœ S3 ë²„í‚·ê³¼ ê°™ì€ ë¦¬ì†ŒìŠ¤ì— ì•¡ì„¸ìŠ¤í•˜ëŠ” ë° í•„ìš”í•œ ê¶Œí•œì„ ì œê³µí•˜ëŠ” IAM ì—­í•  ARNì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f0b12-bf0f-4fd8-bb7d-a0000ee67b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data=\"s3://{}/lora_model.tar.gz\".format(mybucket)\n",
    "\n",
    "model = Model(image_uri=image_uri,\n",
    "              model_data=model_data,\n",
    "              predictor_cls=sagemaker.djl_inference.DJLPredictor,\n",
    "              role=aws_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d5263-2c04-4541-8126-e718ab7e55ce",
   "metadata": {},
   "source": [
    "NOTE: The deployment should be completed within 10 minutes. Any longer than that, your endpoint might have failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269abf73-c194-48c0-8f5e-6c23547acb5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = model.deploy(1, \"ml.g4dn.2xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6aff0-5525-4bc8-945b-96391b9fc1bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a name=\"step7\">Step 7: Test the deployed inference</a>\n",
    "\n",
    "Test the inference endpoint with [predictor.predict](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html#sagemaker.predictor.Predictor.predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d371af-3486-441f-8d6f-ef65bf12395d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = predictor.predict({\"inputs\": \"What security measures does Amazon SageMaker have?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2df51a6-6e71-4c3a-b2be-116fc07851a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a2199-7616-4c05-8f3d-6a5020856332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
